{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIHpnulXtom0"
      },
      "source": [
        "# Loss Graph & Hyperparameter Tune & Validation & Train\n",
        "# Make sure forward pass works (for hard negative data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay9ttw0IlPZ2"
      },
      "source": [
        "## Setup & Dataset Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J965j0wvtom2"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIV97VrN-r05",
        "outputId": "3fddea1a-72e1-4d85-ad9c-c2fe125d820c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray[air] in /usr/local/lib/python3.10/dist-packages (2.5.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from ray[air]) (23.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (8.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[air]) (3.12.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[air]) (4.3.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.0.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[air]) (23.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (4.23.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[air]) (6.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[air]) (2.27.1)\n",
            "Requirement already satisfied: grpcio<=1.51.3,>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.51.3)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.22.4)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.27.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.100.0)\n",
            "Requirement already satisfied: virtualenv<20.21.1,>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (20.21.0)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.5.3)\n",
            "Requirement already satisfied: gpustat>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.1)\n",
            "Requirement already satisfied: aiorwlock in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.3.0)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.7.0)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.3.14)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[air]) (6.3.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (9.0.0)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.11.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.10.11)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.5.5)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.17.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (2.6.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.22.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[air]) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (3.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (1.9.2)\n",
            "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[air]) (12.535.77)\n",
            "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[air]) (5.9.5)\n",
            "Requirement already satisfied: blessed>=1.17.1 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[air]) (1.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->ray[air]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->ray[air]) (2022.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[air]) (0.3.6)\n",
            "Requirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[air]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->ray[air]) (4.7.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->ray[air]) (3.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[air]) (0.19.3)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[air]) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[air]) (2.11.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[air]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[air]) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[air]) (3.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->ray[air]) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->ray[air]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->ray[air]) (1.1.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[air]) (0.2.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[air]) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (1.59.1)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (2.17.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (0.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"ray[air]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "JqYY0cwIkJmq"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "import os\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split\n",
        "# from ray import tune\n",
        "# from ray.air import Checkpoint, session\n",
        "# from ray.tune.schedulers import ASHAScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBkHOxqFtom3"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT3vR52KNjgA",
        "outputId": "455a6c47-9494-45be-f87a-32627c8f6282"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "batch_size = 64\n",
        "supervised = True\n",
        "lrs = [1e-2,5e-3,1e-3,1e-4,1e-3]\n",
        "momentums = [0.9,0.9,0.9,0.9,0.9]\n",
        "weight_decays = [1e-5,5e-5,1e-3,1e-3,1e-5]\n",
        "thresholds = [50,1000,1000,1000]\n",
        "num_epochs = 15\n",
        "records_per_epoch = 4\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "eGYNvwIhker4"
      },
      "outputs": [],
      "source": [
        "# import MNIST\n",
        "def load_mnist(data_dir=\"./data\"):\n",
        "  train_data = datasets.MNIST(\n",
        "      root = data_dir,\n",
        "      train = True,\n",
        "      transform = ToTensor(),\n",
        "      download = True,\n",
        "  )\n",
        "  test_data = datasets.MNIST(\n",
        "      root = data_dir,\n",
        "      train = False,\n",
        "      transform = ToTensor(),\n",
        "      download = True\n",
        "  )\n",
        "  return train_data,test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "exjpqMVzcMBG"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = load_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4C-DcpX9p2o",
        "outputId": "bd55ead2-42cd-43fc-d4ce-db92a38fa41f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7e30800dd450>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7e30800dea10>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Dataloader\n",
        "train_loader = DataLoader(train_data,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        num_workers=1)\n",
        "\n",
        "test_loader = DataLoader(test_data,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=1)\n",
        "\n",
        "train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xEIHA2ZN1wj"
      },
      "source": [
        "## Construct Negative Examples for Unsupervised Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfySAAjudWtk"
      },
      "outputs": [],
      "source": [
        "filter = torch.tensor([[0.0625,0.125,0.0625],[0.125,0.25,0.125],[0.0625,0.125,0.0625]])\n",
        "\n",
        "def blur(img,filter):\n",
        "  # blur img using filter\n",
        "  # params: filter --> torch.tensor(2*radius+1,2*radius+1)\n",
        "  m,n = img.shape\n",
        "  radius = (filter.shape[0]-1)//2\n",
        "  new_img = torch.zeros(m,n)\n",
        "  for i in range(m):\n",
        "    for j in range(n):\n",
        "      top,left,bottom,right = max(0,i-radius),max(0,j-radius),min(m-1,i+radius),min(m-1,j+radius)\n",
        "      new_img[i][j] = sum([img[x][y]*filter[x-i+radius][y-j+radius]\n",
        "                           for y in range(left,right+1) for x in range(top,bottom+1)])\n",
        "  return new_img\n",
        "\n",
        "def generate_negative_example():\n",
        "  # construct a negative example for unsupervised case (3.2 in paper)\n",
        "  mask = (torch.rand(28,28) > 0.5).long() # initiate as random bit image\n",
        "\n",
        "  for i in range(6): # repeat blurring 6 times\n",
        "    mask = blur(mask,filter)\n",
        "\n",
        "  mask = (mask > 0.5).long() # mask\n",
        "\n",
        "  index1,index2 = torch.randint(len(train_data),(2,))\n",
        "  return (mask * train_data[index1][0] + (1-mask) * train_data[index2][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxx1HhrVZU8b"
      },
      "source": [
        "## Model Definition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "dOcFixAKTEOh"
      },
      "outputs": [],
      "source": [
        "# single-layer NN\n",
        "class one_layer_net(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(one_layer_net, self).__init__()\n",
        "\n",
        "        # hidden layer\n",
        "        # self.layer = torch.nn.Sequential(\n",
        "            # torch.nn.Flatten(), # nn.Flatten to standardize input shape\n",
        "            # torch.nn.Linear(in_size, out_size),\n",
        "            # torch.nn.ReLU() # activation function\n",
        "        # )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.layer = nn.Linear(in_size, out_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      f = self.flatten(x)\n",
        "      l = self.layer(f)\n",
        "      r = self.relu(l)\n",
        "      return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "0iLwMtYODMgb"
      },
      "outputs": [],
      "source": [
        "# full network\n",
        "class net(nn.Module):\n",
        "    # Constructor\n",
        "    def __init__(self,in_size):\n",
        "        super(net, self).__init__()\n",
        "        self.layer1 = one_layer_net(in_size,2000)\n",
        "        self.layer2 = one_layer_net(2000,2000)\n",
        "        self.layer3 = one_layer_net(2000,2000)\n",
        "        self.layer4 = one_layer_net(2000,2000)\n",
        "        self.ln = nn.LayerNorm(2000) # layernorm\n",
        "\n",
        "        # softmax layer for hard negative label generation\n",
        "        self.out_layer = nn.Linear(6000, 10)\n",
        "\n",
        "        self.layers = [self.layer1,self.layer2,self.layer3,self.layer4,self.out_layer]\n",
        "\n",
        "    def forward(self,x):\n",
        "        # x-->tensor (batch_size,channel,height,width)\n",
        "        # return res --> tensor (batch_size,10)\n",
        "        interms = []\n",
        "        interm = x.detach().clone()\n",
        "\n",
        "        for index in range(len(self.layers)-1):\n",
        "          layer = self.layers[index] # linear layer\n",
        "          output = layer(interm) # run the layer\n",
        "          interm = self.ln(output) # layer-norm\n",
        "          interms.append(interm.detach().clone()) # store normalized activity\n",
        "\n",
        "        final_input = torch.cat(interms[1:],dim=1)\n",
        "        res = self.out_layer(final_input) # final output layer\n",
        "        return res\n",
        "\n",
        "    def predict(self,x,device):\n",
        "        # x-->tensor (batch_size,channel,height,width)\n",
        "        # return labels --> tensor (batch_size,1)\n",
        "        accumulate_goodness = torch.zeros(x.shape[0],10).to(device) # accumulate goodness for each label\n",
        "        for i in range(10): # iterate through all labels\n",
        "          interm = x.detach().clone()\n",
        "\n",
        "          # update one-hot label encoding\n",
        "          interm[:,:,0,:10] = 0\n",
        "          interm[:,:,0,i] = 1\n",
        "\n",
        "          for index in range(len(self.layers)-1):\n",
        "            layer = self.layers[index]\n",
        "            interm = layer(interm)\n",
        "\n",
        "            if index > 0: # accumulate goodness for all activities but first hidden layer\n",
        "              accumulate_goodness[:,i] = accumulate_goodness[:,i] + self.goodness(interm)\n",
        "\n",
        "            if index + 1 < len(self.layers): # layernorm\n",
        "              interm = self.ln(interm)\n",
        "        return torch.argmax(accumulate_goodness,dim=1)\n",
        "\n",
        "    def goodness(self,x):\n",
        "        # goodness functions--sum of squares\n",
        "        # input: x-->tensor (batch_size, ...) (arbitrary shape that starts with batch_size)\n",
        "        # return goodness-->tensor (batch_size,1)\n",
        "        x = x.reshape(x.shape[0],-1)\n",
        "        return torch.sum(torch.square(x),dim=1)\n",
        "\n",
        "    def criterion(self,x, threshold):\n",
        "        # criterion function--mean(sigmoid(goodness - threshold))\n",
        "        # input: x-->tensor (batch_size, ...) (arbitrary shape that starts with batch_size)\n",
        "        # return loss-->float\n",
        "        goodness = self.goodness(x)\n",
        "        res = torch.mean(torch.sigmoid(goodness - threshold))\n",
        "        # print(goodness,res)\n",
        "        return res\n",
        "\n",
        "    def construct_supervised_example(self,x,y,positive=True):\n",
        "        # construct positive or negative examples for supervised training\n",
        "        # x-->tensor (batch_size,channel,height,width)\n",
        "        # y-->tensor (batch_size,)\n",
        "        # return ans-->tensor (batch_size,channel,height,width)\n",
        "\n",
        "        ans = x.detach().clone()\n",
        "\n",
        "        if positive:\n",
        "          # one-hot encoding\n",
        "          ans[:,:,0,:10] = 0\n",
        "          for i in range(x.shape[0]):\n",
        "             ans[i,:,0,y[i]] = 1\n",
        "\n",
        "        else:\n",
        "          ans[:,:,0,:10] = 0.1 # initialize neutral label\n",
        "\n",
        "          # run forward pass\n",
        "          labels = self.forward(ans)\n",
        "\n",
        "          for i in range(x.shape[0]):\n",
        "             labels[i][y[i]] = -float('inf') # omit positive label\n",
        "          prob = torch.softmax(labels,dim=1) # probability distribution to choose label\n",
        "          negative_label = torch.multinomial(prob,1).squeeze() # choose from distribution\n",
        "\n",
        "          # generate negative example from forward pass\n",
        "          ans[:,:,0,:10] = 0\n",
        "          for i in range(x.shape[0]):\n",
        "              ans[i,:,0,negative_label[i]] = 1\n",
        "\n",
        "        return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXS93nQJOmoD",
        "outputId": "351b1a3a-85ba-42ac-b08d-4be9c06f14ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "net(\n",
              "  (layer1): one_layer_net(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (layer): Linear(in_features=784, out_features=2000, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (layer2): one_layer_net(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (layer): Linear(in_features=2000, out_features=2000, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (layer3): one_layer_net(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (layer): Linear(in_features=2000, out_features=2000, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (layer4): one_layer_net(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (layer): Linear(in_features=2000, out_features=2000, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (ln): LayerNorm((2000,), eps=1e-05, elementwise_affine=True)\n",
              "  (out_layer): Linear(in_features=6000, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# create model\n",
        "model = net(784)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "kC0QvPgPfknW"
      },
      "outputs": [],
      "source": [
        "# apply xavier initialization on weights\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "wbVlBrAYtom7"
      },
      "outputs": [],
      "source": [
        "pos_optimizers, neg_optimizers = [],[]\n",
        "final_optimizer = torch.optim.SGD(model.layers[-1].parameters(), lr=lrs[-1], momentum = momentums[-1], weight_decay=weight_decays[-1], maximize=False)\n",
        "# initialize each layer\n",
        "for index in range(len(model.layers)):\n",
        "    # layer & training initialization\n",
        "    layer = model.layers[index]\n",
        "    layer.apply(init_weights)\n",
        "    # clip gradients to avoid exploding gradients\n",
        "    torch.nn.utils.clip_grad_norm_(parameters=layer.parameters(), max_norm=1,norm_type=2.0)\n",
        "    # define optimizers\n",
        "    if index < len(model.layers)-1:\n",
        "      pos_optimizers.append(torch.optim.SGD(layer.parameters(), lr=lrs[index], momentum = momentums[index], weight_decay=weight_decays[index], maximize=True))\n",
        "      neg_optimizers.append(torch.optim.SGD(layer.parameters(), lr=lrs[index], momentum = momentums[index], weight_decay=weight_decays[index], maximize=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-KK7uRkQMKTi"
      },
      "outputs": [],
      "source": [
        "# design loss function\n",
        "def goodness_loss(pos_goodness,neg_goodness):\n",
        "  # loss function intended for maximizing goodness in positive examples\n",
        "  # and minimizing goodness in negative examples\n",
        "  return torch.log(1+torch.exp(neg_goodness-pos_goodness))\n",
        "\n",
        "\n",
        "# CrossEntropyLoss for final softmax layer optimization\n",
        "ceLoss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "K5-yXqsh1-HP"
      },
      "outputs": [],
      "source": [
        "# model evaluation using test set\n",
        "def test(model,test_loader,loss_func):\n",
        "    # return loss-->float, accuracy-->float\n",
        "    model.eval() # switch to eval mode\n",
        "    with torch.no_grad(): # disable gradient calculation\n",
        "        correct = 0\n",
        "        total_loss = 0\n",
        "        total_steps = 0\n",
        "        total = 0\n",
        "        for x, y in test_loader:\n",
        "            x,y = x.to(device),y.to(device)\n",
        "            # use forward to calculate loss\n",
        "            labels = model.forward(x)\n",
        "            loss = loss_func(labels,y)\n",
        "            total_loss += loss\n",
        "            total_steps += 1\n",
        "            # use predict to predict accuracies\n",
        "            predictions = model.predict(x,device)\n",
        "            acc = torch.sum(predictions == y)\n",
        "            correct += acc\n",
        "            total += x.shape[0]\n",
        "    model.train() # switch back to train mode\n",
        "    return total_loss/total_steps, correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZZ-V48mwWNK"
      },
      "source": [
        "## Hyperparemeter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_gvRB08YYgz3"
      },
      "outputs": [],
      "source": [
        "# train function implementing Ray Tune for hyperparameter tuning\n",
        "# train model\n",
        "def train_tune(config, data_dir='./data'):\n",
        "\n",
        "    # initialize model\n",
        "    model = net(784)\n",
        "\n",
        "    # configure device\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            model = nn.DataParallel(model)\n",
        "    model.to(device)\n",
        "\n",
        "    # configure loss function and optimizers\n",
        "    final_loss = config[\"final_loss\"]\n",
        "    pos_optimizers, neg_optimizers = [],[]\n",
        "    final_optimizer = torch.optim.SGD(model.layers[-1].parameters(), lr=config['lrs'][-1], momentum = config['momentums'][-1], weight_decay=config['weight_decays'][-1], maximize=False)\n",
        "    # iterate each layer\n",
        "    for index in range(len(model.layers)):\n",
        "        # layer & training initialization\n",
        "        layer = model.layers[index]\n",
        "        layer.apply(init_weights)\n",
        "        # clip gradients to avoid exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=layer.parameters(), max_norm=1,norm_type=2.0)\n",
        "        # define optimizers\n",
        "        if index < len(model.layers)-1:\n",
        "          pos_optimizers.append(torch.optim.SGD(layer.parameters(), lr=config['lrs'][index], momentum = config['momentums'][index], weight_decay=config['weight_decays'][index], maximize=True))\n",
        "          neg_optimizers.append(torch.optim.SGD(layer.parameters(), lr=config['lrs'][index], momentum = config['momentums'][index], weight_decay=config['weight_decays'][index], maximize=False))\n",
        "\n",
        "    # configure state of model if checkpoint exists\n",
        "    checkpoint = session.get_checkpoint()\n",
        "    if checkpoint:\n",
        "        checkpoint_state = checkpoint.to_dict()\n",
        "        start_epoch = checkpoint_state[\"epoch\"]\n",
        "        net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
        "        for index in range(len(model.layers)-1):\n",
        "          pos_optimizers[index].load_state_dict(checkpoint_state[\"optimizer_state_dict\"][\"pos\"][index])\n",
        "          neg_optimizers[index].load_state_dict(checkpoint_state[\"optimizer_state_dict\"][\"neg\"][index])\n",
        "        final_optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"][\"final\"])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    # load data\n",
        "    train_data, test_data = load_mnist(data_dir)\n",
        "\n",
        "    test_abs = int(len(train_data) * 0.9)\n",
        "    train_subset, val_subset = random_split(\n",
        "        train_data, [test_abs, len(train_data) - test_abs]\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
        "    )\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # variables for training output\n",
        "    # total_pos_goodnesses,total_neg_goodnesses = [],[]\n",
        "    # total_losses = []\n",
        "    # test_accuracies = []\n",
        "    total_step = len(train_loader)\n",
        "    record_period = total_step//records_per_epoch\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        total_pos_goodness,total_neg_goodness = [0]*(len(model.layers)-1),[0]*(len(model.layers)-1)\n",
        "        total_loss = 0\n",
        "        # total_loss = [0]*len(model.layers)\n",
        "        for i,(x,y) in enumerate(train_loader): # iterate through dataset\n",
        "            x,y = x.to(device),y.to(device)\n",
        "            if not config[\"supervised\"]: # unsupervised training input\n",
        "              pos_imgs = x.squeeze()\n",
        "              neg_imgs = generate_negative_example().squeeze().to(device)\n",
        "            else: # supervised learning input\n",
        "              pos_imgs = model.construct_supervised_example(x,y,True)\n",
        "              neg_imgs = model.construct_supervised_example(x,y,False)\n",
        "\n",
        "            # intermediate variables\n",
        "            pos_interms = []\n",
        "            pos_interm = pos_imgs.clone()\n",
        "            neg_interm = neg_imgs.clone()\n",
        "\n",
        "            # iterate over intermediate layers\n",
        "            for index in range(len(model.layers)-1):\n",
        "              layer = model.layers[index]\n",
        "\n",
        "              # positive pass\n",
        "              pos_output = layer(pos_interm)\n",
        "              pos_goodness = model.criterion(pos_output, threshold=config[\"thresholds\"][index])\n",
        "              pos_interm = model.ln(pos_output).detach()\n",
        "              pos_interms.append(pos_interm.clone())\n",
        "\n",
        "              # update variables\n",
        "              total_pos_goodness[index] += pos_goodness\n",
        "\n",
        "              # clear gradients for this training step\n",
        "              pos_optimizers[index].zero_grad()\n",
        "\n",
        "              # take gradient step\n",
        "              pos_goodness.backward()\n",
        "              pos_optimizers[index].step()\n",
        "\n",
        "              # negative pass\n",
        "              neg_output = layer(neg_interm)\n",
        "              neg_goodness = model.criterion(neg_output, threshold=thresholds[index])\n",
        "              neg_interm = model.ln(neg_output).detach()\n",
        "\n",
        "              total_neg_goodness[index] += neg_goodness\n",
        "              neg_optimizers[index].zero_grad()\n",
        "              neg_goodness.backward()\n",
        "              neg_optimizers[index].step()\n",
        "\n",
        "              # update variables\n",
        "              total_pos_goodness[index] += pos_goodness\n",
        "              total_neg_goodness[index] += neg_goodness\n",
        "\n",
        "              # check progress\n",
        "              if (i+1) % (record_period) == 0:\n",
        "                  avg_pos_goodness = total_pos_goodness[index]/(record_period)\n",
        "                  avg_neg_goodness = total_neg_goodness[index]/(record_period)\n",
        "                  # if len(total_pos_goodnesses)>index:\n",
        "                  #   total_pos_goodnesses[index].append(avg_pos_goodness)\n",
        "                  # else:\n",
        "                  #   total_pos_goodnesses.append([avg_pos_goodness])\n",
        "                  # if len(total_neg_goodnesses)>index:\n",
        "                  #   total_neg_goodnesses[index].append(avg_neg_goodness)\n",
        "                  # else:\n",
        "                  #   total_neg_goodnesses.append([avg_neg_goodness])\n",
        "\n",
        "                  print ('Layer {}, Epoch [{}/{}], Step [{}/{}], Positive Goodness: {:.4f}, Negative Goodness: {:.4f}'\n",
        "                        .format(index, epoch + 1, num_epochs, i + 1, total_step, avg_pos_goodness, avg_neg_goodness))\n",
        "                  total_pos_goodness[index],total_neg_goodness[index] = 0,0\n",
        "\n",
        "            # output layer\n",
        "            final_input = torch.cat(pos_interms[1:],dim=1)\n",
        "            labels = model.layers[-1](final_input)\n",
        "            loss = ceLoss(labels,y)\n",
        "            total_loss += loss\n",
        "\n",
        "            final_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            final_optimizer.step()\n",
        "\n",
        "            if (i+1) % record_period == 0:\n",
        "              avg_loss = total_loss/record_period\n",
        "              # total_losses.append(avg_loss)\n",
        "\n",
        "              print ('Final Layer, Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                    .format(epoch + 1, num_epochs, i + 1, total_step, avg_loss))\n",
        "              total_loss = 0\n",
        "\n",
        "              # validation\n",
        "              val_loss, val_acc = test(model, val_loader, config[\"final_loss\"])\n",
        "              print ('Epoch [{}/{}], Step [{}/{}], Test Accuracy: {:.4f}'\n",
        "                    .format(epoch + 1, num_epochs, i + 1, total_step, val_acc))\n",
        "              # test_accuracies.append(test_acc)\n",
        "\n",
        "              # record in ray tune\n",
        "              checkpoint_data = {\n",
        "                  \"epoch\": epoch,\n",
        "                  \"net_state_dict\": model.state_dict(),\n",
        "                  \"optimizer_state_dict\": {\n",
        "                      \"pos\":{}, \"neg\":{}, \"final\": final_optimizer.state_dict()\n",
        "                  }\n",
        "              }\n",
        "              for index in range(len(model.layers)-1):\n",
        "                  checkpoint_data[\"optimizer_state_dict\"][\"pos\"][index] = pos_optimizers[index].state_dict()\n",
        "                  checkpoint_data[\"optimizer_state_dict\"][\"neg\"][index] = neg_optimizers[index].state_dict()\n",
        "\n",
        "              checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
        "\n",
        "              session.report(\n",
        "                  {\"loss\": val_loss, \"accuracy\": val_acc},\n",
        "                  checkpoint=checkpoint,\n",
        "              )\n",
        "\n",
        "    print(\"finished training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q6zgrTsvekwo",
        "outputId": "21d8d29b-7d94-412b-e4aa-d0b82d6b2b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-07-13 16:19:21 (running for 00:00:00.23)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-13_16-19-21\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "+------------------------+----------+-------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc   |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+-------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_0644a_00000 | PENDING  |       |            8 | 0.0244327   | 0.00010246  | 0.000930011 | 0.0147998   | 7.29386e-05 |       0.00138561  |       1.954e-05   |       0.000503682 |       0.000591823 |       0.000279235 |\n",
            "| train_tune_0644a_00001 | PENDING  |       |            4 | 0.00083234  | 2.40666e-05 | 0.000232531 | 0.0210133   | 0.0332063   |       4.24342e-05 |       0.00011721  |       4.205e-05   |       0.00434638  |       1.61577e-05 |\n",
            "| train_tune_0644a_00002 | PENDING  |       |            8 | 0.0421718   | 0.00169117  | 0.00615397  | 7.3675e-05  | 1.05665e-05 |       2.4264e-05  |       0.00561328  |       0.000959851 |       0.00814322  |       6.36711e-05 |\n",
            "| train_tune_0644a_00003 | PENDING  |       |            8 | 0.00311266  | 0.0786415   | 4.79182e-05 | 4.22913e-05 | 0.00556454  |       5.95056e-05 |       0.00211807  |       0.00032652  |       0.00100081  |       0.0010155   |\n",
            "| train_tune_0644a_00004 | PENDING  |       |           32 | 0.0300339   | 0.000227941 | 0.0111165   | 0.00262431  | 0.000225466 |       0.000871399 |       0.000582336 |       0.00318704  |       9.18902e-05 |       0.00196441  |\n",
            "| train_tune_0644a_00005 | PENDING  |       |           16 | 0.00065454  | 0.0201167   | 0.0653087   | 0.000597673 | 7.91612e-05 |       1.09107e-05 |       2.51601e-05 |       5.91749e-05 |       3.87853e-05 |       0.000189094 |\n",
            "| train_tune_0644a_00006 | PENDING  |       |            8 | 0.000138355 | 0.000161495 | 0.000118505 | 0.0132592   | 0.0126016   |       0.00609441  |       0.00034173  |       0.000983916 |       0.00408371  |       0.000109251 |\n",
            "| train_tune_0644a_00007 | PENDING  |       |            4 | 0.0360325   | 0.000172881 | 0.011321    | 0.00020381  | 2.60414e-05 |       0.000329893 |       1.69882e-05 |       0.000139775 |       0.000381605 |       0.000523742 |\n",
            "| train_tune_0644a_00008 | PENDING  |       |           16 | 0.000527424 | 2.36169e-05 | 0.000902231 | 0.000280967 | 0.00116858  |       0.000955229 |       2.78766e-05 |       1.17624e-05 |       0.000201656 |       3.65109e-05 |\n",
            "| train_tune_0644a_00009 | PENDING  |       |           16 | 1.15468e-05 | 1.13657e-05 | 0.000203024 | 0.0269017   | 0.00014384  |       0.00156817  |       0.000680245 |       0.000400052 |       0.000148311 |       0.00968176  |\n",
            "+------------------------+----------+-------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=3997)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(func pid=3997)\u001b[0m   warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-07-13 16:19:26 (running for 00:00:05.26)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-13_16-19-21\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_0644a_00000 | RUNNING  | 172.28.0.12:3997 |            8 | 0.0244327   | 0.00010246  | 0.000930011 | 0.0147998   | 7.29386e-05 |       0.00138561  |       1.954e-05   |       0.000503682 |       0.000591823 |       0.000279235 |\n",
            "| train_tune_0644a_00001 | PENDING  |                  |            4 | 0.00083234  | 2.40666e-05 | 0.000232531 | 0.0210133   | 0.0332063   |       4.24342e-05 |       0.00011721  |       4.205e-05   |       0.00434638  |       1.61577e-05 |\n",
            "| train_tune_0644a_00002 | PENDING  |                  |            8 | 0.0421718   | 0.00169117  | 0.00615397  | 7.3675e-05  | 1.05665e-05 |       2.4264e-05  |       0.00561328  |       0.000959851 |       0.00814322  |       6.36711e-05 |\n",
            "| train_tune_0644a_00003 | PENDING  |                  |            8 | 0.00311266  | 0.0786415   | 4.79182e-05 | 4.22913e-05 | 0.00556454  |       5.95056e-05 |       0.00211807  |       0.00032652  |       0.00100081  |       0.0010155   |\n",
            "| train_tune_0644a_00004 | PENDING  |                  |           32 | 0.0300339   | 0.000227941 | 0.0111165   | 0.00262431  | 0.000225466 |       0.000871399 |       0.000582336 |       0.00318704  |       9.18902e-05 |       0.00196441  |\n",
            "| train_tune_0644a_00005 | PENDING  |                  |           16 | 0.00065454  | 0.0201167   | 0.0653087   | 0.000597673 | 7.91612e-05 |       1.09107e-05 |       2.51601e-05 |       5.91749e-05 |       3.87853e-05 |       0.000189094 |\n",
            "| train_tune_0644a_00006 | PENDING  |                  |            8 | 0.000138355 | 0.000161495 | 0.000118505 | 0.0132592   | 0.0126016   |       0.00609441  |       0.00034173  |       0.000983916 |       0.00408371  |       0.000109251 |\n",
            "| train_tune_0644a_00007 | PENDING  |                  |            4 | 0.0360325   | 0.000172881 | 0.011321    | 0.00020381  | 2.60414e-05 |       0.000329893 |       1.69882e-05 |       0.000139775 |       0.000381605 |       0.000523742 |\n",
            "| train_tune_0644a_00008 | PENDING  |                  |           16 | 0.000527424 | 2.36169e-05 | 0.000902231 | 0.000280967 | 0.00116858  |       0.000955229 |       2.78766e-05 |       1.17624e-05 |       0.000201656 |       3.65109e-05 |\n",
            "| train_tune_0644a_00009 | PENDING  |                  |           16 | 1.15468e-05 | 1.13657e-05 | 0.000203024 | 0.0269017   | 0.00014384  |       0.00156817  |       0.000680245 |       0.000400052 |       0.000148311 |       0.00968176  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2023-07-13 16:19:31 (running for 00:00:10.32)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-13_16-19-21\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_0644a_00000 | RUNNING  | 172.28.0.12:3997 |            8 | 0.0244327   | 0.00010246  | 0.000930011 | 0.0147998   | 7.29386e-05 |       0.00138561  |       1.954e-05   |       0.000503682 |       0.000591823 |       0.000279235 |\n",
            "| train_tune_0644a_00001 | PENDING  |                  |            4 | 0.00083234  | 2.40666e-05 | 0.000232531 | 0.0210133   | 0.0332063   |       4.24342e-05 |       0.00011721  |       4.205e-05   |       0.00434638  |       1.61577e-05 |\n",
            "| train_tune_0644a_00002 | PENDING  |                  |            8 | 0.0421718   | 0.00169117  | 0.00615397  | 7.3675e-05  | 1.05665e-05 |       2.4264e-05  |       0.00561328  |       0.000959851 |       0.00814322  |       6.36711e-05 |\n",
            "| train_tune_0644a_00003 | PENDING  |                  |            8 | 0.00311266  | 0.0786415   | 4.79182e-05 | 4.22913e-05 | 0.00556454  |       5.95056e-05 |       0.00211807  |       0.00032652  |       0.00100081  |       0.0010155   |\n",
            "| train_tune_0644a_00004 | PENDING  |                  |           32 | 0.0300339   | 0.000227941 | 0.0111165   | 0.00262431  | 0.000225466 |       0.000871399 |       0.000582336 |       0.00318704  |       9.18902e-05 |       0.00196441  |\n",
            "| train_tune_0644a_00005 | PENDING  |                  |           16 | 0.00065454  | 0.0201167   | 0.0653087   | 0.000597673 | 7.91612e-05 |       1.09107e-05 |       2.51601e-05 |       5.91749e-05 |       3.87853e-05 |       0.000189094 |\n",
            "| train_tune_0644a_00006 | PENDING  |                  |            8 | 0.000138355 | 0.000161495 | 0.000118505 | 0.0132592   | 0.0126016   |       0.00609441  |       0.00034173  |       0.000983916 |       0.00408371  |       0.000109251 |\n",
            "| train_tune_0644a_00007 | PENDING  |                  |            4 | 0.0360325   | 0.000172881 | 0.011321    | 0.00020381  | 2.60414e-05 |       0.000329893 |       1.69882e-05 |       0.000139775 |       0.000381605 |       0.000523742 |\n",
            "| train_tune_0644a_00008 | PENDING  |                  |           16 | 0.000527424 | 2.36169e-05 | 0.000902231 | 0.000280967 | 0.00116858  |       0.000955229 |       2.78766e-05 |       1.17624e-05 |       0.000201656 |       3.65109e-05 |\n",
            "| train_tune_0644a_00009 | PENDING  |                  |           16 | 1.15468e-05 | 1.13657e-05 | 0.000203024 | 0.0269017   | 0.00014384  |       0.00156817  |       0.000680245 |       0.000400052 |       0.000148311 |       0.00968176  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2023-07-13 16:19:36 (running for 00:00:15.37)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-13_16-19-21\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_0644a_00000 | RUNNING  | 172.28.0.12:3997 |            8 | 0.0244327   | 0.00010246  | 0.000930011 | 0.0147998   | 7.29386e-05 |       0.00138561  |       1.954e-05   |       0.000503682 |       0.000591823 |       0.000279235 |\n",
            "| train_tune_0644a_00001 | PENDING  |                  |            4 | 0.00083234  | 2.40666e-05 | 0.000232531 | 0.0210133   | 0.0332063   |       4.24342e-05 |       0.00011721  |       4.205e-05   |       0.00434638  |       1.61577e-05 |\n",
            "| train_tune_0644a_00002 | PENDING  |                  |            8 | 0.0421718   | 0.00169117  | 0.00615397  | 7.3675e-05  | 1.05665e-05 |       2.4264e-05  |       0.00561328  |       0.000959851 |       0.00814322  |       6.36711e-05 |\n",
            "| train_tune_0644a_00003 | PENDING  |                  |            8 | 0.00311266  | 0.0786415   | 4.79182e-05 | 4.22913e-05 | 0.00556454  |       5.95056e-05 |       0.00211807  |       0.00032652  |       0.00100081  |       0.0010155   |\n",
            "| train_tune_0644a_00004 | PENDING  |                  |           32 | 0.0300339   | 0.000227941 | 0.0111165   | 0.00262431  | 0.000225466 |       0.000871399 |       0.000582336 |       0.00318704  |       9.18902e-05 |       0.00196441  |\n",
            "| train_tune_0644a_00005 | PENDING  |                  |           16 | 0.00065454  | 0.0201167   | 0.0653087   | 0.000597673 | 7.91612e-05 |       1.09107e-05 |       2.51601e-05 |       5.91749e-05 |       3.87853e-05 |       0.000189094 |\n",
            "| train_tune_0644a_00006 | PENDING  |                  |            8 | 0.000138355 | 0.000161495 | 0.000118505 | 0.0132592   | 0.0126016   |       0.00609441  |       0.00034173  |       0.000983916 |       0.00408371  |       0.000109251 |\n",
            "| train_tune_0644a_00007 | PENDING  |                  |            4 | 0.0360325   | 0.000172881 | 0.011321    | 0.00020381  | 2.60414e-05 |       0.000329893 |       1.69882e-05 |       0.000139775 |       0.000381605 |       0.000523742 |\n",
            "| train_tune_0644a_00008 | PENDING  |                  |           16 | 0.000527424 | 2.36169e-05 | 0.000902231 | 0.000280967 | 0.00116858  |       0.000955229 |       2.78766e-05 |       1.17624e-05 |       0.000201656 |       3.65109e-05 |\n",
            "| train_tune_0644a_00009 | PENDING  |                  |           16 | 1.15468e-05 | 1.13657e-05 | 0.000203024 | 0.0269017   | 0.00014384  |       0.00156817  |       0.000680245 |       0.000400052 |       0.000148311 |       0.00968176  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2023-07-13 16:19:41 (running for 00:00:20.42)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-13_16-19-21\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_0644a_00000 | RUNNING  | 172.28.0.12:3997 |            8 | 0.0244327   | 0.00010246  | 0.000930011 | 0.0147998   | 7.29386e-05 |       0.00138561  |       1.954e-05   |       0.000503682 |       0.000591823 |       0.000279235 |\n",
            "| train_tune_0644a_00001 | PENDING  |                  |            4 | 0.00083234  | 2.40666e-05 | 0.000232531 | 0.0210133   | 0.0332063   |       4.24342e-05 |       0.00011721  |       4.205e-05   |       0.00434638  |       1.61577e-05 |\n",
            "| train_tune_0644a_00002 | PENDING  |                  |            8 | 0.0421718   | 0.00169117  | 0.00615397  | 7.3675e-05  | 1.05665e-05 |       2.4264e-05  |       0.00561328  |       0.000959851 |       0.00814322  |       6.36711e-05 |\n",
            "| train_tune_0644a_00003 | PENDING  |                  |            8 | 0.00311266  | 0.0786415   | 4.79182e-05 | 4.22913e-05 | 0.00556454  |       5.95056e-05 |       0.00211807  |       0.00032652  |       0.00100081  |       0.0010155   |\n",
            "| train_tune_0644a_00004 | PENDING  |                  |           32 | 0.0300339   | 0.000227941 | 0.0111165   | 0.00262431  | 0.000225466 |       0.000871399 |       0.000582336 |       0.00318704  |       9.18902e-05 |       0.00196441  |\n",
            "| train_tune_0644a_00005 | PENDING  |                  |           16 | 0.00065454  | 0.0201167   | 0.0653087   | 0.000597673 | 7.91612e-05 |       1.09107e-05 |       2.51601e-05 |       5.91749e-05 |       3.87853e-05 |       0.000189094 |\n",
            "| train_tune_0644a_00006 | PENDING  |                  |            8 | 0.000138355 | 0.000161495 | 0.000118505 | 0.0132592   | 0.0126016   |       0.00609441  |       0.00034173  |       0.000983916 |       0.00408371  |       0.000109251 |\n",
            "| train_tune_0644a_00007 | PENDING  |                  |            4 | 0.0360325   | 0.000172881 | 0.011321    | 0.00020381  | 2.60414e-05 |       0.000329893 |       1.69882e-05 |       0.000139775 |       0.000381605 |       0.000523742 |\n",
            "| train_tune_0644a_00008 | PENDING  |                  |           16 | 0.000527424 | 2.36169e-05 | 0.000902231 | 0.000280967 | 0.00116858  |       0.000955229 |       2.78766e-05 |       1.17624e-05 |       0.000201656 |       3.65109e-05 |\n",
            "| train_tune_0644a_00009 | PENDING  |                  |           16 | 1.15468e-05 | 1.13657e-05 | 0.000203024 | 0.0269017   | 0.00014384  |       0.00156817  |       0.000680245 |       0.000400052 |       0.000148311 |       0.00968176  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2023-07-13 16:19:46 (running for 00:00:25.47)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-13_16-19-21\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_0644a_00000 | RUNNING  | 172.28.0.12:3997 |            8 | 0.0244327   | 0.00010246  | 0.000930011 | 0.0147998   | 7.29386e-05 |       0.00138561  |       1.954e-05   |       0.000503682 |       0.000591823 |       0.000279235 |\n",
            "| train_tune_0644a_00001 | PENDING  |                  |            4 | 0.00083234  | 2.40666e-05 | 0.000232531 | 0.0210133   | 0.0332063   |       4.24342e-05 |       0.00011721  |       4.205e-05   |       0.00434638  |       1.61577e-05 |\n",
            "| train_tune_0644a_00002 | PENDING  |                  |            8 | 0.0421718   | 0.00169117  | 0.00615397  | 7.3675e-05  | 1.05665e-05 |       2.4264e-05  |       0.00561328  |       0.000959851 |       0.00814322  |       6.36711e-05 |\n",
            "| train_tune_0644a_00003 | PENDING  |                  |            8 | 0.00311266  | 0.0786415   | 4.79182e-05 | 4.22913e-05 | 0.00556454  |       5.95056e-05 |       0.00211807  |       0.00032652  |       0.00100081  |       0.0010155   |\n",
            "| train_tune_0644a_00004 | PENDING  |                  |           32 | 0.0300339   | 0.000227941 | 0.0111165   | 0.00262431  | 0.000225466 |       0.000871399 |       0.000582336 |       0.00318704  |       9.18902e-05 |       0.00196441  |\n",
            "| train_tune_0644a_00005 | PENDING  |                  |           16 | 0.00065454  | 0.0201167   | 0.0653087   | 0.000597673 | 7.91612e-05 |       1.09107e-05 |       2.51601e-05 |       5.91749e-05 |       3.87853e-05 |       0.000189094 |\n",
            "| train_tune_0644a_00006 | PENDING  |                  |            8 | 0.000138355 | 0.000161495 | 0.000118505 | 0.0132592   | 0.0126016   |       0.00609441  |       0.00034173  |       0.000983916 |       0.00408371  |       0.000109251 |\n",
            "| train_tune_0644a_00007 | PENDING  |                  |            4 | 0.0360325   | 0.000172881 | 0.011321    | 0.00020381  | 2.60414e-05 |       0.000329893 |       1.69882e-05 |       0.000139775 |       0.000381605 |       0.000523742 |\n",
            "| train_tune_0644a_00008 | PENDING  |                  |           16 | 0.000527424 | 2.36169e-05 | 0.000902231 | 0.000280967 | 0.00116858  |       0.000955229 |       2.78766e-05 |       1.17624e-05 |       0.000201656 |       3.65109e-05 |\n",
            "| train_tune_0644a_00009 | PENDING  |                  |           16 | 1.15468e-05 | 1.13657e-05 | 0.000203024 | 0.0269017   | 0.00014384  |       0.00156817  |       0.000680245 |       0.000400052 |       0.000148311 |       0.00968176  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2023-07-13 16:19:51 (running for 00:00:30.53)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-13_16-19-21\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_0644a_00000 | RUNNING  | 172.28.0.12:3997 |            8 | 0.0244327   | 0.00010246  | 0.000930011 | 0.0147998   | 7.29386e-05 |       0.00138561  |       1.954e-05   |       0.000503682 |       0.000591823 |       0.000279235 |\n",
            "| train_tune_0644a_00001 | PENDING  |                  |            4 | 0.00083234  | 2.40666e-05 | 0.000232531 | 0.0210133   | 0.0332063   |       4.24342e-05 |       0.00011721  |       4.205e-05   |       0.00434638  |       1.61577e-05 |\n",
            "| train_tune_0644a_00002 | PENDING  |                  |            8 | 0.0421718   | 0.00169117  | 0.00615397  | 7.3675e-05  | 1.05665e-05 |       2.4264e-05  |       0.00561328  |       0.000959851 |       0.00814322  |       6.36711e-05 |\n",
            "| train_tune_0644a_00003 | PENDING  |                  |            8 | 0.00311266  | 0.0786415   | 4.79182e-05 | 4.22913e-05 | 0.00556454  |       5.95056e-05 |       0.00211807  |       0.00032652  |       0.00100081  |       0.0010155   |\n",
            "| train_tune_0644a_00004 | PENDING  |                  |           32 | 0.0300339   | 0.000227941 | 0.0111165   | 0.00262431  | 0.000225466 |       0.000871399 |       0.000582336 |       0.00318704  |       9.18902e-05 |       0.00196441  |\n",
            "| train_tune_0644a_00005 | PENDING  |                  |           16 | 0.00065454  | 0.0201167   | 0.0653087   | 0.000597673 | 7.91612e-05 |       1.09107e-05 |       2.51601e-05 |       5.91749e-05 |       3.87853e-05 |       0.000189094 |\n",
            "| train_tune_0644a_00006 | PENDING  |                  |            8 | 0.000138355 | 0.000161495 | 0.000118505 | 0.0132592   | 0.0126016   |       0.00609441  |       0.00034173  |       0.000983916 |       0.00408371  |       0.000109251 |\n",
            "| train_tune_0644a_00007 | PENDING  |                  |            4 | 0.0360325   | 0.000172881 | 0.011321    | 0.00020381  | 2.60414e-05 |       0.000329893 |       1.69882e-05 |       0.000139775 |       0.000381605 |       0.000523742 |\n",
            "| train_tune_0644a_00008 | PENDING  |                  |           16 | 0.000527424 | 2.36169e-05 | 0.000902231 | 0.000280967 | 0.00116858  |       0.000955229 |       2.78766e-05 |       1.17624e-05 |       0.000201656 |       3.65109e-05 |\n",
            "| train_tune_0644a_00009 | PENDING  |                  |           16 | 1.15468e-05 | 1.13657e-05 | 0.000203024 | 0.0269017   | 0.00014384  |       0.00156817  |       0.000680245 |       0.000400052 |       0.000148311 |       0.00968176  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=3997)\u001b[0m Layer 1, Epoch [1/15], Step [1687/6750], Positive Goodness: 0.7662, Negative Goodness: 0.7721\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=3997)\u001b[0m Final Layer, Epoch [1/15], Step [1687/6750], Loss: 0.8531\n",
            "== Status ==\n",
            "Current time: 2023-07-13 16:19:56 (running for 00:00:35.58)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-13_16-19-21\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_0644a_00000 | RUNNING  | 172.28.0.12:3997 |            8 | 0.0244327   | 0.00010246  | 0.000930011 | 0.0147998   | 7.29386e-05 |       0.00138561  |       1.954e-05   |       0.000503682 |       0.000591823 |       0.000279235 |\n",
            "| train_tune_0644a_00001 | PENDING  |                  |            4 | 0.00083234  | 2.40666e-05 | 0.000232531 | 0.0210133   | 0.0332063   |       4.24342e-05 |       0.00011721  |       4.205e-05   |       0.00434638  |       1.61577e-05 |\n",
            "| train_tune_0644a_00002 | PENDING  |                  |            8 | 0.0421718   | 0.00169117  | 0.00615397  | 7.3675e-05  | 1.05665e-05 |       2.4264e-05  |       0.00561328  |       0.000959851 |       0.00814322  |       6.36711e-05 |\n",
            "| train_tune_0644a_00003 | PENDING  |                  |            8 | 0.00311266  | 0.0786415   | 4.79182e-05 | 4.22913e-05 | 0.00556454  |       5.95056e-05 |       0.00211807  |       0.00032652  |       0.00100081  |       0.0010155   |\n",
            "| train_tune_0644a_00004 | PENDING  |                  |           32 | 0.0300339   | 0.000227941 | 0.0111165   | 0.00262431  | 0.000225466 |       0.000871399 |       0.000582336 |       0.00318704  |       9.18902e-05 |       0.00196441  |\n",
            "| train_tune_0644a_00005 | PENDING  |                  |           16 | 0.00065454  | 0.0201167   | 0.0653087   | 0.000597673 | 7.91612e-05 |       1.09107e-05 |       2.51601e-05 |       5.91749e-05 |       3.87853e-05 |       0.000189094 |\n",
            "| train_tune_0644a_00006 | PENDING  |                  |            8 | 0.000138355 | 0.000161495 | 0.000118505 | 0.0132592   | 0.0126016   |       0.00609441  |       0.00034173  |       0.000983916 |       0.00408371  |       0.000109251 |\n",
            "| train_tune_0644a_00007 | PENDING  |                  |            4 | 0.0360325   | 0.000172881 | 0.011321    | 0.00020381  | 2.60414e-05 |       0.000329893 |       1.69882e-05 |       0.000139775 |       0.000381605 |       0.000523742 |\n",
            "| train_tune_0644a_00008 | PENDING  |                  |           16 | 0.000527424 | 2.36169e-05 | 0.000902231 | 0.000280967 | 0.00116858  |       0.000955229 |       2.78766e-05 |       1.17624e-05 |       0.000201656 |       3.65109e-05 |\n",
            "| train_tune_0644a_00009 | PENDING  |                  |           16 | 1.15468e-05 | 1.13657e-05 | 0.000203024 | 0.0269017   | 0.00014384  |       0.00156817  |       0.000680245 |       0.000400052 |       0.000148311 |       0.00968176  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2023-07-13 16:20:02 (running for 00:00:40.64)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-13_16-19-21\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_0644a_00000 | RUNNING  | 172.28.0.12:3997 |            8 | 0.0244327   | 0.00010246  | 0.000930011 | 0.0147998   | 7.29386e-05 |       0.00138561  |       1.954e-05   |       0.000503682 |       0.000591823 |       0.000279235 |\n",
            "| train_tune_0644a_00001 | PENDING  |                  |            4 | 0.00083234  | 2.40666e-05 | 0.000232531 | 0.0210133   | 0.0332063   |       4.24342e-05 |       0.00011721  |       4.205e-05   |       0.00434638  |       1.61577e-05 |\n",
            "| train_tune_0644a_00002 | PENDING  |                  |            8 | 0.0421718   | 0.00169117  | 0.00615397  | 7.3675e-05  | 1.05665e-05 |       2.4264e-05  |       0.00561328  |       0.000959851 |       0.00814322  |       6.36711e-05 |\n",
            "| train_tune_0644a_00003 | PENDING  |                  |            8 | 0.00311266  | 0.0786415   | 4.79182e-05 | 4.22913e-05 | 0.00556454  |       5.95056e-05 |       0.00211807  |       0.00032652  |       0.00100081  |       0.0010155   |\n",
            "| train_tune_0644a_00004 | PENDING  |                  |           32 | 0.0300339   | 0.000227941 | 0.0111165   | 0.00262431  | 0.000225466 |       0.000871399 |       0.000582336 |       0.00318704  |       9.18902e-05 |       0.00196441  |\n",
            "| train_tune_0644a_00005 | PENDING  |                  |           16 | 0.00065454  | 0.0201167   | 0.0653087   | 0.000597673 | 7.91612e-05 |       1.09107e-05 |       2.51601e-05 |       5.91749e-05 |       3.87853e-05 |       0.000189094 |\n",
            "| train_tune_0644a_00006 | PENDING  |                  |            8 | 0.000138355 | 0.000161495 | 0.000118505 | 0.0132592   | 0.0126016   |       0.00609441  |       0.00034173  |       0.000983916 |       0.00408371  |       0.000109251 |\n",
            "| train_tune_0644a_00007 | PENDING  |                  |            4 | 0.0360325   | 0.000172881 | 0.011321    | 0.00020381  | 2.60414e-05 |       0.000329893 |       1.69882e-05 |       0.000139775 |       0.000381605 |       0.000523742 |\n",
            "| train_tune_0644a_00008 | PENDING  |                  |           16 | 0.000527424 | 2.36169e-05 | 0.000902231 | 0.000280967 | 0.00116858  |       0.000955229 |       2.78766e-05 |       1.17624e-05 |       0.000201656 |       3.65109e-05 |\n",
            "| train_tune_0644a_00009 | PENDING  |                  |           16 | 1.15468e-05 | 1.13657e-05 | 0.000203024 | 0.0269017   | 0.00014384  |       0.00156817  |       0.000680245 |       0.000400052 |       0.000148311 |       0.00968176  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2023-07-13 16:20:07 (running for 00:00:45.69)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-13_16-19-21\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_0644a_00000 | RUNNING  | 172.28.0.12:3997 |            8 | 0.0244327   | 0.00010246  | 0.000930011 | 0.0147998   | 7.29386e-05 |       0.00138561  |       1.954e-05   |       0.000503682 |       0.000591823 |       0.000279235 |\n",
            "| train_tune_0644a_00001 | PENDING  |                  |            4 | 0.00083234  | 2.40666e-05 | 0.000232531 | 0.0210133   | 0.0332063   |       4.24342e-05 |       0.00011721  |       4.205e-05   |       0.00434638  |       1.61577e-05 |\n",
            "| train_tune_0644a_00002 | PENDING  |                  |            8 | 0.0421718   | 0.00169117  | 0.00615397  | 7.3675e-05  | 1.05665e-05 |       2.4264e-05  |       0.00561328  |       0.000959851 |       0.00814322  |       6.36711e-05 |\n",
            "| train_tune_0644a_00003 | PENDING  |                  |            8 | 0.00311266  | 0.0786415   | 4.79182e-05 | 4.22913e-05 | 0.00556454  |       5.95056e-05 |       0.00211807  |       0.00032652  |       0.00100081  |       0.0010155   |\n",
            "| train_tune_0644a_00004 | PENDING  |                  |           32 | 0.0300339   | 0.000227941 | 0.0111165   | 0.00262431  | 0.000225466 |       0.000871399 |       0.000582336 |       0.00318704  |       9.18902e-05 |       0.00196441  |\n",
            "| train_tune_0644a_00005 | PENDING  |                  |           16 | 0.00065454  | 0.0201167   | 0.0653087   | 0.000597673 | 7.91612e-05 |       1.09107e-05 |       2.51601e-05 |       5.91749e-05 |       3.87853e-05 |       0.000189094 |\n",
            "| train_tune_0644a_00006 | PENDING  |                  |            8 | 0.000138355 | 0.000161495 | 0.000118505 | 0.0132592   | 0.0126016   |       0.00609441  |       0.00034173  |       0.000983916 |       0.00408371  |       0.000109251 |\n",
            "| train_tune_0644a_00007 | PENDING  |                  |            4 | 0.0360325   | 0.000172881 | 0.011321    | 0.00020381  | 2.60414e-05 |       0.000329893 |       1.69882e-05 |       0.000139775 |       0.000381605 |       0.000523742 |\n",
            "| train_tune_0644a_00008 | PENDING  |                  |           16 | 0.000527424 | 2.36169e-05 | 0.000902231 | 0.000280967 | 0.00116858  |       0.000955229 |       2.78766e-05 |       1.17624e-05 |       0.000201656 |       3.65109e-05 |\n",
            "| train_tune_0644a_00009 | PENDING  |                  |           16 | 1.15468e-05 | 1.13657e-05 | 0.000203024 | 0.0269017   | 0.00014384  |       0.00156817  |       0.000680245 |       0.000400052 |       0.000148311 |       0.00968176  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=3997)\u001b[0m Epoch [1/15], Step [1687/6750], Test Accuracy: 0.0763\n",
            "\u001b[2m\u001b[36m(func pid=3997)\u001b[0m Layer 3, Epoch [1/15], Step [1687/6750], Positive Goodness: 0.7083, Negative Goodness: 0.7079\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "Result for train_tune_0644a_00000:\n",
            "  accuracy: tensor(0.0763, device='cuda:0')\n",
            "  date: 2023-07-13_16-20-09\n",
            "  done: false\n",
            "  hostname: 63cfe84d5d12\n",
            "  iterations_since_restore: 1\n",
            "  loss: tensor(1.2079, device='cuda:0')\n",
            "  node_ip: 172.28.0.12\n",
            "  pid: 3997\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 44.78590488433838\n",
            "  time_this_iter_s: 44.78590488433838\n",
            "  time_total_s: 44.78590488433838\n",
            "  timestamp: 1689265209\n",
            "  training_iteration: 1\n",
            "  trial_id: 0644a_00000\n",
            "  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-0f8d10e7137e>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# You can change the number of GPUs per trial here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-0f8d10e7137e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     result = tune.run(\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mresources_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, checkpoint_keep_all_ranks, checkpoint_upload_from_workers, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, trial_executor, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                 \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVerbosity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1_EXPERIMENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                     \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mair_verbosity\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36m_report_progress\u001b[0;34m(runner, reporter, done)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0msched_debug_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler_alg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mused_resources_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_used_resources_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msched_debug_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_resources_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/schedulers/async_hyperband.py\u001b[0m in \u001b[0;36mdebug_string\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Using AsyncHyperBand: num_stopped={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_stopped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_brackets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/schedulers/async_hyperband.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Using AsyncHyperBand: num_stopped={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_stopped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_brackets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/schedulers/async_hyperband.py\u001b[0m in \u001b[0;36mdebug_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# TODO: fix up the output for this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         iters = \" | \".join(\n\u001b[0;32m--> 257\u001b[0;31m             [\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;34m\"Iter {:.3f}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmilestone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecorded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmilestone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecorded\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rungs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/schedulers/async_hyperband.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    256\u001b[0m         iters = \" | \".join(\n\u001b[1;32m    257\u001b[0m             [\n\u001b[0;32m--> 258\u001b[0;31m                 \u001b[0;34m\"Iter {:.3f}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmilestone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecorded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmilestone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecorded\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rungs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/schedulers/async_hyperband.py\u001b[0m in \u001b[0;36mcutoff\u001b[0;34m(self, recorded)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrecorded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecorded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_iter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_rew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mnanpercentile\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py\u001b[0m in \u001b[0;36mnanpercentile\u001b[0;34m(a, q, axis, out, overwrite_input, method, keepdims, interpolation)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             method, interpolation, \"nanpercentile\")\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_divide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m     \u001b[0;31m# undo any decay that the ufunc performed (see gh-13105)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ],
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "\n",
        "    config = {\n",
        "    \"batch_size\": tune.choice([4,8,16,32,64]),\n",
        "    \"num_epochs\": max_num_epochs,\n",
        "    \"final_loss\": ceLoss,\n",
        "    \"lrs\": [tune.loguniform(1e-5, 1e-1),tune.loguniform(1e-5, 1e-1),tune.loguniform(1e-5, 1e-1),tune.loguniform(1e-5, 1e-1),tune.loguniform(1e-5, 1e-1)],\n",
        "    \"momentums\": [0.9,0.9,0.9,0.9,0.9],\n",
        "    \"weight_decays\": [tune.loguniform(1e-5, 1e-2),tune.loguniform(1e-5, 1e-2),tune.loguniform(1e-5, 1e-2),tune.loguniform(1e-5, 1e-2),tune.loguniform(1e-5, 1e-2)],\n",
        "    \"thresholds\": [50,1000,1000,1000],\n",
        "    \"records_per_epoch\": 4,\n",
        "    \"supervised\" : True\n",
        "    }\n",
        "\n",
        "    _, test_data = load_mnist(data_dir)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=64, shuffle=True, num_workers=8\n",
        "    )\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2,\n",
        "    )\n",
        "\n",
        "    result = tune.run(\n",
        "        partial(train_tune, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "    )\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(f\"Best trial config: {best_trial.config}\")\n",
        "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
        "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
        "\n",
        "    best_trained_model = net(784)\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if gpus_per_trial > 1:\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    best_trained_model.to(device)\n",
        "\n",
        "    best_checkpoint = best_trial.checkpoint.to_air_checkpoint()\n",
        "    best_checkpoint_data = best_checkpoint.to_dict()\n",
        "\n",
        "    best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n",
        "\n",
        "    test_acc = test(best_trained_model, test_loader, config[\"final_loss\"])\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the number of GPUs per trial here:\n",
        "    main(num_samples=10, max_num_epochs=20, gpus_per_trial=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi5CDidzl5jN"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "-ZLMO_AQ2Cj5"
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "def train(num_epochs, model, train_loader, test_loader, final_loss, pos_optimizers, neg_optimizers, final_optimizer, thresholds, records_per_epoch, supervised = True):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # variables for training output\n",
        "    total_pos_goodnesses,total_neg_goodnesses = [],[]\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "    total_step = len(train_loader)\n",
        "    record_period = total_step//records_per_epoch\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_pos_goodness,total_neg_goodness = [0]*(len(model.layers)-1),[0]*(len(model.layers)-1)\n",
        "        total_loss = 0\n",
        "\n",
        "        for i,(x,y) in enumerate(train_loader): # iterate through dataset\n",
        "            x,y = x.to(device),y.to(device)\n",
        "            if not supervised: # unsupervised training input\n",
        "              pos_imgs = x.squeeze()\n",
        "              neg_imgs = generate_negative_example().squeeze().to(device)\n",
        "            else: # supervised learning input\n",
        "              pos_imgs = model.construct_supervised_example(x,y,True)\n",
        "              neg_imgs = model.construct_supervised_example(x,y,False)\n",
        "\n",
        "            # intermediate variables\n",
        "            pos_interms = []\n",
        "            pos_interm = pos_imgs.clone()\n",
        "            neg_interm = neg_imgs.clone()\n",
        "\n",
        "            # iterate over intermediate layers\n",
        "            for index in range(len(model.layers)-1):\n",
        "              layer = model.layers[index]\n",
        "\n",
        "              # positive pass\n",
        "              pos_output = layer(pos_interm)\n",
        "              pos_goodness = model.criterion(pos_output, threshold=thresholds[index])\n",
        "              pos_interm = model.ln(pos_output).detach()\n",
        "              pos_interms.append(pos_interm.clone())\n",
        "\n",
        "              # update variables\n",
        "              total_pos_goodness[index] += pos_goodness\n",
        "\n",
        "              # clear gradients for this training step\n",
        "              pos_optimizers[index].zero_grad()\n",
        "\n",
        "              # take gradient step\n",
        "              pos_goodness.backward()\n",
        "              pos_optimizers[index].step()\n",
        "\n",
        "              # negative pass\n",
        "              neg_output = layer(neg_interm)\n",
        "              neg_goodness = model.criterion(neg_output, threshold=thresholds[index])\n",
        "              neg_interm = model.ln(neg_output).detach()\n",
        "\n",
        "              total_neg_goodness[index] += neg_goodness\n",
        "              neg_optimizers[index].zero_grad()\n",
        "              neg_goodness.backward()\n",
        "              neg_optimizers[index].step()\n",
        "\n",
        "              # update variables\n",
        "              total_pos_goodness[index] += pos_goodness\n",
        "              total_neg_goodness[index] += neg_goodness\n",
        "\n",
        "              # check progress\n",
        "              if (i+1) % (record_period) == 0:\n",
        "                  avg_pos_goodness = total_pos_goodness[index]/(record_period)\n",
        "                  avg_neg_goodness = total_neg_goodness[index]/(record_period)\n",
        "                  if len(total_pos_goodnesses)>index:\n",
        "                    total_pos_goodnesses[index].append(avg_pos_goodness)\n",
        "                  else:\n",
        "                    total_pos_goodnesses.append([avg_pos_goodness])\n",
        "                  if len(total_neg_goodnesses)>index:\n",
        "                    total_neg_goodnesses[index].append(avg_neg_goodness)\n",
        "                  else:\n",
        "                    total_neg_goodnesses.append([avg_neg_goodness])\n",
        "\n",
        "                  print ('Layer {}, Epoch [{}/{}], Step [{}/{}], Positive Goodness: {:.4f}, Negative Goodness: {:.4f}'\n",
        "                        .format(index, epoch + 1, num_epochs, i + 1, total_step, avg_pos_goodness, avg_neg_goodness))\n",
        "                  total_pos_goodness[index],total_neg_goodness[index] = 0,0\n",
        "\n",
        "            # output layer\n",
        "            final_input = torch.cat(pos_interms[1:],dim=1)\n",
        "            labels = model.layers[-1](final_input)\n",
        "            loss = ceLoss(labels,y)\n",
        "            total_loss += loss\n",
        "\n",
        "            final_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            final_optimizer.step()\n",
        "\n",
        "            if (i+1) % record_period == 0:\n",
        "              avg_loss = total_loss/record_period\n",
        "              train_losses.append(avg_loss)\n",
        "\n",
        "              print ('Final Layer, Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                    .format(epoch + 1, num_epochs, i + 1, total_step, avg_loss))\n",
        "              total_loss = 0\n",
        "\n",
        "              # test model on test set\n",
        "              test_loss, test_acc = test(model, test_loader, final_loss)\n",
        "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Test Accuracy: {:.4f}'\n",
        "                    .format(epoch + 1, num_epochs, i + 1, total_step, test_loss, test_acc))\n",
        "              test_losses.append(test_loss)\n",
        "              test_accuracies.append(test_acc)\n",
        "\n",
        "        pass\n",
        "\n",
        "        # save model every 5 epochs\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            torch.save(model.state_dict(), 'model_v1.1_epoch{}.pth'.format(epoch+1))\n",
        "        pass\n",
        "    return total_pos_goodnesses, total_neg_goodnesses, train_losses, test_losses, test_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2baZqMWtom8",
        "outputId": "60389fd7-85b6-4fc0-ba1c-ee19b3469b7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0, Epoch [1/15], Step [234/938], Positive Goodness: 1.8283, Negative Goodness: 1.8280\n",
            "Layer 1, Epoch [1/15], Step [234/938], Positive Goodness: 1.9875, Negative Goodness: 1.9971\n",
            "Layer 2, Epoch [1/15], Step [234/938], Positive Goodness: 1.9882, Negative Goodness: 1.9943\n",
            "Layer 3, Epoch [1/15], Step [234/938], Positive Goodness: 0.0390, Negative Goodness: 0.0412\n",
            "Final Layer, Epoch [1/15], Step [234/938], Loss: 0.3783\n",
            "Epoch [1/15], Step [234/938], Loss: 0.3777, Test Accuracy: 0.0273\n",
            "Layer 0, Epoch [1/15], Step [468/938], Positive Goodness: 1.8525, Negative Goodness: 1.7705\n",
            "Layer 1, Epoch [1/15], Step [468/938], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "Layer 2, Epoch [1/15], Step [468/938], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "Layer 3, Epoch [1/15], Step [468/938], Positive Goodness: 0.0056, Negative Goodness: 0.0064\n",
            "Final Layer, Epoch [1/15], Step [468/938], Loss: 0.1284\n",
            "Epoch [1/15], Step [468/938], Loss: 0.3520, Test Accuracy: 0.0746\n",
            "Layer 0, Epoch [1/15], Step [702/938], Positive Goodness: 1.8204, Negative Goodness: 1.6424\n",
            "Layer 1, Epoch [1/15], Step [702/938], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "Layer 2, Epoch [1/15], Step [702/938], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "Layer 3, Epoch [1/15], Step [702/938], Positive Goodness: 0.0065, Negative Goodness: 0.0065\n",
            "Final Layer, Epoch [1/15], Step [702/938], Loss: 0.0844\n",
            "Epoch [1/15], Step [702/938], Loss: 0.3549, Test Accuracy: 0.1022\n",
            "Layer 0, Epoch [1/15], Step [936/938], Positive Goodness: 1.7930, Negative Goodness: 1.5657\n",
            "Layer 1, Epoch [1/15], Step [936/938], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "Layer 2, Epoch [1/15], Step [936/938], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "Layer 3, Epoch [1/15], Step [936/938], Positive Goodness: 0.0180, Negative Goodness: 0.0190\n",
            "Final Layer, Epoch [1/15], Step [936/938], Loss: 0.0685\n",
            "Epoch [1/15], Step [936/938], Loss: 0.3406, Test Accuracy: 0.0994\n",
            "Layer 0, Epoch [2/15], Step [234/938], Positive Goodness: 1.7342, Negative Goodness: 1.4554\n",
            "Layer 1, Epoch [2/15], Step [234/938], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "Layer 2, Epoch [2/15], Step [234/938], Positive Goodness: 1.9051, Negative Goodness: 1.8940\n",
            "Layer 3, Epoch [2/15], Step [234/938], Positive Goodness: 0.1428, Negative Goodness: 0.1428\n",
            "Final Layer, Epoch [2/15], Step [234/938], Loss: 0.0519\n",
            "Epoch [2/15], Step [234/938], Loss: 0.3554, Test Accuracy: 0.0406\n",
            "Layer 0, Epoch [2/15], Step [468/938], Positive Goodness: 1.6956, Negative Goodness: 1.3270\n",
            "Layer 1, Epoch [2/15], Step [468/938], Positive Goodness: 1.5634, Negative Goodness: 1.5820\n",
            "Layer 2, Epoch [2/15], Step [468/938], Positive Goodness: 0.4260, Negative Goodness: 0.4440\n",
            "Layer 3, Epoch [2/15], Step [468/938], Positive Goodness: 0.7780, Negative Goodness: 0.8311\n",
            "Final Layer, Epoch [2/15], Step [468/938], Loss: 0.0711\n",
            "Epoch [2/15], Step [468/938], Loss: 0.4355, Test Accuracy: 0.0985\n",
            "Layer 0, Epoch [2/15], Step [702/938], Positive Goodness: 1.7478, Negative Goodness: 1.3051\n",
            "Layer 1, Epoch [2/15], Step [702/938], Positive Goodness: 1.8999, Negative Goodness: 1.8414\n",
            "Layer 2, Epoch [2/15], Step [702/938], Positive Goodness: 0.8997, Negative Goodness: 0.9171\n",
            "Layer 3, Epoch [2/15], Step [702/938], Positive Goodness: 1.0510, Negative Goodness: 1.0687\n",
            "Final Layer, Epoch [2/15], Step [702/938], Loss: 0.0696\n",
            "Epoch [2/15], Step [702/938], Loss: 0.4740, Test Accuracy: 0.0923\n",
            "Layer 0, Epoch [2/15], Step [936/938], Positive Goodness: 1.7722, Negative Goodness: 1.2357\n",
            "Layer 1, Epoch [2/15], Step [936/938], Positive Goodness: 1.5392, Negative Goodness: 1.4376\n",
            "Layer 2, Epoch [2/15], Step [936/938], Positive Goodness: 0.5912, Negative Goodness: 0.5778\n",
            "Layer 3, Epoch [2/15], Step [936/938], Positive Goodness: 0.8493, Negative Goodness: 0.7786\n",
            "Final Layer, Epoch [2/15], Step [936/938], Loss: 0.0662\n",
            "Epoch [2/15], Step [936/938], Loss: 0.4675, Test Accuracy: 0.1269\n",
            "Layer 0, Epoch [3/15], Step [234/938], Positive Goodness: 1.7849, Negative Goodness: 1.1766\n",
            "Layer 1, Epoch [3/15], Step [234/938], Positive Goodness: 1.7440, Negative Goodness: 1.6155\n",
            "Layer 2, Epoch [3/15], Step [234/938], Positive Goodness: 0.1644, Negative Goodness: 0.1536\n",
            "Layer 3, Epoch [3/15], Step [234/938], Positive Goodness: 0.7135, Negative Goodness: 0.6518\n",
            "Final Layer, Epoch [3/15], Step [234/938], Loss: 0.0603\n",
            "Epoch [3/15], Step [234/938], Loss: 0.4582, Test Accuracy: 0.0863\n",
            "Layer 0, Epoch [3/15], Step [468/938], Positive Goodness: 1.7976, Negative Goodness: 1.1346\n",
            "Layer 1, Epoch [3/15], Step [468/938], Positive Goodness: 1.9079, Negative Goodness: 1.7516\n",
            "Layer 2, Epoch [3/15], Step [468/938], Positive Goodness: 0.0004, Negative Goodness: 0.0028\n",
            "Layer 3, Epoch [3/15], Step [468/938], Positive Goodness: 0.5073, Negative Goodness: 0.3991\n",
            "Final Layer, Epoch [3/15], Step [468/938], Loss: 0.0511\n",
            "Epoch [3/15], Step [468/938], Loss: 0.4031, Test Accuracy: 0.1140\n",
            "Layer 0, Epoch [3/15], Step [702/938], Positive Goodness: 1.8199, Negative Goodness: 1.1060\n",
            "Layer 1, Epoch [3/15], Step [702/938], Positive Goodness: 1.9225, Negative Goodness: 1.7653\n",
            "Layer 2, Epoch [3/15], Step [702/938], Positive Goodness: 0.0001, Negative Goodness: 0.0019\n",
            "Layer 3, Epoch [3/15], Step [702/938], Positive Goodness: 0.6304, Negative Goodness: 0.4790\n",
            "Final Layer, Epoch [3/15], Step [702/938], Loss: 0.0411\n",
            "Epoch [3/15], Step [702/938], Loss: 0.5312, Test Accuracy: 0.1158\n",
            "Layer 0, Epoch [3/15], Step [936/938], Positive Goodness: 1.8313, Negative Goodness: 1.1167\n",
            "Layer 1, Epoch [3/15], Step [936/938], Positive Goodness: 1.9198, Negative Goodness: 1.7770\n",
            "Layer 2, Epoch [3/15], Step [936/938], Positive Goodness: 0.0000, Negative Goodness: 0.0023\n",
            "Layer 3, Epoch [3/15], Step [936/938], Positive Goodness: 0.7009, Negative Goodness: 0.5099\n",
            "Final Layer, Epoch [3/15], Step [936/938], Loss: 0.0441\n",
            "Epoch [3/15], Step [936/938], Loss: 0.5087, Test Accuracy: 0.1038\n",
            "Layer 0, Epoch [4/15], Step [234/938], Positive Goodness: 1.8463, Negative Goodness: 1.1030\n",
            "Layer 1, Epoch [4/15], Step [234/938], Positive Goodness: 1.8639, Negative Goodness: 1.6609\n",
            "Layer 2, Epoch [4/15], Step [234/938], Positive Goodness: 0.0071, Negative Goodness: 0.0080\n",
            "Layer 3, Epoch [4/15], Step [234/938], Positive Goodness: 0.6764, Negative Goodness: 0.4434\n",
            "Final Layer, Epoch [4/15], Step [234/938], Loss: 0.0404\n",
            "Epoch [4/15], Step [234/938], Loss: 0.5208, Test Accuracy: 0.1099\n",
            "Layer 0, Epoch [4/15], Step [468/938], Positive Goodness: 1.8561, Negative Goodness: 1.1153\n",
            "Layer 1, Epoch [4/15], Step [468/938], Positive Goodness: 1.8957, Negative Goodness: 1.7076\n",
            "Layer 2, Epoch [4/15], Step [468/938], Positive Goodness: 0.0275, Negative Goodness: 0.0245\n",
            "Layer 3, Epoch [4/15], Step [468/938], Positive Goodness: 0.5617, Negative Goodness: 0.3029\n",
            "Final Layer, Epoch [4/15], Step [468/938], Loss: 0.0417\n",
            "Epoch [4/15], Step [468/938], Loss: 0.4454, Test Accuracy: 0.1574\n",
            "Layer 0, Epoch [4/15], Step [702/938], Positive Goodness: 1.8401, Negative Goodness: 1.0861\n",
            "Layer 1, Epoch [4/15], Step [702/938], Positive Goodness: 1.7869, Negative Goodness: 1.5941\n",
            "Layer 2, Epoch [4/15], Step [702/938], Positive Goodness: 0.0148, Negative Goodness: 0.0144\n",
            "Layer 3, Epoch [4/15], Step [702/938], Positive Goodness: 0.6214, Negative Goodness: 0.3182\n",
            "Final Layer, Epoch [4/15], Step [702/938], Loss: 0.0424\n",
            "Epoch [4/15], Step [702/938], Loss: 0.4936, Test Accuracy: 0.1935\n",
            "Layer 0, Epoch [4/15], Step [936/938], Positive Goodness: 1.8545, Negative Goodness: 1.0551\n",
            "Layer 1, Epoch [4/15], Step [936/938], Positive Goodness: 1.5958, Negative Goodness: 1.3596\n",
            "Layer 2, Epoch [4/15], Step [936/938], Positive Goodness: 0.0074, Negative Goodness: 0.0048\n",
            "Layer 3, Epoch [4/15], Step [936/938], Positive Goodness: 0.6688, Negative Goodness: 0.3912\n",
            "Final Layer, Epoch [4/15], Step [936/938], Loss: 0.0485\n",
            "Epoch [4/15], Step [936/938], Loss: 0.5292, Test Accuracy: 0.2090\n",
            "Layer 0, Epoch [5/15], Step [234/938], Positive Goodness: 1.8653, Negative Goodness: 1.0551\n",
            "Layer 1, Epoch [5/15], Step [234/938], Positive Goodness: 1.5198, Negative Goodness: 1.3123\n",
            "Layer 2, Epoch [5/15], Step [234/938], Positive Goodness: 0.1649, Negative Goodness: 0.1307\n",
            "Layer 3, Epoch [5/15], Step [234/938], Positive Goodness: 0.6411, Negative Goodness: 0.3443\n",
            "Final Layer, Epoch [5/15], Step [234/938], Loss: 0.0414\n",
            "Epoch [5/15], Step [234/938], Loss: 0.5233, Test Accuracy: 0.2592\n",
            "Layer 0, Epoch [5/15], Step [468/938], Positive Goodness: 1.8596, Negative Goodness: 1.0478\n",
            "Layer 1, Epoch [5/15], Step [468/938], Positive Goodness: 1.5582, Negative Goodness: 1.2677\n",
            "Layer 2, Epoch [5/15], Step [468/938], Positive Goodness: 0.4330, Negative Goodness: 0.3394\n",
            "Layer 3, Epoch [5/15], Step [468/938], Positive Goodness: 0.6817, Negative Goodness: 0.3546\n",
            "Final Layer, Epoch [5/15], Step [468/938], Loss: 0.0446\n",
            "Epoch [5/15], Step [468/938], Loss: 0.4462, Test Accuracy: 0.3082\n",
            "Layer 0, Epoch [5/15], Step [702/938], Positive Goodness: 1.8539, Negative Goodness: 1.0282\n",
            "Layer 1, Epoch [5/15], Step [702/938], Positive Goodness: 1.4709, Negative Goodness: 1.1427\n",
            "Layer 2, Epoch [5/15], Step [702/938], Positive Goodness: 0.4148, Negative Goodness: 0.3050\n",
            "Layer 3, Epoch [5/15], Step [702/938], Positive Goodness: 0.7273, Negative Goodness: 0.4149\n",
            "Final Layer, Epoch [5/15], Step [702/938], Loss: 0.0398\n",
            "Epoch [5/15], Step [702/938], Loss: 0.4592, Test Accuracy: 0.2743\n",
            "Layer 0, Epoch [5/15], Step [936/938], Positive Goodness: 1.8578, Negative Goodness: 0.9996\n",
            "Layer 1, Epoch [5/15], Step [936/938], Positive Goodness: 1.4787, Negative Goodness: 1.1429\n",
            "Layer 2, Epoch [5/15], Step [936/938], Positive Goodness: 0.5047, Negative Goodness: 0.3470\n",
            "Layer 3, Epoch [5/15], Step [936/938], Positive Goodness: 0.7091, Negative Goodness: 0.3478\n",
            "Final Layer, Epoch [5/15], Step [936/938], Loss: 0.0425\n",
            "Epoch [5/15], Step [936/938], Loss: 0.4723, Test Accuracy: 0.3796\n",
            "Layer 0, Epoch [6/15], Step [234/938], Positive Goodness: 1.8427, Negative Goodness: 0.9642\n",
            "Layer 1, Epoch [6/15], Step [234/938], Positive Goodness: 1.6593, Negative Goodness: 1.2914\n",
            "Layer 2, Epoch [6/15], Step [234/938], Positive Goodness: 0.3056, Negative Goodness: 0.2265\n",
            "Layer 3, Epoch [6/15], Step [234/938], Positive Goodness: 0.7226, Negative Goodness: 0.3007\n",
            "Final Layer, Epoch [6/15], Step [234/938], Loss: 0.0396\n",
            "Epoch [6/15], Step [234/938], Loss: 0.5605, Test Accuracy: 0.3805\n",
            "Layer 0, Epoch [6/15], Step [468/938], Positive Goodness: 1.8414, Negative Goodness: 0.9448\n",
            "Layer 1, Epoch [6/15], Step [468/938], Positive Goodness: 1.6082, Negative Goodness: 1.1547\n",
            "Layer 2, Epoch [6/15], Step [468/938], Positive Goodness: 0.1478, Negative Goodness: 0.0883\n",
            "Layer 3, Epoch [6/15], Step [468/938], Positive Goodness: 0.8307, Negative Goodness: 0.3367\n",
            "Final Layer, Epoch [6/15], Step [468/938], Loss: 0.0307\n",
            "Epoch [6/15], Step [468/938], Loss: 0.4299, Test Accuracy: 0.3986\n",
            "Layer 0, Epoch [6/15], Step [702/938], Positive Goodness: 1.8433, Negative Goodness: 0.9518\n",
            "Layer 1, Epoch [6/15], Step [702/938], Positive Goodness: 1.5458, Negative Goodness: 1.0792\n",
            "Layer 2, Epoch [6/15], Step [702/938], Positive Goodness: 0.2953, Negative Goodness: 0.2173\n",
            "Layer 3, Epoch [6/15], Step [702/938], Positive Goodness: 0.8945, Negative Goodness: 0.3927\n",
            "Final Layer, Epoch [6/15], Step [702/938], Loss: 0.0317\n",
            "Epoch [6/15], Step [702/938], Loss: 0.4917, Test Accuracy: 0.4383\n",
            "Layer 0, Epoch [6/15], Step [936/938], Positive Goodness: 1.8274, Negative Goodness: 0.8768\n",
            "Layer 1, Epoch [6/15], Step [936/938], Positive Goodness: 1.4514, Negative Goodness: 0.9191\n",
            "Layer 2, Epoch [6/15], Step [936/938], Positive Goodness: 0.3888, Negative Goodness: 0.3027\n",
            "Layer 3, Epoch [6/15], Step [936/938], Positive Goodness: 0.8663, Negative Goodness: 0.3529\n",
            "Final Layer, Epoch [6/15], Step [936/938], Loss: 0.0310\n",
            "Epoch [6/15], Step [936/938], Loss: 0.5123, Test Accuracy: 0.3770\n",
            "Layer 0, Epoch [7/15], Step [234/938], Positive Goodness: 1.8377, Negative Goodness: 0.8477\n",
            "Layer 1, Epoch [7/15], Step [234/938], Positive Goodness: 1.4584, Negative Goodness: 0.9508\n",
            "Layer 2, Epoch [7/15], Step [234/938], Positive Goodness: 0.3517, Negative Goodness: 0.2206\n",
            "Layer 3, Epoch [7/15], Step [234/938], Positive Goodness: 0.9015, Negative Goodness: 0.3760\n",
            "Final Layer, Epoch [7/15], Step [234/938], Loss: 0.0291\n",
            "Epoch [7/15], Step [234/938], Loss: 0.4581, Test Accuracy: 0.4884\n",
            "Layer 0, Epoch [7/15], Step [468/938], Positive Goodness: 1.8424, Negative Goodness: 0.8403\n",
            "Layer 1, Epoch [7/15], Step [468/938], Positive Goodness: 1.3323, Negative Goodness: 0.7771\n",
            "Layer 2, Epoch [7/15], Step [468/938], Positive Goodness: 0.2880, Negative Goodness: 0.1448\n",
            "Layer 3, Epoch [7/15], Step [468/938], Positive Goodness: 0.9374, Negative Goodness: 0.3782\n",
            "Final Layer, Epoch [7/15], Step [468/938], Loss: 0.0264\n",
            "Epoch [7/15], Step [468/938], Loss: 0.5522, Test Accuracy: 0.5020\n",
            "Layer 0, Epoch [7/15], Step [702/938], Positive Goodness: 1.8374, Negative Goodness: 0.7770\n",
            "Layer 1, Epoch [7/15], Step [702/938], Positive Goodness: 1.3725, Negative Goodness: 0.7478\n",
            "Layer 2, Epoch [7/15], Step [702/938], Positive Goodness: 0.4934, Negative Goodness: 0.2985\n",
            "Layer 3, Epoch [7/15], Step [702/938], Positive Goodness: 0.9726, Negative Goodness: 0.3496\n",
            "Final Layer, Epoch [7/15], Step [702/938], Loss: 0.0261\n",
            "Epoch [7/15], Step [702/938], Loss: 0.5075, Test Accuracy: 0.5415\n",
            "Layer 0, Epoch [7/15], Step [936/938], Positive Goodness: 1.8550, Negative Goodness: 0.7681\n",
            "Layer 1, Epoch [7/15], Step [936/938], Positive Goodness: 1.3145, Negative Goodness: 0.6874\n",
            "Layer 2, Epoch [7/15], Step [936/938], Positive Goodness: 0.5127, Negative Goodness: 0.3481\n",
            "Layer 3, Epoch [7/15], Step [936/938], Positive Goodness: 0.9415, Negative Goodness: 0.3875\n",
            "Final Layer, Epoch [7/15], Step [936/938], Loss: 0.0318\n",
            "Epoch [7/15], Step [936/938], Loss: 0.4836, Test Accuracy: 0.5058\n",
            "Layer 0, Epoch [8/15], Step [234/938], Positive Goodness: 1.8558, Negative Goodness: 0.7477\n",
            "Layer 1, Epoch [8/15], Step [234/938], Positive Goodness: 1.2907, Negative Goodness: 0.6880\n",
            "Layer 2, Epoch [8/15], Step [234/938], Positive Goodness: 0.4569, Negative Goodness: 0.2316\n",
            "Layer 3, Epoch [8/15], Step [234/938], Positive Goodness: 0.9125, Negative Goodness: 0.3213\n",
            "Final Layer, Epoch [8/15], Step [234/938], Loss: 0.0224\n",
            "Epoch [8/15], Step [234/938], Loss: 0.4805, Test Accuracy: 0.6030\n",
            "Layer 0, Epoch [8/15], Step [468/938], Positive Goodness: 1.8568, Negative Goodness: 0.6785\n",
            "Layer 1, Epoch [8/15], Step [468/938], Positive Goodness: 1.2712, Negative Goodness: 0.5293\n",
            "Layer 2, Epoch [8/15], Step [468/938], Positive Goodness: 0.5353, Negative Goodness: 0.2269\n",
            "Layer 3, Epoch [8/15], Step [468/938], Positive Goodness: 0.9495, Negative Goodness: 0.3639\n",
            "Final Layer, Epoch [8/15], Step [468/938], Loss: 0.0237\n",
            "Epoch [8/15], Step [468/938], Loss: 0.5602, Test Accuracy: 0.6794\n",
            "Layer 0, Epoch [8/15], Step [702/938], Positive Goodness: 1.8597, Negative Goodness: 0.6493\n",
            "Layer 1, Epoch [8/15], Step [702/938], Positive Goodness: 1.3282, Negative Goodness: 0.6236\n",
            "Layer 2, Epoch [8/15], Step [702/938], Positive Goodness: 0.5297, Negative Goodness: 0.2069\n",
            "Layer 3, Epoch [8/15], Step [702/938], Positive Goodness: 0.9564, Negative Goodness: 0.3598\n",
            "Final Layer, Epoch [8/15], Step [702/938], Loss: 0.0251\n",
            "Epoch [8/15], Step [702/938], Loss: 0.4484, Test Accuracy: 0.6545\n",
            "Layer 0, Epoch [8/15], Step [936/938], Positive Goodness: 1.8637, Negative Goodness: 0.6335\n",
            "Layer 1, Epoch [8/15], Step [936/938], Positive Goodness: 1.3485, Negative Goodness: 0.6264\n",
            "Layer 2, Epoch [8/15], Step [936/938], Positive Goodness: 0.4827, Negative Goodness: 0.1288\n",
            "Layer 3, Epoch [8/15], Step [936/938], Positive Goodness: 0.9511, Negative Goodness: 0.3576\n",
            "Final Layer, Epoch [8/15], Step [936/938], Loss: 0.0273\n",
            "Epoch [8/15], Step [936/938], Loss: 0.4757, Test Accuracy: 0.6163\n",
            "Layer 0, Epoch [9/15], Step [234/938], Positive Goodness: 1.8517, Negative Goodness: 0.6458\n",
            "Layer 1, Epoch [9/15], Step [234/938], Positive Goodness: 1.2775, Negative Goodness: 0.6127\n",
            "Layer 2, Epoch [9/15], Step [234/938], Positive Goodness: 0.5387, Negative Goodness: 0.1695\n",
            "Layer 3, Epoch [9/15], Step [234/938], Positive Goodness: 0.9604, Negative Goodness: 0.3648\n",
            "Final Layer, Epoch [9/15], Step [234/938], Loss: 0.0230\n",
            "Epoch [9/15], Step [234/938], Loss: 0.4857, Test Accuracy: 0.5441\n",
            "Layer 0, Epoch [9/15], Step [468/938], Positive Goodness: 1.8589, Negative Goodness: 0.5799\n",
            "Layer 1, Epoch [9/15], Step [468/938], Positive Goodness: 1.3772, Negative Goodness: 0.6414\n",
            "Layer 2, Epoch [9/15], Step [468/938], Positive Goodness: 0.5931, Negative Goodness: 0.2315\n",
            "Layer 3, Epoch [9/15], Step [468/938], Positive Goodness: 0.9804, Negative Goodness: 0.3714\n",
            "Final Layer, Epoch [9/15], Step [468/938], Loss: 0.0184\n",
            "Epoch [9/15], Step [468/938], Loss: 0.4334, Test Accuracy: 0.6619\n",
            "Layer 0, Epoch [9/15], Step [702/938], Positive Goodness: 1.8454, Negative Goodness: 0.5651\n",
            "Layer 1, Epoch [9/15], Step [702/938], Positive Goodness: 1.4868, Negative Goodness: 0.6968\n",
            "Layer 2, Epoch [9/15], Step [702/938], Positive Goodness: 0.5908, Negative Goodness: 0.1982\n",
            "Layer 3, Epoch [9/15], Step [702/938], Positive Goodness: 0.9922, Negative Goodness: 0.3504\n",
            "Final Layer, Epoch [9/15], Step [702/938], Loss: 0.0182\n",
            "Epoch [9/15], Step [702/938], Loss: 0.4542, Test Accuracy: 0.6035\n",
            "Layer 0, Epoch [9/15], Step [936/938], Positive Goodness: 1.8376, Negative Goodness: 0.5117\n",
            "Layer 1, Epoch [9/15], Step [936/938], Positive Goodness: 1.4050, Negative Goodness: 0.6156\n",
            "Layer 2, Epoch [9/15], Step [936/938], Positive Goodness: 0.6246, Negative Goodness: 0.2253\n",
            "Layer 3, Epoch [9/15], Step [936/938], Positive Goodness: 0.9890, Negative Goodness: 0.3707\n",
            "Final Layer, Epoch [9/15], Step [936/938], Loss: 0.0224\n",
            "Epoch [9/15], Step [936/938], Loss: 0.4004, Test Accuracy: 0.6671\n",
            "Layer 0, Epoch [10/15], Step [234/938], Positive Goodness: 1.8588, Negative Goodness: 0.5037\n",
            "Layer 1, Epoch [10/15], Step [234/938], Positive Goodness: 1.5611, Negative Goodness: 0.7508\n",
            "Layer 2, Epoch [10/15], Step [234/938], Positive Goodness: 0.5981, Negative Goodness: 0.2207\n",
            "Layer 3, Epoch [10/15], Step [234/938], Positive Goodness: 0.9832, Negative Goodness: 0.3295\n",
            "Final Layer, Epoch [10/15], Step [234/938], Loss: 0.0198\n",
            "Epoch [10/15], Step [234/938], Loss: 0.4429, Test Accuracy: 0.6945\n",
            "Layer 0, Epoch [10/15], Step [468/938], Positive Goodness: 1.8475, Negative Goodness: 0.4906\n",
            "Layer 1, Epoch [10/15], Step [468/938], Positive Goodness: 1.4888, Negative Goodness: 0.6056\n",
            "Layer 2, Epoch [10/15], Step [468/938], Positive Goodness: 0.4212, Negative Goodness: 0.1257\n",
            "Layer 3, Epoch [10/15], Step [468/938], Positive Goodness: 0.9879, Negative Goodness: 0.3233\n",
            "Final Layer, Epoch [10/15], Step [468/938], Loss: 0.0170\n",
            "Epoch [10/15], Step [468/938], Loss: 0.4165, Test Accuracy: 0.7142\n",
            "Layer 0, Epoch [10/15], Step [702/938], Positive Goodness: 1.8602, Negative Goodness: 0.4551\n",
            "Layer 1, Epoch [10/15], Step [702/938], Positive Goodness: 1.4985, Negative Goodness: 0.5911\n",
            "Layer 2, Epoch [10/15], Step [702/938], Positive Goodness: 0.3441, Negative Goodness: 0.0716\n",
            "Layer 3, Epoch [10/15], Step [702/938], Positive Goodness: 1.0268, Negative Goodness: 0.3227\n",
            "Final Layer, Epoch [10/15], Step [702/938], Loss: 0.0128\n",
            "Epoch [10/15], Step [702/938], Loss: 0.4593, Test Accuracy: 0.7183\n",
            "Layer 0, Epoch [10/15], Step [936/938], Positive Goodness: 1.8627, Negative Goodness: 0.4292\n",
            "Layer 1, Epoch [10/15], Step [936/938], Positive Goodness: 1.5391, Negative Goodness: 0.6101\n",
            "Layer 2, Epoch [10/15], Step [936/938], Positive Goodness: 0.3309, Negative Goodness: 0.0570\n",
            "Layer 3, Epoch [10/15], Step [936/938], Positive Goodness: 0.9696, Negative Goodness: 0.2673\n",
            "Final Layer, Epoch [10/15], Step [936/938], Loss: 0.0152\n",
            "Epoch [10/15], Step [936/938], Loss: 0.5620, Test Accuracy: 0.7004\n",
            "Layer 0, Epoch [11/15], Step [234/938], Positive Goodness: 1.8706, Negative Goodness: 0.4255\n",
            "Layer 1, Epoch [11/15], Step [234/938], Positive Goodness: 1.6279, Negative Goodness: 0.7141\n",
            "Layer 2, Epoch [11/15], Step [234/938], Positive Goodness: 0.2559, Negative Goodness: 0.0477\n",
            "Layer 3, Epoch [11/15], Step [234/938], Positive Goodness: 0.9641, Negative Goodness: 0.2511\n",
            "Final Layer, Epoch [11/15], Step [234/938], Loss: 0.0128\n",
            "Epoch [11/15], Step [234/938], Loss: 0.4046, Test Accuracy: 0.6273\n",
            "Layer 0, Epoch [11/15], Step [468/938], Positive Goodness: 1.8476, Negative Goodness: 0.4165\n",
            "Layer 1, Epoch [11/15], Step [468/938], Positive Goodness: 1.5252, Negative Goodness: 0.6060\n",
            "Layer 2, Epoch [11/15], Step [468/938], Positive Goodness: 0.1588, Negative Goodness: 0.0299\n",
            "Layer 3, Epoch [11/15], Step [468/938], Positive Goodness: 1.0160, Negative Goodness: 0.2662\n",
            "Final Layer, Epoch [11/15], Step [468/938], Loss: 0.0193\n",
            "Epoch [11/15], Step [468/938], Loss: 0.4554, Test Accuracy: 0.6548\n",
            "Layer 0, Epoch [11/15], Step [702/938], Positive Goodness: 1.8675, Negative Goodness: 0.3826\n",
            "Layer 1, Epoch [11/15], Step [702/938], Positive Goodness: 1.5704, Negative Goodness: 0.5624\n",
            "Layer 2, Epoch [11/15], Step [702/938], Positive Goodness: 0.1537, Negative Goodness: 0.0259\n",
            "Layer 3, Epoch [11/15], Step [702/938], Positive Goodness: 1.0342, Negative Goodness: 0.2529\n",
            "Final Layer, Epoch [11/15], Step [702/938], Loss: 0.0154\n",
            "Epoch [11/15], Step [702/938], Loss: 0.4039, Test Accuracy: 0.7330\n",
            "Layer 0, Epoch [11/15], Step [936/938], Positive Goodness: 1.8635, Negative Goodness: 0.3642\n",
            "Layer 1, Epoch [11/15], Step [936/938], Positive Goodness: 1.4880, Negative Goodness: 0.4119\n",
            "Layer 2, Epoch [11/15], Step [936/938], Positive Goodness: 0.1631, Negative Goodness: 0.0272\n",
            "Layer 3, Epoch [11/15], Step [936/938], Positive Goodness: 1.0756, Negative Goodness: 0.2974\n",
            "Final Layer, Epoch [11/15], Step [936/938], Loss: 0.0126\n",
            "Epoch [11/15], Step [936/938], Loss: 0.5037, Test Accuracy: 0.8065\n",
            "Layer 0, Epoch [12/15], Step [234/938], Positive Goodness: 1.8718, Negative Goodness: 0.3362\n",
            "Layer 1, Epoch [12/15], Step [234/938], Positive Goodness: 1.5613, Negative Goodness: 0.4968\n",
            "Layer 2, Epoch [12/15], Step [234/938], Positive Goodness: 0.1708, Negative Goodness: 0.0317\n",
            "Layer 3, Epoch [12/15], Step [234/938], Positive Goodness: 1.0424, Negative Goodness: 0.2637\n",
            "Final Layer, Epoch [12/15], Step [234/938], Loss: 0.0111\n",
            "Epoch [12/15], Step [234/938], Loss: 0.3833, Test Accuracy: 0.7997\n",
            "Layer 0, Epoch [12/15], Step [468/938], Positive Goodness: 1.8675, Negative Goodness: 0.3174\n",
            "Layer 1, Epoch [12/15], Step [468/938], Positive Goodness: 1.6210, Negative Goodness: 0.5278\n",
            "Layer 2, Epoch [12/15], Step [468/938], Positive Goodness: 0.1710, Negative Goodness: 0.0452\n",
            "Layer 3, Epoch [12/15], Step [468/938], Positive Goodness: 1.0509, Negative Goodness: 0.2341\n",
            "Final Layer, Epoch [12/15], Step [468/938], Loss: 0.0100\n",
            "Epoch [12/15], Step [468/938], Loss: 0.4384, Test Accuracy: 0.6789\n",
            "Layer 0, Epoch [12/15], Step [702/938], Positive Goodness: 1.8735, Negative Goodness: 0.3264\n",
            "Layer 1, Epoch [12/15], Step [702/938], Positive Goodness: 1.5857, Negative Goodness: 0.4855\n",
            "Layer 2, Epoch [12/15], Step [702/938], Positive Goodness: 0.1718, Negative Goodness: 0.0434\n",
            "Layer 3, Epoch [12/15], Step [702/938], Positive Goodness: 1.0491, Negative Goodness: 0.2046\n",
            "Final Layer, Epoch [12/15], Step [702/938], Loss: 0.0081\n",
            "Epoch [12/15], Step [702/938], Loss: 0.5297, Test Accuracy: 0.7092\n",
            "Layer 0, Epoch [12/15], Step [936/938], Positive Goodness: 1.8915, Negative Goodness: 0.3065\n",
            "Layer 1, Epoch [12/15], Step [936/938], Positive Goodness: 1.5555, Negative Goodness: 0.3841\n",
            "Layer 2, Epoch [12/15], Step [936/938], Positive Goodness: 0.1669, Negative Goodness: 0.0280\n",
            "Layer 3, Epoch [12/15], Step [936/938], Positive Goodness: 1.0593, Negative Goodness: 0.1991\n",
            "Final Layer, Epoch [12/15], Step [936/938], Loss: 0.0104\n",
            "Epoch [12/15], Step [936/938], Loss: 0.6183, Test Accuracy: 0.8257\n",
            "Layer 0, Epoch [13/15], Step [234/938], Positive Goodness: 1.8840, Negative Goodness: 0.3572\n",
            "Layer 1, Epoch [13/15], Step [234/938], Positive Goodness: 1.6357, Negative Goodness: 0.4969\n",
            "Layer 2, Epoch [13/15], Step [234/938], Positive Goodness: 0.1732, Negative Goodness: 0.0227\n",
            "Layer 3, Epoch [13/15], Step [234/938], Positive Goodness: 1.0681, Negative Goodness: 0.1930\n",
            "Final Layer, Epoch [13/15], Step [234/938], Loss: 0.0105\n",
            "Epoch [13/15], Step [234/938], Loss: 0.4987, Test Accuracy: 0.6981\n",
            "Layer 0, Epoch [13/15], Step [468/938], Positive Goodness: 1.8939, Negative Goodness: 0.3149\n",
            "Layer 1, Epoch [13/15], Step [468/938], Positive Goodness: 1.6240, Negative Goodness: 0.4597\n",
            "Layer 2, Epoch [13/15], Step [468/938], Positive Goodness: 0.1567, Negative Goodness: 0.0199\n",
            "Layer 3, Epoch [13/15], Step [468/938], Positive Goodness: 1.0189, Negative Goodness: 0.2055\n",
            "Final Layer, Epoch [13/15], Step [468/938], Loss: 0.0112\n",
            "Epoch [13/15], Step [468/938], Loss: 0.4884, Test Accuracy: 0.8131\n",
            "Layer 0, Epoch [13/15], Step [702/938], Positive Goodness: 1.8892, Negative Goodness: 0.2823\n",
            "Layer 1, Epoch [13/15], Step [702/938], Positive Goodness: 1.5552, Negative Goodness: 0.3527\n",
            "Layer 2, Epoch [13/15], Step [702/938], Positive Goodness: 0.1751, Negative Goodness: 0.0371\n",
            "Layer 3, Epoch [13/15], Step [702/938], Positive Goodness: 1.0676, Negative Goodness: 0.2082\n",
            "Final Layer, Epoch [13/15], Step [702/938], Loss: 0.0093\n",
            "Epoch [13/15], Step [702/938], Loss: 0.6157, Test Accuracy: 0.8384\n",
            "Layer 0, Epoch [13/15], Step [936/938], Positive Goodness: 1.8859, Negative Goodness: 0.2931\n",
            "Layer 1, Epoch [13/15], Step [936/938], Positive Goodness: 1.5046, Negative Goodness: 0.3486\n",
            "Layer 2, Epoch [13/15], Step [936/938], Positive Goodness: 0.1715, Negative Goodness: 0.0280\n",
            "Layer 3, Epoch [13/15], Step [936/938], Positive Goodness: 1.0525, Negative Goodness: 0.1845\n",
            "Final Layer, Epoch [13/15], Step [936/938], Loss: 0.0125\n",
            "Epoch [13/15], Step [936/938], Loss: 0.4357, Test Accuracy: 0.8556\n",
            "Layer 0, Epoch [14/15], Step [234/938], Positive Goodness: 1.8911, Negative Goodness: 0.2640\n",
            "Layer 1, Epoch [14/15], Step [234/938], Positive Goodness: 1.6562, Negative Goodness: 0.3730\n",
            "Layer 2, Epoch [14/15], Step [234/938], Positive Goodness: 0.1684, Negative Goodness: 0.0257\n",
            "Layer 3, Epoch [14/15], Step [234/938], Positive Goodness: 1.0625, Negative Goodness: 0.1771\n",
            "Final Layer, Epoch [14/15], Step [234/938], Loss: 0.0069\n",
            "Epoch [14/15], Step [234/938], Loss: 0.4449, Test Accuracy: 0.7862\n",
            "Layer 0, Epoch [14/15], Step [468/938], Positive Goodness: 1.8812, Negative Goodness: 0.2685\n",
            "Layer 1, Epoch [14/15], Step [468/938], Positive Goodness: 1.5792, Negative Goodness: 0.3072\n",
            "Layer 2, Epoch [14/15], Step [468/938], Positive Goodness: 0.1733, Negative Goodness: 0.0439\n",
            "Layer 3, Epoch [14/15], Step [468/938], Positive Goodness: 1.0673, Negative Goodness: 0.1782\n",
            "Final Layer, Epoch [14/15], Step [468/938], Loss: 0.0082\n",
            "Epoch [14/15], Step [468/938], Loss: 0.5232, Test Accuracy: 0.7711\n",
            "Layer 0, Epoch [14/15], Step [702/938], Positive Goodness: 1.8846, Negative Goodness: 0.2435\n",
            "Layer 1, Epoch [14/15], Step [702/938], Positive Goodness: 1.6301, Negative Goodness: 0.3986\n",
            "Layer 2, Epoch [14/15], Step [702/938], Positive Goodness: 0.1749, Negative Goodness: 0.0279\n",
            "Layer 3, Epoch [14/15], Step [702/938], Positive Goodness: 1.0612, Negative Goodness: 0.1679\n",
            "Final Layer, Epoch [14/15], Step [702/938], Loss: 0.0082\n",
            "Epoch [14/15], Step [702/938], Loss: 0.4691, Test Accuracy: 0.8061\n",
            "Layer 0, Epoch [14/15], Step [936/938], Positive Goodness: 1.8825, Negative Goodness: 0.2389\n",
            "Layer 1, Epoch [14/15], Step [936/938], Positive Goodness: 1.5927, Negative Goodness: 0.3448\n",
            "Layer 2, Epoch [14/15], Step [936/938], Positive Goodness: 0.1709, Negative Goodness: 0.0256\n",
            "Layer 3, Epoch [14/15], Step [936/938], Positive Goodness: 1.0670, Negative Goodness: 0.1736\n",
            "Final Layer, Epoch [14/15], Step [936/938], Loss: 0.0101\n",
            "Epoch [14/15], Step [936/938], Loss: 0.4762, Test Accuracy: 0.7935\n",
            "Layer 0, Epoch [15/15], Step [234/938], Positive Goodness: 1.8987, Negative Goodness: 0.1956\n",
            "Layer 1, Epoch [15/15], Step [234/938], Positive Goodness: 1.4756, Negative Goodness: 0.2072\n",
            "Layer 2, Epoch [15/15], Step [234/938], Positive Goodness: 0.1673, Negative Goodness: 0.0281\n",
            "Layer 3, Epoch [15/15], Step [234/938], Positive Goodness: 1.0809, Negative Goodness: 0.1946\n",
            "Final Layer, Epoch [15/15], Step [234/938], Loss: 0.0079\n",
            "Epoch [15/15], Step [234/938], Loss: 0.6574, Test Accuracy: 0.8292\n",
            "Layer 0, Epoch [15/15], Step [468/938], Positive Goodness: 1.8889, Negative Goodness: 0.1830\n",
            "Layer 1, Epoch [15/15], Step [468/938], Positive Goodness: 1.5730, Negative Goodness: 0.2798\n",
            "Layer 2, Epoch [15/15], Step [468/938], Positive Goodness: 0.1761, Negative Goodness: 0.0323\n",
            "Layer 3, Epoch [15/15], Step [468/938], Positive Goodness: 1.0927, Negative Goodness: 0.1770\n",
            "Final Layer, Epoch [15/15], Step [468/938], Loss: 0.0083\n",
            "Epoch [15/15], Step [468/938], Loss: 0.4792, Test Accuracy: 0.7726\n",
            "Layer 0, Epoch [15/15], Step [702/938], Positive Goodness: 1.8944, Negative Goodness: 0.1899\n",
            "Layer 1, Epoch [15/15], Step [702/938], Positive Goodness: 1.6246, Negative Goodness: 0.2903\n",
            "Layer 2, Epoch [15/15], Step [702/938], Positive Goodness: 0.1611, Negative Goodness: 0.0179\n",
            "Layer 3, Epoch [15/15], Step [702/938], Positive Goodness: 1.0801, Negative Goodness: 0.1527\n",
            "Final Layer, Epoch [15/15], Step [702/938], Loss: 0.0081\n",
            "Epoch [15/15], Step [702/938], Loss: 0.4482, Test Accuracy: 0.7886\n",
            "Layer 0, Epoch [15/15], Step [936/938], Positive Goodness: 1.8899, Negative Goodness: 0.1932\n",
            "Layer 1, Epoch [15/15], Step [936/938], Positive Goodness: 1.6106, Negative Goodness: 0.2699\n",
            "Layer 2, Epoch [15/15], Step [936/938], Positive Goodness: 0.1749, Negative Goodness: 0.0240\n",
            "Layer 3, Epoch [15/15], Step [936/938], Positive Goodness: 1.0990, Negative Goodness: 0.1538\n",
            "Final Layer, Epoch [15/15], Step [936/938], Loss: 0.0068\n",
            "Epoch [15/15], Step [936/938], Loss: 0.4646, Test Accuracy: 0.7945\n"
          ]
        }
      ],
      "source": [
        "total_pos_goodnesses, total_neg_goodnesses, train_losses, test_losses, test_accuracies = train(num_epochs, model, train_loader, test_loader, ceLoss, pos_optimizers, neg_optimizers, final_optimizer, thresholds, records_per_epoch, supervised)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2GZVnrctom8"
      },
      "source": [
        "## Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_8yPfk03oLT",
        "outputId": "cf5678fd-9686-48d1-dbd9-6c61a2bfddcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the test set:  (tensor(0.4561, device='cuda:0'), tensor(0.7993, device='cuda:0'))\n"
          ]
        }
      ],
      "source": [
        "test_acc = test(model, test_loader,ceLoss)\n",
        "print('Test Accuracy of the model on the test set: ',test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "PMheoEy12rn2",
        "outputId": "f3958c2a-2483-409d-d761-99d1969dbc27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:  tensor([6, 0, 6, 6, 8, 2, 3, 8, 4, 0, 8, 0, 8, 0, 5, 8, 2, 9, 2, 5, 8, 5, 0, 2,\n",
            "        4, 8, 8, 3, 9, 6, 5, 5, 6, 3, 3, 9, 4, 8, 8, 8, 6, 6, 6, 2, 9, 7, 4, 0,\n",
            "        5, 7, 4, 7, 7, 0, 4, 2, 6, 9, 4, 8, 9, 9, 8, 4], device='cuda:0')\n",
            " Label:  tensor([6, 0, 6, 6, 3, 2, 3, 5, 7, 0, 5, 0, 8, 8, 5, 1, 2, 9, 2, 5, 6, 5, 0, 3,\n",
            "        4, 1, 5, 3, 9, 6, 5, 3, 6, 3, 3, 9, 9, 1, 1, 8, 6, 2, 6, 2, 9, 7, 4, 0,\n",
            "        5, 7, 4, 7, 7, 0, 4, 2, 6, 9, 4, 8, 9, 9, 6, 9], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# prediction test & visualization\n",
        "x,y = next(iter(train_loader))\n",
        "x,y = x.to(device),y.to(device)\n",
        "neg_x = model.construct_supervised_example(x,y,False)\n",
        "print(\"Prediction: \", model.predict(x,device))\n",
        "print(\" Label: \",y)\n",
        "plt.imshow(neg_x[0].squeeze().cpu(), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot positive goodness and negative goodness\n",
        "epoch_axis = torch.arange(0.25,15.25,0.25)\n",
        "for i in range(len(total_pos_goodnesses)):\n",
        "    plt.plot(epoch_axis,total_pos_goodnesses[i],label=\"Positive Goodness Layer {}\".format(i))\n",
        "for i in range(len(total_neg_goodnesses)):\n",
        "    plt.plot(epoch_axis,total_neg_goodnesses[i],label=\"Negative Goodness Layer {}\".format(i))\n",
        "plt.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08JW-2PaLYEF",
        "outputId": "2d774a35-a5cf-4674-cdc8-6ba765cdf2b4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWYmB7jR9ZXz"
      },
      "source": [
        "## Previous Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLzUiNdH9YWN",
        "outputId": "b2f6ff8d-b849-49e1-97b7-308e5cc99ccc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('model_v1.pth',map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgSMzN_F-Ibw",
        "outputId": "886249df-924d-4661-967c-cf10206446b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f129f0bc310>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Accuracy of the model on the test set: 0.65\n"
          ]
        }
      ],
      "source": [
        "# adjusted test function (label is messed up when model_v1)\n",
        "def test_v1():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for x, label in test_loader:\n",
        "            test_output = model.predict(x.to(device)) + 1\n",
        "            if label == 0:\n",
        "              test_output -= 5\n",
        "            correct += (test_output == label)\n",
        "            total += 1\n",
        "    print()\n",
        "    print('Test Accuracy of the model on the test set: %.2f' % (correct/total))\n",
        "test_v1()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8xEIHA2ZN1wj"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}