{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIHpnulXtom0"
      },
      "source": [
        "# Fix Hyperparameters\n",
        "# Add Linear Scheduler\n",
        "# Implement Forward Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay9ttw0IlPZ2"
      },
      "source": [
        "## Setup & Dataset Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J965j0wvtom2"
      },
      "source": [
        "### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIV97VrN-r05",
        "outputId": "a17783f3-0a2b-4228-b9bc-c782312b69e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ray[air] in /usr/local/lib/python3.10/dist-packages (2.6.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (8.1.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[air]) (3.12.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[air]) (4.3.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.0.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[air]) (23.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (4.23.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[air]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[air]) (2.27.1)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.56.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.22.4)\n",
            "Requirement already satisfied: virtualenv<20.21.1,>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (20.21.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.23.1)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.7.0)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.5.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[air]) (2023.6.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (9.0.0)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.11.2)\n",
            "Requirement already satisfied: aiorwlock in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.3.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (2.6.1)\n",
            "Requirement already satisfied: pydantic<2 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.10.11)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.5.5)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.27.0)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (3.8.4)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.17.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[air]) (6.3.0)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.3.14)\n",
            "Requirement already satisfied: gpustat>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.100.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (1.9.2)\n",
            "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[air]) (12.535.77)\n",
            "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[air]) (5.9.5)\n",
            "Requirement already satisfied: blessed>=1.17.1 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[air]) (1.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->ray[air]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->ray[air]) (2022.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2->ray[air]) (4.7.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[air]) (0.3.7)\n",
            "Requirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[air]) (3.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->ray[air]) (3.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[air]) (0.19.3)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[air]) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[air]) (2.11.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[air]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[air]) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[air]) (3.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->ray[air]) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->ray[air]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->ray[air]) (1.1.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[air]) (0.2.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[air]) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (1.59.1)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (2.17.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (0.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"ray[air]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JqYY0cwIkJmq"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "import os\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split\n",
        "from matplotlib.pyplot import figure\n",
        "# from ray import tune\n",
        "# from ray.air import Checkpoint, session\n",
        "# from ray.tune.schedulers import ASHAScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBkHOxqFtom3"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT3vR52KNjgA",
        "outputId": "fa3b2390-6bad-4bae-ad48-4cfd88602fdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "batch_size = 128\n",
        "supervised = True\n",
        "lrs = [1e-2,3e-4,4e-4,5e-5,3e-4]\n",
        "momentums = [0.9,0.9,0.9,0.9,0.9]\n",
        "weight_decays = [1e-5,1e-5,4e-4,3e-4,1e-5]\n",
        "weight_decay = 1e-3\n",
        "weight_decay_final = 3e-3\n",
        "thresholds = [50,1000,1000,1000]\n",
        "num_epochs = 100\n",
        "records_per_epoch = 4\n",
        "seed = 0\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read & Load Data"
      ],
      "metadata": {
        "id": "ZWXh5TUHjfyX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "eGYNvwIhker4"
      },
      "outputs": [],
      "source": [
        "# import MNIST\n",
        "def load_mnist(data_dir=\"./data\"):\n",
        "  train_data = datasets.MNIST(\n",
        "      root = data_dir,\n",
        "      train = True,\n",
        "      transform = ToTensor(),\n",
        "      download = True,\n",
        "  )\n",
        "  test_data = datasets.MNIST(\n",
        "      root = data_dir,\n",
        "      train = False,\n",
        "      transform = ToTensor(),\n",
        "      download = True\n",
        "  )\n",
        "  return train_data,test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "exjpqMVzcMBG"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = load_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4C-DcpX9p2o",
        "outputId": "c04453e1-4a36-4cfd-9cb4-939a89f31a1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x79002b4d03a0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x79002b4d1cf0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Dataloader\n",
        "train_loader = DataLoader(train_data,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        num_workers=1)\n",
        "\n",
        "test_loader = DataLoader(test_data,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=1)\n",
        "\n",
        "train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xEIHA2ZN1wj"
      },
      "source": [
        "## Construct Negative Examples for Unsupervised Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfySAAjudWtk"
      },
      "outputs": [],
      "source": [
        "filter = torch.tensor([[0.0625,0.125,0.0625],[0.125,0.25,0.125],[0.0625,0.125,0.0625]])\n",
        "\n",
        "def blur(img,filter):\n",
        "  # blur img using filter\n",
        "  # params: filter --> torch.tensor(2*radius+1,2*radius+1)\n",
        "  m,n = img.shape\n",
        "  radius = (filter.shape[0]-1)//2\n",
        "  new_img = torch.zeros(m,n)\n",
        "  for i in range(m):\n",
        "    for j in range(n):\n",
        "      top,left,bottom,right = max(0,i-radius),max(0,j-radius),min(m-1,i+radius),min(m-1,j+radius)\n",
        "      new_img[i][j] = sum([img[x][y]*filter[x-i+radius][y-j+radius]\n",
        "                           for y in range(left,right+1) for x in range(top,bottom+1)])\n",
        "  return new_img\n",
        "\n",
        "def generate_negative_example():\n",
        "  # construct a negative example for unsupervised case (3.2 in paper)\n",
        "  mask = (torch.rand(28,28) > 0.5).long() # initiate as random bit image\n",
        "\n",
        "  for i in range(6): # repeat blurring 6 times\n",
        "    mask = blur(mask,filter)\n",
        "\n",
        "  mask = (mask > 0.5).long() # mask\n",
        "\n",
        "  index1,index2 = torch.randint(len(train_data),(2,))\n",
        "  return (mask * train_data[index1][0] + (1-mask) * train_data[index2][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxx1HhrVZU8b"
      },
      "source": [
        "## Model Definition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dOcFixAKTEOh"
      },
      "outputs": [],
      "source": [
        "# single-layer NN\n",
        "class one_layer_net(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(one_layer_net, self).__init__()\n",
        "\n",
        "        # hidden layer\n",
        "        # self.layer = torch.nn.Sequential(\n",
        "            # torch.nn.Flatten(), # nn.Flatten to standardize input shape\n",
        "            # torch.nn.Linear(in_size, out_size),\n",
        "            # torch.nn.ReLU() # activation function\n",
        "        # )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.layer = nn.Linear(in_size, out_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      f = self.flatten(x)\n",
        "      l = self.layer(f)\n",
        "      r = self.relu(l)\n",
        "      return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0iLwMtYODMgb"
      },
      "outputs": [],
      "source": [
        "# full network\n",
        "class net(nn.Module):\n",
        "    # Constructor\n",
        "    def __init__(self,in_size):\n",
        "        super(net, self).__init__()\n",
        "        self.layer1 = one_layer_net(in_size,2000)\n",
        "        self.layer2 = one_layer_net(2000,2000)\n",
        "        self.layer3 = one_layer_net(2000,2000)\n",
        "        self.layer4 = one_layer_net(2000,2000)\n",
        "        self.ln = nn.LayerNorm(2000) # layernorm\n",
        "\n",
        "        # softmax layer for hard negative label generation\n",
        "        self.out_layer = nn.Linear(6000, 10)\n",
        "\n",
        "        self.layers = [self.layer1,self.layer2,self.layer3,self.layer4,self.out_layer]\n",
        "\n",
        "    def forward(self,x):\n",
        "        # x-->tensor (batch_size,channel,height,width)\n",
        "        # return res --> tensor (batch_size,10)\n",
        "        interms = []\n",
        "        interm = x.detach().clone()\n",
        "\n",
        "        for index in range(len(self.layers)-1):\n",
        "          layer = self.layers[index] # linear layer\n",
        "          output = layer(interm) # run the layer\n",
        "          interm = self.ln(output) # layer-norm\n",
        "          interms.append(interm.detach().clone()) # store normalized activity\n",
        "\n",
        "        final_input = torch.cat(interms[1:],dim=1)\n",
        "        res = self.out_layer(final_input) # final output layer\n",
        "        return res\n",
        "\n",
        "    def predict(self,x,device):\n",
        "        # x-->tensor (batch_size,channel,height,width)\n",
        "        # return labels --> tensor (batch_size,1)\n",
        "        accumulate_goodness = torch.zeros(x.shape[0],10).to(device) # accumulate goodness for each label\n",
        "        for i in range(10): # iterate through all labels\n",
        "          interm = x.detach().clone()\n",
        "\n",
        "          # update one-hot label encoding\n",
        "          interm[:,:,0,:10] = 0\n",
        "          interm[:,:,0,i] = 1\n",
        "\n",
        "          for index in range(len(self.layers)-1):\n",
        "            layer = self.layers[index]\n",
        "            interm = layer(interm)\n",
        "\n",
        "            if index > 0: # accumulate goodness for all activities but first hidden layer\n",
        "              accumulate_goodness[:,i] = accumulate_goodness[:,i] + self.goodness(interm)\n",
        "\n",
        "            if index + 1 < len(self.layers): # layernorm\n",
        "              interm = self.ln(interm)\n",
        "        return torch.argmax(accumulate_goodness,dim=1)\n",
        "\n",
        "    def goodness(self,x):\n",
        "        # goodness functions--sum of squares\n",
        "        # input: x-->tensor (batch_size, ...) (arbitrary shape that starts with batch_size)\n",
        "        # return goodness-->tensor (batch_size,1)\n",
        "\n",
        "        x = x.reshape(x.shape[0],-1)\n",
        "        return torch.sum(torch.square(x),dim=1)\n",
        "\n",
        "    def criterion(self,x, threshold):\n",
        "        # criterion function--mean(sigmoid(goodness - threshold))\n",
        "        # input: x-->tensor (batch_size, ...) (arbitrary shape that starts with batch_size)\n",
        "        # return loss-->float\n",
        "\n",
        "        goodness = self.goodness(x)\n",
        "        res = torch.mean(torch.sigmoid(goodness - threshold))\n",
        "        # print(goodness,res)\n",
        "        return res\n",
        "\n",
        "    def construct_supervised_example(self,x,y,positive=True):\n",
        "        # construct positive or negative examples for supervised training\n",
        "        # x-->tensor (batch_size,channel,height,width)\n",
        "        # y-->tensor (batch_size,)\n",
        "        # return ans-->tensor (batch_size,channel,height,width)\n",
        "\n",
        "        ans = x.detach().clone()\n",
        "\n",
        "        if positive:\n",
        "          # one-hot encoding\n",
        "          ans[:,:,0,:10] = 0\n",
        "          for i in range(x.shape[0]):\n",
        "             ans[i,:,0,y[i]] = 1\n",
        "\n",
        "        else:\n",
        "          ans[:,:,0,:10] = 0.1 # initialize neutral label\n",
        "\n",
        "          # run forward pass\n",
        "          labels = self.forward(ans)\n",
        "\n",
        "          for i in range(x.shape[0]):\n",
        "             labels[i][y[i]] = -float('inf') # omit positive label\n",
        "          prob = torch.softmax(labels,dim=1) # probability distribution to choose label\n",
        "          negative_label = torch.multinomial(prob,1).squeeze() # choose from distribution\n",
        "\n",
        "          # generate negative example from forward pass\n",
        "          ans[:,:,0,:10] = 0\n",
        "          for i in range(x.shape[0]):\n",
        "              ans[i,:,0,negative_label[i]] = 1\n",
        "\n",
        "        return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kC0QvPgPfknW"
      },
      "outputs": [],
      "source": [
        "# apply xavier initialization on weights\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-KK7uRkQMKTi"
      },
      "outputs": [],
      "source": [
        "# design loss function\n",
        "def goodness_loss(pos_goodness,neg_goodness):\n",
        "  # loss function intended for maximizing goodness in positive examples\n",
        "  # and minimizing goodness in negative examples\n",
        "  return torch.log(1+torch.exp(neg_goodness-pos_goodness))\n",
        "\n",
        "\n",
        "# CrossEntropyLoss for final softmax layer optimization\n",
        "ceLoss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "K5-yXqsh1-HP"
      },
      "outputs": [],
      "source": [
        "# model evaluation using test set\n",
        "def test(model,test_loader,loss_func):\n",
        "    # return loss-->float, accuracy-->float\n",
        "    model.eval() # switch to eval mode\n",
        "    with torch.no_grad(): # disable gradient calculation\n",
        "        correct = 0\n",
        "        total_loss = 0\n",
        "        total_steps = 0\n",
        "        total = 0\n",
        "        for x, y in test_loader:\n",
        "            x,y = x.to(device),y.to(device)\n",
        "            # use forward to calculate loss\n",
        "            labels = model.forward(x)\n",
        "            loss = loss_func(labels,y)\n",
        "            total_loss += loss\n",
        "            total_steps += 1\n",
        "            # use predict to predict accuracies\n",
        "            predictions = model.predict(x,device)\n",
        "            acc = torch.sum(predictions == y)\n",
        "            correct += acc\n",
        "            total += x.shape[0]\n",
        "    model.train() # switch back to train mode\n",
        "    return total_loss/total_steps, correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZZ-V48mwWNK"
      },
      "source": [
        "## Hyperparemeter Tuning with Ray Tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gvRB08YYgz3"
      },
      "outputs": [],
      "source": [
        "# train function implementing Ray Tune for hyperparameter tuning\n",
        "# train model\n",
        "def train_tune(tune_config, max_num_epochs, data_dir='./data'):\n",
        "\n",
        "    # initialize model\n",
        "    model = net(784)\n",
        "\n",
        "    # configure device\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            model = nn.DataParallel(model)\n",
        "    model.to(device)\n",
        "\n",
        "    # read config\n",
        "    config = {\n",
        "      \"batch_size\": 64,\n",
        "      \"num_epochs\": max_num_epochs,\n",
        "      \"final_loss\": ceLoss,\n",
        "      \"lrs\": [5e-3,7e-5,6e-4,8e-3,1e-4],\n",
        "      \"momentums\": [0.9,0.9,0.9,0.9,0.9],\n",
        "      \"weight_decays\": [1e-5,2e-5,4e-4,1e-4,3e-5],\n",
        "      \"thresholds\": [50,1000,1000,1000],\n",
        "      \"records_per_epoch\": 4,\n",
        "      \"supervised\" : True\n",
        "    }\n",
        "    if \"lrs_0\" in tune_config:\n",
        "      config[\"lrs\"][0] = tune_config[\"lrs_0\"]\n",
        "    if \"lrs_1\" in tune_config:\n",
        "      config[\"lrs\"][1] = tune_config[\"lrs_1\"]\n",
        "    if \"lrs_2\" in tune_config:\n",
        "      config[\"lrs\"][2] = tune_config[\"lrs_2\"]\n",
        "    if \"lrs_3\" in tune_config:\n",
        "      config[\"lrs\"][3] = tune_config[\"lrs_3\"]\n",
        "    if \"lrs_4\" in tune_config:\n",
        "      config[\"lrs\"][4] = tune_config[\"lrs_4\"]\n",
        "    if \"weight_decays_0\" in tune_config:\n",
        "      config[\"weight_decays\"][0] = tune_config[\"weight_decays_0\"]\n",
        "    if \"weight_decays_1\" in tune_config:\n",
        "      config[\"weight_decays\"][1] = tune_config[\"weight_decays_1\"]\n",
        "    if \"weight_decays_2\" in tune_config:\n",
        "      config[\"weight_decays\"][2] = tune_config[\"weight_decays_2\"]\n",
        "    if \"weight_decays_3\" in tune_config:\n",
        "      config[\"weight_decays\"][3] = tune_config[\"weight_decays_3\"]\n",
        "    if \"weight_decays_4\" in tune_config:\n",
        "      config[\"weight_decays\"][4] = tune_config[\"weight_decays_4\"]\n",
        "    if \"thresholds_0\" in tune_config:\n",
        "      config[\"thresholds\"][0] = tune_config[\"thresholds_0\"]\n",
        "    if \"thresholds_1\" in tune_config:\n",
        "      config[\"thresholds\"][1] = tune_config[\"thresholds_1\"]\n",
        "    if \"thresholds_2\" in tune_config:\n",
        "      config[\"thresholds\"][2] = tune_config[\"thresholds_2\"]\n",
        "    if \"thresholds_3\" in tune_config:\n",
        "      config[\"thresholds\"][3] = tune_config[\"thresholds_3\"]\n",
        "\n",
        "    # configure loss function and optimizers\n",
        "    final_loss = config[\"final_loss\"]\n",
        "    pos_optimizers, neg_optimizers, pos_schedulers, neg_schedulers = [],[],[],[]\n",
        "    final_optimizer = torch.optim.SGD(model.layers[-1].parameters(), lr=config['lrs'][-1], momentum = config['momentums'][-1], weight_decay=config['weight_decays'][-1], maximize=False)\n",
        "    # final_scheduler = torch.optim.lr_scheduler.CyclicLR(final_optimizer, base_lr=config['lrs'][-1]/10, max_lr=config['lrs'][-1])\n",
        "    # iterate each layer\n",
        "    for index in range(len(model.layers)):\n",
        "        # layer & training initialization\n",
        "        layer = model.layers[index]\n",
        "        layer.apply(init_weights)\n",
        "        # clip gradients to avoid exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=layer.parameters(), max_norm=1,norm_type=2.0)\n",
        "        # define optimizers\n",
        "        if index < len(model.layers)-1:\n",
        "            pos_optimizers.append(torch.optim.SGD(layer.parameters(), lr=config['lrs'][index], momentum = config['momentums'][index], weight_decay=config['weight_decays'][index], maximize=True))\n",
        "            neg_optimizers.append(torch.optim.SGD(layer.parameters(), lr=config['lrs'][index], momentum = config['momentums'][index], weight_decay=config['weight_decays'][index], maximize=False))\n",
        "            # pos_schedulers.append(torch.optim.lr_scheduler.CyclicLR(pos_optimizers[index], base_lr=config['lrs'][index]/10, max_lr=config['lrs'][index]))\n",
        "            # neg_schedulers.append(torch.optim.lr_scheduler.CyclicLR(neg_optimizers[index], base_lr=config['lrs'][index]/10, max_lr=config['lrs'][index]))\n",
        "\n",
        "    # configure state of model if checkpoint exists\n",
        "    checkpoint = session.get_checkpoint()\n",
        "    if checkpoint:\n",
        "        checkpoint_state = checkpoint.to_dict()\n",
        "        start_epoch = checkpoint_state[\"epoch\"]\n",
        "        net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
        "        for index in range(len(model.layers)-1):\n",
        "          pos_optimizers[index].load_state_dict(checkpoint_state[\"optimizer_state_dict\"][\"pos\"][index])\n",
        "          neg_optimizers[index].load_state_dict(checkpoint_state[\"optimizer_state_dict\"][\"neg\"][index])\n",
        "          # pos_schedulers[index].load_state_dict(checkpoint_state[\"scheduler_state_dict\"][\"pos\"][index])\n",
        "          # neg_schedulers[index].load_state_dict(checkpoint_state[\"scheduler_state_dict\"][\"neg\"][index])\n",
        "        final_optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"][\"final\"])\n",
        "        # final_scheduler.load_state_dict(checkpoint_state[\"scheduler_state_dict\"][\"final\"])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    # load data\n",
        "    train_data, test_data = load_mnist(data_dir)\n",
        "\n",
        "    test_abs = int(len(train_data) * 0.9)\n",
        "    train_subset, val_subset = random_split(\n",
        "        train_data, [test_abs, len(train_data) - test_abs]\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=2\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=2\n",
        "    )\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # variables for training output\n",
        "    # total_pos_goodnesses,total_neg_goodnesses = [],[]\n",
        "    # total_losses = []\n",
        "    # test_accuracies = []\n",
        "    total_step = len(train_loader)\n",
        "    record_period = total_step//records_per_epoch\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        total_pos_goodness,total_neg_goodness = [0]*(len(model.layers)-1),[0]*(len(model.layers)-1)\n",
        "        total_loss = 0\n",
        "        # total_loss = [0]*len(model.layers)\n",
        "        for i,(x,y) in enumerate(train_loader): # iterate through dataset\n",
        "            x,y = x.to(device),y.to(device)\n",
        "            if not config[\"supervised\"]: # unsupervised training input\n",
        "              pos_imgs = x.squeeze()\n",
        "              neg_imgs = generate_negative_example().squeeze().to(device)\n",
        "            else: # supervised learning input\n",
        "              pos_imgs = model.construct_supervised_example(x,y,True)\n",
        "              neg_imgs = model.construct_supervised_example(x,y,False)\n",
        "\n",
        "            # intermediate variables\n",
        "            pos_interms = []\n",
        "            pos_interm = pos_imgs.clone()\n",
        "            neg_interm = neg_imgs.clone()\n",
        "\n",
        "            # iterate over intermediate layers\n",
        "            for index in range(len(model.layers)-1):\n",
        "              layer = model.layers[index]\n",
        "\n",
        "              # positive pass\n",
        "              pos_output = layer(pos_interm)\n",
        "              pos_goodness = model.criterion(pos_output, threshold=config[\"thresholds\"][index])\n",
        "              pos_interm = model.ln(pos_output).detach()\n",
        "              pos_interms.append(pos_interm.clone())\n",
        "\n",
        "              # update variables\n",
        "              total_pos_goodness[index] += pos_goodness\n",
        "\n",
        "              # clear gradients for this training step\n",
        "              pos_optimizers[index].zero_grad()\n",
        "\n",
        "              # take gradient step\n",
        "              pos_goodness.backward()\n",
        "              pos_optimizers[index].step()\n",
        "\n",
        "              # negative pass\n",
        "              neg_output = layer(neg_interm)\n",
        "              neg_goodness = model.criterion(neg_output, threshold=thresholds[index])\n",
        "              neg_interm = model.ln(neg_output).detach()\n",
        "\n",
        "              total_neg_goodness[index] += neg_goodness\n",
        "              neg_optimizers[index].zero_grad()\n",
        "              neg_goodness.backward()\n",
        "              neg_optimizers[index].step()\n",
        "\n",
        "              # update variables\n",
        "              total_pos_goodness[index] += pos_goodness\n",
        "              total_neg_goodness[index] += neg_goodness\n",
        "\n",
        "              # update schedulers\n",
        "              # pos_schedulers[index].step()\n",
        "              # neg_schedulers[index].step()\n",
        "\n",
        "              # check progress\n",
        "              if (i+1) % (record_period) == 0:\n",
        "                  avg_pos_goodness = total_pos_goodness[index]/(record_period)\n",
        "                  avg_neg_goodness = total_neg_goodness[index]/(record_period)\n",
        "                  # if len(total_pos_goodnesses)>index:\n",
        "                  #   total_pos_goodnesses[index].append(avg_pos_goodness)\n",
        "                  # else:\n",
        "                  #   total_pos_goodnesses.append([avg_pos_goodness])\n",
        "                  # if len(total_neg_goodnesses)>index:\n",
        "                  #   total_neg_goodnesses[index].append(avg_neg_goodness)\n",
        "                  # else:\n",
        "                  #   total_neg_goodnesses.append([avg_neg_goodness])\n",
        "\n",
        "                  print ('Layer {}, Epoch [{}/{}], Step [{}/{}], Positive Goodness: {:.4f}, Negative Goodness: {:.4f}'\n",
        "                        .format(index, epoch + 1, num_epochs, i + 1, total_step, avg_pos_goodness, avg_neg_goodness))\n",
        "                  total_pos_goodness[index],total_neg_goodness[index] = 0,0\n",
        "\n",
        "            # output layer\n",
        "            final_input = torch.cat(pos_interms[1:],dim=1)\n",
        "            labels = model.layers[-1](final_input)\n",
        "            loss = ceLoss(labels,y)\n",
        "            total_loss += loss\n",
        "\n",
        "            final_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            final_optimizer.step()\n",
        "            # final_scheduler.step()\n",
        "\n",
        "            if (i+1) % record_period == 0:\n",
        "              avg_loss = total_loss/record_period\n",
        "\n",
        "              print ('Final Layer, Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                    .format(epoch + 1, num_epochs, i + 1, total_step, avg_loss))\n",
        "              total_loss = 0\n",
        "\n",
        "              # validation\n",
        "              val_loss, val_acc = test(model, val_loader, config[\"final_loss\"])\n",
        "              print ('Epoch [{}/{}], Step [{}/{}], Test Accuracy: {:.4f}'\n",
        "                    .format(epoch + 1, num_epochs, i + 1, total_step, val_acc))\n",
        "\n",
        "              # record in ray tune\n",
        "              checkpoint_data = {\n",
        "                  \"config\": config,\n",
        "                  \"epoch\": epoch,\n",
        "                  \"net_state_dict\": model.state_dict(),\n",
        "                  \"optimizer_state_dict\": {\n",
        "                      \"pos\":{}, \"neg\":{}, \"final\": final_optimizer.state_dict()\n",
        "                  }\n",
        "                  # ,\"scheduler_state_dict\":{\n",
        "                  #     \"pos\":{}, \"neg\":{}, \"final\": final_scheduler.state_dict()\n",
        "                  # }\n",
        "              }\n",
        "              for index in range(len(model.layers)-1):\n",
        "                  checkpoint_data[\"optimizer_state_dict\"][\"pos\"][index] = pos_optimizers[index].state_dict()\n",
        "                  checkpoint_data[\"optimizer_state_dict\"][\"neg\"][index] = neg_optimizers[index].state_dict()\n",
        "                  # checkpoint_data[\"scheduler_state_dict\"][\"pos\"][index] = pos_schedulers[index].state_dict()\n",
        "                  # checkpoint_data[\"scheduler_state_dict\"][\"neg\"][index] = neg_schedulers[index].state_dict()\n",
        "\n",
        "              checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
        "\n",
        "              session.report(\n",
        "                  {\"loss\": val_loss.cpu(), \"accuracy\": val_acc.cpu()},\n",
        "                  checkpoint=checkpoint,\n",
        "              )\n",
        "\n",
        "    print(\"finished training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q6zgrTsvekwo",
        "outputId": "c03a8ef5-fd21-432e-d787-dacb70fc20a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-22 09:31:23,918\tINFO worker.py:1612 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "2023-07-22 09:31:26,608\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
            "2023-07-22 09:31:26,616\tINFO tune.py:666 -- [output] This will use the new output engine with verbosity 2. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------+\n",
            "| Configuration for experiment     train_tune_2023-07-22_09-31-27   |\n",
            "+-------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator            |\n",
            "| Scheduler                        AsyncHyperBandScheduler          |\n",
            "| Number of trials                 10                               |\n",
            "+-------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/train_tune_2023-07-22_09-31-27\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/train_tune_2023-07-22_09-31-27`\n",
            "\n",
            "Trial status: 10 PENDING\n",
            "Current time: 2023-07-22 09:31:44. Total running time: 17s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs\n",
            "+---------------------------------------------------------------------+\n",
            "| Trial name               status           lrs_0     weight_decays_0 |\n",
            "+---------------------------------------------------------------------+\n",
            "| train_tune_88374_00000   PENDING    0.000512757         0.000286396 |\n",
            "| train_tune_88374_00001   PENDING    0.000996274         0.00758037  |\n",
            "| train_tune_88374_00002   PENDING    7.66377e-05         0.0013749   |\n",
            "| train_tune_88374_00003   PENDING    0.0225626           0.000118333 |\n",
            "| train_tune_88374_00004   PENDING    2.96833e-05         0.00205915  |\n",
            "| train_tune_88374_00005   PENDING    0.000905822         5.36917e-05 |\n",
            "| train_tune_88374_00006   PENDING    5.76985e-05         0.00545915  |\n",
            "| train_tune_88374_00007   PENDING    2.71157e-05         7.75433e-05 |\n",
            "| train_tune_88374_00008   PENDING    0.00239529          0.00569982  |\n",
            "| train_tune_88374_00009   PENDING    0.0018854           2.00235e-05 |\n",
            "+---------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-22 09:31:45,601\tWARNING worker.py:2006 -- Warning: The actor ImplicitFunc is very large (90 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial train_tune_88374_00000 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 config             |\n",
            "+-------------------------------------------------+\n",
            "| lrs_0                                   0.00051 |\n",
            "| weight_decays_0                         0.00029 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [1/20], Step [211/844], Positive Goodness: 1.5152, Negative Goodness: 1.5193\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [1/20], Step [211/844], Positive Goodness: 1.8842, Negative Goodness: 1.8955\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [1/20], Step [211/844], Positive Goodness: 1.9702, Negative Goodness: 1.9828\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [1/20], Step [211/844], Positive Goodness: 1.9824, Negative Goodness: 1.9884\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [1/20], Step [211/844], Loss: 1.0473\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [1/20], Step [211/844], Test Accuracy: 0.1517\n",
            "Trial train_tune_88374_00000 finished iteration 1 at 2023-07-22 09:32:06. Total running time: 38s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              12.38613 |\n",
            "| time_total_s                                  12.38613 |\n",
            "| training_iteration                                   1 |\n",
            "| accuracy                                tensor(0.1517) |\n",
            "| loss                                    tensor(0.6688) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 1 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000000\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [1/20], Step [422/844], Positive Goodness: 1.5326, Negative Goodness: 1.5327\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [1/20], Step [422/844], Positive Goodness: 1.9283, Negative Goodness: 1.9315\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [1/20], Step [422/844], Positive Goodness: 1.9997, Negative Goodness: 1.9997\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [1/20], Step [422/844], Positive Goodness: 2.0000, Negative Goodness: 1.9999\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [1/20], Step [422/844], Loss: 0.4005\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2023-07-22 09:32:14. Total running time: 47s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status           lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00000   RUNNING    0.000512757         0.000286396        1            12.3861 |\n",
            "| train_tune_88374_00001   PENDING    0.000996274         0.00758037                              |\n",
            "| train_tune_88374_00002   PENDING    7.66377e-05         0.0013749                               |\n",
            "| train_tune_88374_00003   PENDING    0.0225626           0.000118333                             |\n",
            "| train_tune_88374_00004   PENDING    2.96833e-05         0.00205915                              |\n",
            "| train_tune_88374_00005   PENDING    0.000905822         5.36917e-05                             |\n",
            "| train_tune_88374_00006   PENDING    5.76985e-05         0.00545915                              |\n",
            "| train_tune_88374_00007   PENDING    2.71157e-05         7.75433e-05                             |\n",
            "| train_tune_88374_00008   PENDING    0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00009   PENDING    0.0018854           2.00235e-05                             |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [1/20], Step [422/844], Test Accuracy: 0.1462\n",
            "Trial train_tune_88374_00000 finished iteration 2 at 2023-07-22 09:32:15. Total running time: 48s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                               9.62206 |\n",
            "| time_total_s                                  22.00819 |\n",
            "| training_iteration                                   2 |\n",
            "| accuracy                                tensor(0.1462) |\n",
            "| loss                                    tensor(0.5228) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 2 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000001\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [1/20], Step [633/844], Positive Goodness: 1.5302, Negative Goodness: 1.5258\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [1/20], Step [633/844], Positive Goodness: 1.9838, Negative Goodness: 1.9836\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [1/20], Step [633/844], Positive Goodness: 1.9990, Negative Goodness: 1.9992\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [1/20], Step [633/844], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [1/20], Step [633/844], Loss: 0.2905\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [1/20], Step [633/844], Test Accuracy: 0.1395\n",
            "Trial train_tune_88374_00000 finished iteration 3 at 2023-07-22 09:32:26. Total running time: 59s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              10.71556 |\n",
            "| time_total_s                                  32.72375 |\n",
            "| training_iteration                                   3 |\n",
            "| accuracy                                tensor(0.1395) |\n",
            "| loss                                    tensor(0.4647) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 3 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000002\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [1/20], Step [844/844], Positive Goodness: 1.5810, Negative Goodness: 1.5699\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [1/20], Step [844/844], Positive Goodness: 1.9888, Negative Goodness: 1.9892\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [1/20], Step [844/844], Positive Goodness: 1.9998, Negative Goodness: 1.9993\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [1/20], Step [844/844], Positive Goodness: 2.0000, Negative Goodness: 1.9998\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [1/20], Step [844/844], Loss: 0.2395\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [1/20], Step [844/844], Test Accuracy: 0.1373\n",
            "Trial train_tune_88374_00000 finished iteration 4 at 2023-07-22 09:32:35. Total running time: 1min 7s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                               8.44458 |\n",
            "| time_total_s                                  41.16832 |\n",
            "| training_iteration                                   4 |\n",
            "| accuracy                                tensor(0.1373) |\n",
            "| loss                                    tensor(0.4363) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 4 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000003\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [2/20], Step [211/844], Positive Goodness: 1.5977, Negative Goodness: 1.5833\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [2/20], Step [211/844], Positive Goodness: 1.9797, Negative Goodness: 1.9815\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [2/20], Step [211/844], Positive Goodness: 1.9991, Negative Goodness: 1.9995\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [2/20], Step [211/844], Positive Goodness: 1.9998, Negative Goodness: 1.9988\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [2/20], Step [211/844], Loss: 0.2006\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2023-07-22 09:32:44. Total running time: 1min 17s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status           lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00000   RUNNING    0.000512757         0.000286396        4            41.1683 |\n",
            "| train_tune_88374_00001   PENDING    0.000996274         0.00758037                              |\n",
            "| train_tune_88374_00002   PENDING    7.66377e-05         0.0013749                               |\n",
            "| train_tune_88374_00003   PENDING    0.0225626           0.000118333                             |\n",
            "| train_tune_88374_00004   PENDING    2.96833e-05         0.00205915                              |\n",
            "| train_tune_88374_00005   PENDING    0.000905822         5.36917e-05                             |\n",
            "| train_tune_88374_00006   PENDING    5.76985e-05         0.00545915                              |\n",
            "| train_tune_88374_00007   PENDING    2.71157e-05         7.75433e-05                             |\n",
            "| train_tune_88374_00008   PENDING    0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00009   PENDING    0.0018854           2.00235e-05                             |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [2/20], Step [211/844], Test Accuracy: 0.1338\n",
            "Trial train_tune_88374_00000 finished iteration 5 at 2023-07-22 09:32:45. Total running time: 1min 18s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              10.74703 |\n",
            "| time_total_s                                  51.91535 |\n",
            "| training_iteration                                   5 |\n",
            "| accuracy                                tensor(0.1338) |\n",
            "| loss                                    tensor(0.4170) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 5 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000004\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [2/20], Step [422/844], Positive Goodness: 1.5987, Negative Goodness: 1.5734\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [2/20], Step [422/844], Positive Goodness: 1.9891, Negative Goodness: 1.9865\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [2/20], Step [422/844], Positive Goodness: 1.9988, Negative Goodness: 1.9989\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [2/20], Step [422/844], Positive Goodness: 2.0000, Negative Goodness: 1.9997\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [2/20], Step [422/844], Loss: 0.1795\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [2/20], Step [422/844], Test Accuracy: 0.1328\n",
            "Trial train_tune_88374_00000 finished iteration 6 at 2023-07-22 09:32:56. Total running time: 1min 28s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              10.31261 |\n",
            "| time_total_s                                  62.22796 |\n",
            "| training_iteration                                   6 |\n",
            "| accuracy                                tensor(0.1328) |\n",
            "| loss                                    tensor(0.4036) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 6 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000005\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [2/20], Step [633/844], Positive Goodness: 1.5866, Negative Goodness: 1.5582\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [2/20], Step [633/844], Positive Goodness: 1.9803, Negative Goodness: 1.9780\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [2/20], Step [633/844], Positive Goodness: 1.9957, Negative Goodness: 1.9961\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [2/20], Step [633/844], Positive Goodness: 2.0000, Negative Goodness: 1.9997\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [2/20], Step [633/844], Loss: 0.1651\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [2/20], Step [633/844], Test Accuracy: 0.1288\n",
            "Trial train_tune_88374_00000 finished iteration 7 at 2023-07-22 09:33:05. Total running time: 1min 37s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                               8.91592 |\n",
            "| time_total_s                                  71.14388 |\n",
            "| training_iteration                                   7 |\n",
            "| accuracy                                tensor(0.1288) |\n",
            "| loss                                    tensor(0.3896) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 7 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000006\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [2/20], Step [844/844], Positive Goodness: 1.5736, Negative Goodness: 1.5392\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [2/20], Step [844/844], Positive Goodness: 1.8895, Negative Goodness: 1.8873\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [2/20], Step [844/844], Positive Goodness: 1.9985, Negative Goodness: 1.9988\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [2/20], Step [844/844], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [2/20], Step [844/844], Loss: 0.1438\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2023-07-22 09:33:14. Total running time: 1min 47s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status           lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00000   RUNNING    0.000512757         0.000286396        7            71.1439 |\n",
            "| train_tune_88374_00001   PENDING    0.000996274         0.00758037                              |\n",
            "| train_tune_88374_00002   PENDING    7.66377e-05         0.0013749                               |\n",
            "| train_tune_88374_00003   PENDING    0.0225626           0.000118333                             |\n",
            "| train_tune_88374_00004   PENDING    2.96833e-05         0.00205915                              |\n",
            "| train_tune_88374_00005   PENDING    0.000905822         5.36917e-05                             |\n",
            "| train_tune_88374_00006   PENDING    5.76985e-05         0.00545915                              |\n",
            "| train_tune_88374_00007   PENDING    2.71157e-05         7.75433e-05                             |\n",
            "| train_tune_88374_00008   PENDING    0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00009   PENDING    0.0018854           2.00235e-05                             |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [2/20], Step [844/844], Test Accuracy: 0.1273\n",
            "Trial train_tune_88374_00000 finished iteration 8 at 2023-07-22 09:33:15. Total running time: 1min 48s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              10.49438 |\n",
            "| time_total_s                                  81.63827 |\n",
            "| training_iteration                                   8 |\n",
            "| accuracy                                tensor(0.1273) |\n",
            "| loss                                    tensor(0.3804) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 8 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000007\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [3/20], Step [211/844], Positive Goodness: 1.5445, Negative Goodness: 1.5035\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [3/20], Step [211/844], Positive Goodness: 1.5662, Negative Goodness: 1.5826\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [3/20], Step [211/844], Positive Goodness: 1.9560, Negative Goodness: 1.9581\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [3/20], Step [211/844], Positive Goodness: 2.0000, Negative Goodness: 1.9999\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [3/20], Step [211/844], Loss: 0.1317\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [3/20], Step [211/844], Test Accuracy: 0.1258\n",
            "Trial train_tune_88374_00000 finished iteration 9 at 2023-07-22 09:33:27. Total running time: 1min 59s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              11.86638 |\n",
            "| time_total_s                                  93.50464 |\n",
            "| training_iteration                                   9 |\n",
            "| accuracy                                tensor(0.1258) |\n",
            "| loss                                    tensor(0.3740) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 9 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000008\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [3/20], Step [422/844], Positive Goodness: 1.5444, Negative Goodness: 1.4950\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [3/20], Step [422/844], Positive Goodness: 1.5822, Negative Goodness: 1.5775\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [3/20], Step [422/844], Positive Goodness: 1.7982, Negative Goodness: 1.8005\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [3/20], Step [422/844], Positive Goodness: 1.9999, Negative Goodness: 1.9993\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [3/20], Step [422/844], Loss: 0.1226\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [3/20], Step [422/844], Test Accuracy: 0.1185\n",
            "Trial train_tune_88374_00000 finished iteration 10 at 2023-07-22 09:33:36. Total running time: 2min 9s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                               9.14556 |\n",
            "| time_total_s                                 102.65021 |\n",
            "| training_iteration                                  10 |\n",
            "| accuracy                                tensor(0.1185) |\n",
            "| loss                                    tensor(0.3675) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 10 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000009\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [3/20], Step [633/844], Positive Goodness: 1.5710, Negative Goodness: 1.5165\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [3/20], Step [633/844], Positive Goodness: 1.6254, Negative Goodness: 1.6144\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [3/20], Step [633/844], Positive Goodness: 1.8959, Negative Goodness: 1.8793\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [3/20], Step [633/844], Positive Goodness: 2.0000, Negative Goodness: 1.9997\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [3/20], Step [633/844], Loss: 0.1200\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2023-07-22 09:33:44. Total running time: 2min 17s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status           lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00000   RUNNING    0.000512757         0.000286396       10             102.65 |\n",
            "| train_tune_88374_00001   PENDING    0.000996274         0.00758037                              |\n",
            "| train_tune_88374_00002   PENDING    7.66377e-05         0.0013749                               |\n",
            "| train_tune_88374_00003   PENDING    0.0225626           0.000118333                             |\n",
            "| train_tune_88374_00004   PENDING    2.96833e-05         0.00205915                              |\n",
            "| train_tune_88374_00005   PENDING    0.000905822         5.36917e-05                             |\n",
            "| train_tune_88374_00006   PENDING    5.76985e-05         0.00545915                              |\n",
            "| train_tune_88374_00007   PENDING    2.71157e-05         7.75433e-05                             |\n",
            "| train_tune_88374_00008   PENDING    0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00009   PENDING    0.0018854           2.00235e-05                             |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [3/20], Step [633/844], Test Accuracy: 0.1098\n",
            "Trial train_tune_88374_00000 finished iteration 11 at 2023-07-22 09:33:47. Total running time: 2min 19s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                               10.5434 |\n",
            "| time_total_s                                 113.19361 |\n",
            "| training_iteration                                  11 |\n",
            "| accuracy                                tensor(0.1098) |\n",
            "| loss                                    tensor(0.3658) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 11 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000010\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [3/20], Step [844/844], Positive Goodness: 1.5541, Negative Goodness: 1.4936\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [3/20], Step [844/844], Positive Goodness: 1.8471, Negative Goodness: 1.8236\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [3/20], Step [844/844], Positive Goodness: 1.8855, Negative Goodness: 1.8612\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [3/20], Step [844/844], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [3/20], Step [844/844], Loss: 0.1061\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [3/20], Step [844/844], Test Accuracy: 0.1050\n",
            "Trial train_tune_88374_00000 finished iteration 12 at 2023-07-22 09:33:56. Total running time: 2min 28s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                               8.92015 |\n",
            "| time_total_s                                 122.11375 |\n",
            "| training_iteration                                  12 |\n",
            "| accuracy                                tensor(0.1050) |\n",
            "| loss                                    tensor(0.3652) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 12 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000011\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [4/20], Step [211/844], Positive Goodness: 1.5692, Negative Goodness: 1.4947\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [4/20], Step [211/844], Positive Goodness: 1.6525, Negative Goodness: 1.6230\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [4/20], Step [211/844], Positive Goodness: 1.9821, Negative Goodness: 1.8926\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [4/20], Step [211/844], Positive Goodness: 2.0000, Negative Goodness: 1.9997\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [4/20], Step [211/844], Loss: 0.1049\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [4/20], Step [211/844], Test Accuracy: 0.1090\n",
            "Trial train_tune_88374_00000 finished iteration 13 at 2023-07-22 09:34:06. Total running time: 2min 38s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              10.34409 |\n",
            "| time_total_s                                 132.45784 |\n",
            "| training_iteration                                  13 |\n",
            "| accuracy                                tensor(0.1090) |\n",
            "| loss                                    tensor(0.3575) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 13 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000012\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [4/20], Step [422/844], Positive Goodness: 1.5583, Negative Goodness: 1.4793\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [4/20], Step [422/844], Positive Goodness: 1.7227, Negative Goodness: 1.6600\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [4/20], Step [422/844], Positive Goodness: 1.9747, Negative Goodness: 1.8585\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [4/20], Step [422/844], Positive Goodness: 2.0000, Negative Goodness: 1.9997\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [4/20], Step [422/844], Loss: 0.0963\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2023-07-22 09:34:14. Total running time: 2min 47s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status           lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00000   RUNNING    0.000512757         0.000286396       13            132.458 |\n",
            "| train_tune_88374_00001   PENDING    0.000996274         0.00758037                              |\n",
            "| train_tune_88374_00002   PENDING    7.66377e-05         0.0013749                               |\n",
            "| train_tune_88374_00003   PENDING    0.0225626           0.000118333                             |\n",
            "| train_tune_88374_00004   PENDING    2.96833e-05         0.00205915                              |\n",
            "| train_tune_88374_00005   PENDING    0.000905822         5.36917e-05                             |\n",
            "| train_tune_88374_00006   PENDING    5.76985e-05         0.00545915                              |\n",
            "| train_tune_88374_00007   PENDING    2.71157e-05         7.75433e-05                             |\n",
            "| train_tune_88374_00008   PENDING    0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00009   PENDING    0.0018854           2.00235e-05                             |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [4/20], Step [422/844], Test Accuracy: 0.1123\n",
            "Trial train_tune_88374_00000 finished iteration 14 at 2023-07-22 09:34:17. Total running time: 2min 50s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              11.34344 |\n",
            "| time_total_s                                 143.80128 |\n",
            "| training_iteration                                  14 |\n",
            "| accuracy                                tensor(0.1123) |\n",
            "| loss                                    tensor(0.3535) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 14 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000013\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [4/20], Step [633/844], Positive Goodness: 1.5397, Negative Goodness: 1.4552\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [4/20], Step [633/844], Positive Goodness: 1.5572, Negative Goodness: 1.4785\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [4/20], Step [633/844], Positive Goodness: 1.9649, Negative Goodness: 1.8308\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [4/20], Step [633/844], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [4/20], Step [633/844], Loss: 0.0948\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [4/20], Step [633/844], Test Accuracy: 0.1133\n",
            "Trial train_tune_88374_00000 finished iteration 15 at 2023-07-22 09:34:28. Total running time: 3min 0s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              10.54546 |\n",
            "| time_total_s                                 154.34674 |\n",
            "| training_iteration                                  15 |\n",
            "| accuracy                                tensor(0.1133) |\n",
            "| loss                                    tensor(0.3522) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 15 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000014\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [4/20], Step [844/844], Positive Goodness: 1.4730, Negative Goodness: 1.3755\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [4/20], Step [844/844], Positive Goodness: 1.5349, Negative Goodness: 1.4105\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [4/20], Step [844/844], Positive Goodness: 1.9061, Negative Goodness: 1.7631\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [4/20], Step [844/844], Positive Goodness: 2.0000, Negative Goodness: 1.9996\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [4/20], Step [844/844], Loss: 0.0876\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [4/20], Step [844/844], Test Accuracy: 0.1125\n",
            "Trial train_tune_88374_00000 finished iteration 16 at 2023-07-22 09:34:38. Total running time: 3min 10s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              10.19793 |\n",
            "| time_total_s                                 164.54467 |\n",
            "| training_iteration                                  16 |\n",
            "| accuracy                                tensor(0.1125) |\n",
            "| loss                                    tensor(0.3541) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 16 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000015\n",
            "\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2023-07-22 09:34:45. Total running time: 3min 17s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status           lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00000   RUNNING    0.000512757         0.000286396       16            164.545 |\n",
            "| train_tune_88374_00001   PENDING    0.000996274         0.00758037                              |\n",
            "| train_tune_88374_00002   PENDING    7.66377e-05         0.0013749                               |\n",
            "| train_tune_88374_00003   PENDING    0.0225626           0.000118333                             |\n",
            "| train_tune_88374_00004   PENDING    2.96833e-05         0.00205915                              |\n",
            "| train_tune_88374_00005   PENDING    0.000905822         5.36917e-05                             |\n",
            "| train_tune_88374_00006   PENDING    5.76985e-05         0.00545915                              |\n",
            "| train_tune_88374_00007   PENDING    2.71157e-05         7.75433e-05                             |\n",
            "| train_tune_88374_00008   PENDING    0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00009   PENDING    0.0018854           2.00235e-05                             |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [5/20], Step [211/844], Positive Goodness: 1.5346, Negative Goodness: 1.4260\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [5/20], Step [211/844], Positive Goodness: 1.5777, Negative Goodness: 1.4124\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [5/20], Step [211/844], Positive Goodness: 1.9160, Negative Goodness: 1.7767\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [5/20], Step [211/844], Positive Goodness: 2.0000, Negative Goodness: 1.9996\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [5/20], Step [211/844], Loss: 0.0882\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [5/20], Step [211/844], Test Accuracy: 0.1112\n",
            "Trial train_tune_88374_00000 finished iteration 17 at 2023-07-22 09:34:49. Total running time: 3min 22s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              11.08816 |\n",
            "| time_total_s                                 175.63282 |\n",
            "| training_iteration                                  17 |\n",
            "| accuracy                                tensor(0.1112) |\n",
            "| loss                                    tensor(0.3497) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 17 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000016\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [5/20], Step [422/844], Positive Goodness: 1.5017, Negative Goodness: 1.3777\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [5/20], Step [422/844], Positive Goodness: 1.5585, Negative Goodness: 1.3400\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [5/20], Step [422/844], Positive Goodness: 1.9696, Negative Goodness: 1.7964\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [5/20], Step [422/844], Positive Goodness: 2.0000, Negative Goodness: 1.9996\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [5/20], Step [422/844], Loss: 0.0804\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [5/20], Step [422/844], Test Accuracy: 0.1152\n",
            "Trial train_tune_88374_00000 finished iteration 18 at 2023-07-22 09:34:58. Total running time: 3min 30s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                               8.67257 |\n",
            "| time_total_s                                 184.30539 |\n",
            "| training_iteration                                  18 |\n",
            "| accuracy                                tensor(0.1152) |\n",
            "| loss                                    tensor(0.3465) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 18 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000017\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [5/20], Step [633/844], Positive Goodness: 1.4966, Negative Goodness: 1.3720\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [5/20], Step [633/844], Positive Goodness: 1.5866, Negative Goodness: 1.3383\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [5/20], Step [633/844], Positive Goodness: 1.9345, Negative Goodness: 1.7608\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [5/20], Step [633/844], Positive Goodness: 2.0000, Negative Goodness: 1.9996\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [5/20], Step [633/844], Loss: 0.0803\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [5/20], Step [633/844], Test Accuracy: 0.1142\n",
            "Trial train_tune_88374_00000 finished iteration 19 at 2023-07-22 09:35:08. Total running time: 3min 41s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              10.66227 |\n",
            "| time_total_s                                 194.96767 |\n",
            "| training_iteration                                  19 |\n",
            "| accuracy                                tensor(0.1142) |\n",
            "| loss                                    tensor(0.3511) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 19 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000018\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 0, Epoch [5/20], Step [844/844], Positive Goodness: 1.4745, Negative Goodness: 1.3215\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 1, Epoch [5/20], Step [844/844], Positive Goodness: 1.5265, Negative Goodness: 1.2065\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 2, Epoch [5/20], Step [844/844], Positive Goodness: 1.9534, Negative Goodness: 1.7827\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Layer 3, Epoch [5/20], Step [844/844], Positive Goodness: 2.0000, Negative Goodness: 1.9994\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Final Layer, Epoch [5/20], Step [844/844], Loss: 0.0774\n",
            "Trial status: 1 RUNNING | 9 PENDING\n",
            "Current time: 2023-07-22 09:35:15. Total running time: 3min 47s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status           lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00000   RUNNING    0.000512757         0.000286396       19            194.968 |\n",
            "| train_tune_88374_00001   PENDING    0.000996274         0.00758037                              |\n",
            "| train_tune_88374_00002   PENDING    7.66377e-05         0.0013749                               |\n",
            "| train_tune_88374_00003   PENDING    0.0225626           0.000118333                             |\n",
            "| train_tune_88374_00004   PENDING    2.96833e-05         0.00205915                              |\n",
            "| train_tune_88374_00005   PENDING    0.000905822         5.36917e-05                             |\n",
            "| train_tune_88374_00006   PENDING    5.76985e-05         0.00545915                              |\n",
            "| train_tune_88374_00007   PENDING    2.71157e-05         7.75433e-05                             |\n",
            "| train_tune_88374_00008   PENDING    0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00009   PENDING    0.0018854           2.00235e-05                             |\n",
            "+-------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=31386)\u001b[0m Epoch [5/20], Step [844/844], Test Accuracy: 0.1147\n",
            "Trial train_tune_88374_00000 finished iteration 20 at 2023-07-22 09:35:18. Total running time: 3min 51s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00000 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                               9.82604 |\n",
            "| time_total_s                                 204.79371 |\n",
            "| training_iteration                                  20 |\n",
            "| accuracy                                tensor(0.1147) |\n",
            "| loss                                    tensor(0.3437) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00000 saved a checkpoint for iteration 20 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00000_0_lrs_0=0.0005,weight_decays_0=0.0003_2023-07-22_09-31-35/checkpoint_000019\n",
            "\n",
            "Trial train_tune_88374_00000 completed after 20 iterations at 2023-07-22 09:35:18. Total running time: 3min 51s\n",
            "\n",
            "Trial train_tune_88374_00001 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_tune_88374_00001 config             |\n",
            "+-------------------------------------------------+\n",
            "| lrs_0                                     0.001 |\n",
            "| weight_decays_0                         0.00758 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=32590)\u001b[0m Layer 0, Epoch [1/20], Step [211/844], Positive Goodness: 1.5504, Negative Goodness: 1.5592\n",
            "\u001b[2m\u001b[36m(func pid=32590)\u001b[0m Layer 1, Epoch [1/20], Step [211/844], Positive Goodness: 1.7447, Negative Goodness: 1.7467\n",
            "\u001b[2m\u001b[36m(func pid=32590)\u001b[0m Layer 2, Epoch [1/20], Step [211/844], Positive Goodness: 1.9645, Negative Goodness: 1.9741\n",
            "\u001b[2m\u001b[36m(func pid=32590)\u001b[0m Layer 3, Epoch [1/20], Step [211/844], Positive Goodness: 1.9729, Negative Goodness: 1.9784\n",
            "\u001b[2m\u001b[36m(func pid=32590)\u001b[0m Final Layer, Epoch [1/20], Step [211/844], Loss: 0.9510\n",
            "\u001b[2m\u001b[36m(func pid=32590)\u001b[0m Epoch [1/20], Step [211/844], Test Accuracy: 0.1527\n",
            "Trial train_tune_88374_00001 finished iteration 1 at 2023-07-22 09:35:38. Total running time: 4min 10s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00001 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              13.37651 |\n",
            "| time_total_s                                  13.37651 |\n",
            "| training_iteration                                   1 |\n",
            "| accuracy                                tensor(0.1527) |\n",
            "| loss                                    tensor(0.6721) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00001 saved a checkpoint for iteration 1 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00001_1_lrs_0=0.0010,weight_decays_0=0.0076_2023-07-22_09-31-36/checkpoint_000000\n",
            "\n",
            "Trial train_tune_88374_00001 completed after 1 iterations at 2023-07-22 09:35:38. Total running time: 4min 10s\n",
            "\n",
            "Trial train_tune_88374_00002 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_tune_88374_00002 config             |\n",
            "+-------------------------------------------------+\n",
            "| lrs_0                                     8e-05 |\n",
            "| weight_decays_0                         0.00137 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "Trial status: 2 TERMINATED | 1 RUNNING | 7 PENDING\n",
            "Current time: 2023-07-22 09:35:45. Total running time: 4min 17s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status             lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00002   RUNNING      7.66377e-05         0.0013749                               |\n",
            "| train_tune_88374_00000   TERMINATED   0.000512757         0.000286396       20           204.794  |\n",
            "| train_tune_88374_00001   TERMINATED   0.000996274         0.00758037         1            13.3765 |\n",
            "| train_tune_88374_00003   PENDING      0.0225626           0.000118333                             |\n",
            "| train_tune_88374_00004   PENDING      2.96833e-05         0.00205915                              |\n",
            "| train_tune_88374_00005   PENDING      0.000905822         5.36917e-05                             |\n",
            "| train_tune_88374_00006   PENDING      5.76985e-05         0.00545915                              |\n",
            "| train_tune_88374_00007   PENDING      2.71157e-05         7.75433e-05                             |\n",
            "| train_tune_88374_00008   PENDING      0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00009   PENDING      0.0018854           2.00235e-05                             |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=32739)\u001b[0m Layer 3, Epoch [1/20], Step [211/844], Positive Goodness: 1.9930, Negative Goodness: 1.9985\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=32739)\u001b[0m Final Layer, Epoch [1/20], Step [211/844], Loss: 1.0401\n",
            "\u001b[2m\u001b[36m(func pid=32739)\u001b[0m Epoch [1/20], Step [211/844], Test Accuracy: 0.1203\n",
            "Trial train_tune_88374_00002 finished iteration 1 at 2023-07-22 09:35:56. Total running time: 4min 28s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00002 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                               13.2354 |\n",
            "| time_total_s                                   13.2354 |\n",
            "| training_iteration                                   1 |\n",
            "| accuracy                                tensor(0.1203) |\n",
            "| loss                                    tensor(0.6671) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00002 saved a checkpoint for iteration 1 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00002_2_lrs_0=0.0001,weight_decays_0=0.0014_2023-07-22_09-31-37/checkpoint_000000\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=32739)\u001b[0m Final Layer, Epoch [1/20], Step [422/844], Loss: 0.3882\n",
            "\u001b[2m\u001b[36m(func pid=32739)\u001b[0m Layer 3, Epoch [1/20], Step [422/844], Positive Goodness: 2.0000, Negative Goodness: 2.0000\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=32739)\u001b[0m Epoch [1/20], Step [422/844], Test Accuracy: 0.1202\n",
            "Trial train_tune_88374_00002 finished iteration 2 at 2023-07-22 09:36:05. Total running time: 4min 37s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00002 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                               9.17585 |\n",
            "| time_total_s                                  22.41125 |\n",
            "| training_iteration                                   2 |\n",
            "| accuracy                                tensor(0.1202) |\n",
            "| loss                                    tensor(0.5261) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00002 saved a checkpoint for iteration 2 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00002_2_lrs_0=0.0001,weight_decays_0=0.0014_2023-07-22_09-31-37/checkpoint_000001\n",
            "\n",
            "Trial train_tune_88374_00002 completed after 2 iterations at 2023-07-22 09:36:05. Total running time: 4min 37s\n",
            "\n",
            "Trial train_tune_88374_00003 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_tune_88374_00003 config             |\n",
            "+-------------------------------------------------+\n",
            "| lrs_0                                   0.02256 |\n",
            "| weight_decays_0                         0.00012 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "Trial status: 3 TERMINATED | 1 RUNNING | 6 PENDING\n",
            "Current time: 2023-07-22 09:36:15. Total running time: 4min 47s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status             lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00003   RUNNING      0.0225626           0.000118333                             |\n",
            "| train_tune_88374_00000   TERMINATED   0.000512757         0.000286396       20           204.794  |\n",
            "| train_tune_88374_00001   TERMINATED   0.000996274         0.00758037         1            13.3765 |\n",
            "| train_tune_88374_00002   TERMINATED   7.66377e-05         0.0013749          2            22.4112 |\n",
            "| train_tune_88374_00004   PENDING      2.96833e-05         0.00205915                              |\n",
            "| train_tune_88374_00005   PENDING      0.000905822         5.36917e-05                             |\n",
            "| train_tune_88374_00006   PENDING      5.76985e-05         0.00545915                              |\n",
            "| train_tune_88374_00007   PENDING      2.71157e-05         7.75433e-05                             |\n",
            "| train_tune_88374_00008   PENDING      0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00009   PENDING      0.0018854           2.00235e-05                             |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=32938)\u001b[0m Final Layer, Epoch [1/20], Step [211/844], Loss: 1.0968\n",
            "\u001b[2m\u001b[36m(func pid=32938)\u001b[0m Layer 3, Epoch [1/20], Step [211/844], Positive Goodness: 1.9971, Negative Goodness: 1.9994\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=32938)\u001b[0m Epoch [1/20], Step [211/844], Test Accuracy: 0.0630\n",
            "Trial train_tune_88374_00003 finished iteration 1 at 2023-07-22 09:36:25. Total running time: 4min 57s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00003 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              13.29113 |\n",
            "| time_total_s                                  13.29113 |\n",
            "| training_iteration                                   1 |\n",
            "| accuracy                                tensor(0.0630) |\n",
            "| loss                                    tensor(0.7671) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00003 saved a checkpoint for iteration 1 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00003_3_lrs_0=0.0226,weight_decays_0=0.0001_2023-07-22_09-31-38/checkpoint_000000\n",
            "\n",
            "Trial train_tune_88374_00003 completed after 1 iterations at 2023-07-22 09:36:25. Total running time: 4min 57s\n",
            "\n",
            "Trial train_tune_88374_00004 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_tune_88374_00004 config             |\n",
            "+-------------------------------------------------+\n",
            "| lrs_0                                     3e-05 |\n",
            "| weight_decays_0                         0.00206 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=33091)\u001b[0m Final Layer, Epoch [1/20], Step [211/844], Loss: 1.0045\n",
            "\u001b[2m\u001b[36m(func pid=33091)\u001b[0m Layer 3, Epoch [1/20], Step [211/844], Positive Goodness: 1.9973, Negative Goodness: 1.9994\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=33091)\u001b[0m Epoch [1/20], Step [211/844], Test Accuracy: 0.1170\n",
            "Trial train_tune_88374_00004 finished iteration 1 at 2023-07-22 09:36:43. Total running time: 5min 16s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00004 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              13.06766 |\n",
            "| time_total_s                                  13.06766 |\n",
            "| training_iteration                                   1 |\n",
            "| accuracy                                tensor(0.1170) |\n",
            "| loss                                    tensor(0.6848) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00004 saved a checkpoint for iteration 1 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00004_4_lrs_0=0.0000,weight_decays_0=0.0021_2023-07-22_09-31-38/checkpoint_000000\n",
            "\n",
            "Trial train_tune_88374_00004 completed after 1 iterations at 2023-07-22 09:36:43. Total running time: 5min 16s\n",
            "\n",
            "Trial status: 5 TERMINATED | 5 PENDING\n",
            "Current time: 2023-07-22 09:36:45. Total running time: 5min 17s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status             lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00000   TERMINATED   0.000512757         0.000286396       20           204.794  |\n",
            "| train_tune_88374_00001   TERMINATED   0.000996274         0.00758037         1            13.3765 |\n",
            "| train_tune_88374_00002   TERMINATED   7.66377e-05         0.0013749          2            22.4112 |\n",
            "| train_tune_88374_00003   TERMINATED   0.0225626           0.000118333        1            13.2911 |\n",
            "| train_tune_88374_00004   TERMINATED   2.96833e-05         0.00205915         1            13.0677 |\n",
            "| train_tune_88374_00005   PENDING      0.000905822         5.36917e-05                             |\n",
            "| train_tune_88374_00006   PENDING      5.76985e-05         0.00545915                              |\n",
            "| train_tune_88374_00007   PENDING      2.71157e-05         7.75433e-05                             |\n",
            "| train_tune_88374_00008   PENDING      0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00009   PENDING      0.0018854           2.00235e-05                             |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00005 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_tune_88374_00005 config             |\n",
            "+-------------------------------------------------+\n",
            "| lrs_0                                   0.00091 |\n",
            "| weight_decays_0                           5e-05 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=33236)\u001b[0m Final Layer, Epoch [1/20], Step [211/844], Loss: 1.0288\n",
            "\u001b[2m\u001b[36m(func pid=33236)\u001b[0m Layer 3, Epoch [1/20], Step [211/844], Positive Goodness: 1.9964, Negative Goodness: 1.9996\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=33236)\u001b[0m Epoch [1/20], Step [211/844], Test Accuracy: 0.1232\n",
            "Trial train_tune_88374_00005 finished iteration 1 at 2023-07-22 09:37:04. Total running time: 5min 37s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00005 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              15.79842 |\n",
            "| time_total_s                                  15.79842 |\n",
            "| training_iteration                                   1 |\n",
            "| accuracy                                tensor(0.1232) |\n",
            "| loss                                    tensor(0.6802) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00005 saved a checkpoint for iteration 1 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00005_5_lrs_0=0.0009,weight_decays_0=0.0001_2023-07-22_09-31-39/checkpoint_000000\n",
            "\n",
            "Trial train_tune_88374_00005 completed after 1 iterations at 2023-07-22 09:37:04. Total running time: 5min 37s\n",
            "\n",
            "Trial train_tune_88374_00006 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_tune_88374_00006 config             |\n",
            "+-------------------------------------------------+\n",
            "| lrs_0                                     6e-05 |\n",
            "| weight_decays_0                         0.00546 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "Trial status: 6 TERMINATED | 1 RUNNING | 3 PENDING\n",
            "Current time: 2023-07-22 09:37:15. Total running time: 5min 47s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status             lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00006   RUNNING      5.76985e-05         0.00545915                              |\n",
            "| train_tune_88374_00000   TERMINATED   0.000512757         0.000286396       20           204.794  |\n",
            "| train_tune_88374_00001   TERMINATED   0.000996274         0.00758037         1            13.3765 |\n",
            "| train_tune_88374_00002   TERMINATED   7.66377e-05         0.0013749          2            22.4112 |\n",
            "| train_tune_88374_00003   TERMINATED   0.0225626           0.000118333        1            13.2911 |\n",
            "| train_tune_88374_00004   TERMINATED   2.96833e-05         0.00205915         1            13.0677 |\n",
            "| train_tune_88374_00005   TERMINATED   0.000905822         5.36917e-05        1            15.7984 |\n",
            "| train_tune_88374_00007   PENDING      2.71157e-05         7.75433e-05                             |\n",
            "| train_tune_88374_00008   PENDING      0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00009   PENDING      0.0018854           2.00235e-05                             |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=33391)\u001b[0m Layer 2, Epoch [1/20], Step [211/844], Positive Goodness: 0.0307, Negative Goodness: 0.0342\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=33391)\u001b[0m Final Layer, Epoch [1/20], Step [211/844], Loss: 1.0366\n",
            "\u001b[2m\u001b[36m(func pid=33391)\u001b[0m Epoch [1/20], Step [211/844], Test Accuracy: 0.1685\n",
            "Trial train_tune_88374_00006 finished iteration 1 at 2023-07-22 09:37:22. Total running time: 5min 55s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00006 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              12.95523 |\n",
            "| time_total_s                                  12.95523 |\n",
            "| training_iteration                                   1 |\n",
            "| accuracy                                tensor(0.1685) |\n",
            "| loss                                    tensor(0.6644) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00006 saved a checkpoint for iteration 1 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00006_6_lrs_0=0.0001,weight_decays_0=0.0055_2023-07-22_09-31-40/checkpoint_000000\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=33391)\u001b[0m Final Layer, Epoch [1/20], Step [422/844], Loss: 0.4147\n",
            "\u001b[2m\u001b[36m(func pid=33391)\u001b[0m Layer 3, Epoch [1/20], Step [422/844], Positive Goodness: 1.6019, Negative Goodness: 1.6243\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=33391)\u001b[0m Epoch [1/20], Step [422/844], Test Accuracy: 0.1798\n",
            "Trial train_tune_88374_00006 finished iteration 2 at 2023-07-22 09:37:33. Total running time: 6min 5s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00006 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              10.40085 |\n",
            "| time_total_s                                  23.35608 |\n",
            "| training_iteration                                   2 |\n",
            "| accuracy                                tensor(0.1798) |\n",
            "| loss                                    tensor(0.5512) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00006 saved a checkpoint for iteration 2 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00006_6_lrs_0=0.0001,weight_decays_0=0.0055_2023-07-22_09-31-40/checkpoint_000001\n",
            "\n",
            "Trial train_tune_88374_00006 completed after 2 iterations at 2023-07-22 09:37:33. Total running time: 6min 5s\n",
            "\n",
            "Trial train_tune_88374_00007 started with configuration:\n",
            "+-----------------------------------------------+\n",
            "| Trial train_tune_88374_00007 config           |\n",
            "+-----------------------------------------------+\n",
            "| lrs_0                                   3e-05 |\n",
            "| weight_decays_0                         8e-05 |\n",
            "+-----------------------------------------------+\n",
            "\n",
            "Trial status: 7 TERMINATED | 1 RUNNING | 2 PENDING\n",
            "Current time: 2023-07-22 09:37:45. Total running time: 6min 17s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status             lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00007   RUNNING      2.71157e-05         7.75433e-05                             |\n",
            "| train_tune_88374_00000   TERMINATED   0.000512757         0.000286396       20           204.794  |\n",
            "| train_tune_88374_00001   TERMINATED   0.000996274         0.00758037         1            13.3765 |\n",
            "| train_tune_88374_00002   TERMINATED   7.66377e-05         0.0013749          2            22.4112 |\n",
            "| train_tune_88374_00003   TERMINATED   0.0225626           0.000118333        1            13.2911 |\n",
            "| train_tune_88374_00004   TERMINATED   2.96833e-05         0.00205915         1            13.0677 |\n",
            "| train_tune_88374_00005   TERMINATED   0.000905822         5.36917e-05        1            15.7984 |\n",
            "| train_tune_88374_00006   TERMINATED   5.76985e-05         0.00545915         2            23.3561 |\n",
            "| train_tune_88374_00008   PENDING      0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00009   PENDING      0.0018854           2.00235e-05                             |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=33594)\u001b[0m Layer 2, Epoch [1/20], Step [211/844], Positive Goodness: 1.9455, Negative Goodness: 1.9406\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=33594)\u001b[0m Final Layer, Epoch [1/20], Step [211/844], Loss: 0.9542\n",
            "\u001b[2m\u001b[36m(func pid=33594)\u001b[0m Epoch [1/20], Step [211/844], Test Accuracy: 0.1763\n",
            "Trial train_tune_88374_00007 finished iteration 1 at 2023-07-22 09:37:52. Total running time: 6min 24s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00007 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              13.26614 |\n",
            "| time_total_s                                  13.26614 |\n",
            "| training_iteration                                   1 |\n",
            "| accuracy                                tensor(0.1763) |\n",
            "| loss                                    tensor(0.6713) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00007 saved a checkpoint for iteration 1 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00007_7_lrs_0=0.0000,weight_decays_0=0.0001_2023-07-22_09-31-41/checkpoint_000000\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=33594)\u001b[0m Final Layer, Epoch [1/20], Step [422/844], Loss: 0.3680\n",
            "\u001b[2m\u001b[36m(func pid=33594)\u001b[0m Layer 3, Epoch [1/20], Step [422/844], Positive Goodness: 1.9998, Negative Goodness: 2.0000\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=33594)\u001b[0m Epoch [1/20], Step [422/844], Test Accuracy: 0.1673\n",
            "Trial train_tune_88374_00007 finished iteration 2 at 2023-07-22 09:38:02. Total running time: 6min 34s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00007 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              10.34612 |\n",
            "| time_total_s                                  23.61226 |\n",
            "| training_iteration                                   2 |\n",
            "| accuracy                                tensor(0.1673) |\n",
            "| loss                                    tensor(0.5376) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00007 saved a checkpoint for iteration 2 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00007_7_lrs_0=0.0000,weight_decays_0=0.0001_2023-07-22_09-31-41/checkpoint_000001\n",
            "\n",
            "Trial train_tune_88374_00007 completed after 2 iterations at 2023-07-22 09:38:02. Total running time: 6min 35s\n",
            "\n",
            "Trial train_tune_88374_00008 started with configuration:\n",
            "+------------------------------------------------+\n",
            "| Trial train_tune_88374_00008 config            |\n",
            "+------------------------------------------------+\n",
            "| lrs_0                                   0.0024 |\n",
            "| weight_decays_0                         0.0057 |\n",
            "+------------------------------------------------+\n",
            "\n",
            "Trial status: 8 TERMINATED | 1 RUNNING | 1 PENDING\n",
            "Current time: 2023-07-22 09:38:15. Total running time: 6min 47s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status             lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00008   RUNNING      0.00239529          0.00569982                              |\n",
            "| train_tune_88374_00000   TERMINATED   0.000512757         0.000286396       20           204.794  |\n",
            "| train_tune_88374_00001   TERMINATED   0.000996274         0.00758037         1            13.3765 |\n",
            "| train_tune_88374_00002   TERMINATED   7.66377e-05         0.0013749          2            22.4112 |\n",
            "| train_tune_88374_00003   TERMINATED   0.0225626           0.000118333        1            13.2911 |\n",
            "| train_tune_88374_00004   TERMINATED   2.96833e-05         0.00205915         1            13.0677 |\n",
            "| train_tune_88374_00005   TERMINATED   0.000905822         5.36917e-05        1            15.7984 |\n",
            "| train_tune_88374_00006   TERMINATED   5.76985e-05         0.00545915         2            23.3561 |\n",
            "| train_tune_88374_00007   TERMINATED   2.71157e-05         7.75433e-05        2            23.6123 |\n",
            "| train_tune_88374_00009   PENDING      0.0018854           2.00235e-05                             |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=33797)\u001b[0m Final Layer, Epoch [1/20], Step [211/844], Loss: 1.0546\n",
            "\u001b[2m\u001b[36m(func pid=33797)\u001b[0m Layer 3, Epoch [1/20], Step [211/844], Positive Goodness: 1.9966, Negative Goodness: 1.9996\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=33797)\u001b[0m Epoch [1/20], Step [211/844], Test Accuracy: 0.1043\n",
            "Trial train_tune_88374_00008 finished iteration 1 at 2023-07-22 09:38:21. Total running time: 6min 53s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00008 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              13.15474 |\n",
            "| time_total_s                                  13.15474 |\n",
            "| training_iteration                                   1 |\n",
            "| accuracy                                tensor(0.1043) |\n",
            "| loss                                    tensor(0.7153) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00008 saved a checkpoint for iteration 1 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00008_8_lrs_0=0.0024,weight_decays_0=0.0057_2023-07-22_09-31-42/checkpoint_000000\n",
            "\n",
            "Trial train_tune_88374_00008 completed after 1 iterations at 2023-07-22 09:38:21. Total running time: 6min 53s\n",
            "\n",
            "Trial train_tune_88374_00009 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_tune_88374_00009 config             |\n",
            "+-------------------------------------------------+\n",
            "| lrs_0                                   0.00189 |\n",
            "| weight_decays_0                           2e-05 |\n",
            "+-------------------------------------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(func pid=33944)\u001b[0m Final Layer, Epoch [1/20], Step [211/844], Loss: 0.9831\n",
            "\u001b[2m\u001b[36m(func pid=33944)\u001b[0m Layer 3, Epoch [1/20], Step [211/844], Positive Goodness: 0.0699, Negative Goodness: 0.0763\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=33944)\u001b[0m Epoch [1/20], Step [211/844], Test Accuracy: 0.1617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-22 09:38:39,251\tWARNING experiment_analysis.py:783 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial train_tune_88374_00009 finished iteration 1 at 2023-07-22 09:38:39. Total running time: 7min 11s\n",
            "+--------------------------------------------------------+\n",
            "| Trial train_tune_88374_00009 result                    |\n",
            "+--------------------------------------------------------+\n",
            "| time_this_iter_s                              13.11707 |\n",
            "| time_total_s                                  13.11707 |\n",
            "| training_iteration                                   1 |\n",
            "| accuracy                                tensor(0.1617) |\n",
            "| loss                                    tensor(0.6930) |\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Trial train_tune_88374_00009 saved a checkpoint for iteration 1 at: /root/ray_results/train_tune_2023-07-22_09-31-27/train_tune_88374_00009_9_lrs_0=0.0019,weight_decays_0=0.0000_2023-07-22_09-31-43/checkpoint_000000\n",
            "\n",
            "Trial train_tune_88374_00009 completed after 1 iterations at 2023-07-22 09:38:39. Total running time: 7min 11s\n",
            "\n",
            "Trial status: 10 TERMINATED\n",
            "Current time: 2023-07-22 09:38:39. Total running time: 7min 11s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| Trial name               status             lrs_0     weight_decays_0     iter     total time (s) |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "| train_tune_88374_00000   TERMINATED   0.000512757         0.000286396       20           204.794  |\n",
            "| train_tune_88374_00001   TERMINATED   0.000996274         0.00758037         1            13.3765 |\n",
            "| train_tune_88374_00002   TERMINATED   7.66377e-05         0.0013749          2            22.4112 |\n",
            "| train_tune_88374_00003   TERMINATED   0.0225626           0.000118333        1            13.2911 |\n",
            "| train_tune_88374_00004   TERMINATED   2.96833e-05         0.00205915         1            13.0677 |\n",
            "| train_tune_88374_00005   TERMINATED   0.000905822         5.36917e-05        1            15.7984 |\n",
            "| train_tune_88374_00006   TERMINATED   5.76985e-05         0.00545915         2            23.3561 |\n",
            "| train_tune_88374_00007   TERMINATED   2.71157e-05         7.75433e-05        2            23.6123 |\n",
            "| train_tune_88374_00008   TERMINATED   0.00239529          0.00569982         1            13.1547 |\n",
            "| train_tune_88374_00009   TERMINATED   0.0018854           2.00235e-05        1            13.1171 |\n",
            "+---------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5bfd83e60043>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-5bfd83e60043>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best trial config: {best_trial.config}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best trial final validation loss: {best_trial.last_result['loss']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'config'"
          ]
        }
      ],
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "\n",
        "    config = {\n",
        "      \"thresholds_0\": tune.uniform(25,75),\n",
        "      \"thresholds_1\": tune.uniform(500,1500),\n",
        "      \"thresholds_2\": tune.uniform(500,1500),\n",
        "      \"thresholds_3\": tune.uniform(500,1500)\n",
        "    }\n",
        "\n",
        "    _, test_data = load_mnist(data_dir)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=64, shuffle=True, num_workers=2\n",
        "    )\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2,\n",
        "    )\n",
        "\n",
        "    result = tune.run(\n",
        "        partial(train_tune, max_num_epochs=max_num_epochs, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "    )\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(f\"Best trial config: {best_trial.config}\")\n",
        "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
        "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
        "\n",
        "    best_trained_model = net(784)\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if gpus_per_trial > 1:\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    best_trained_model.to(device)\n",
        "\n",
        "    best_checkpoint = best_trial.checkpoint.to_air_checkpoint()\n",
        "    best_checkpoint_data = best_checkpoint.to_dict()\n",
        "\n",
        "    best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n",
        "\n",
        "    test_acc = test(best_trained_model, test_loader, config[\"final_loss\"])\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(num_samples=10, max_num_epochs=20, gpus_per_trial=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi5CDidzl5jN"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw5MYk68mQ5j"
      },
      "source": [
        "### Baseline Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "mXS93nQJOmoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447bae9c-5164-4dd7-bb32-0090972d652f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "net(\n",
              "  (layer1): one_layer_net(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (layer): Linear(in_features=784, out_features=2000, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (layer2): one_layer_net(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (layer): Linear(in_features=2000, out_features=2000, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (layer3): one_layer_net(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (layer): Linear(in_features=2000, out_features=2000, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (layer4): one_layer_net(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (layer): Linear(in_features=2000, out_features=2000, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (ln): LayerNorm((2000,), eps=1e-05, elementwise_affine=True)\n",
              "  (out_layer): Linear(in_features=6000, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# create model\n",
        "torch.manual_seed(seed)\n",
        "model_sigmoid_threshold = net(784)\n",
        "model_sigmoid_threshold.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "wbVlBrAYtom7"
      },
      "outputs": [],
      "source": [
        "# setup for training using sigmoid of goodness minus threshold\n",
        "pos_optimizers, neg_optimizers = [],[]\n",
        "# pos_schedulers, neg_schedulers = [],[]\n",
        "final_optimizer = torch.optim.SGD(model_sigmoid_threshold.layers[-1].parameters(), lr=lrs[-1], momentum = momentums[-1], weight_decay=weight_decays[-1], maximize=False)\n",
        "# final_scheduler = torch.optim.lr_scheduler.CyclicLR(final_optimizer, base_lr=lrs[-1]/10, max_lr=lrs[-1])\n",
        "\n",
        "# initialize each layer\n",
        "for index in range(len(model_sigmoid_threshold.layers)):\n",
        "    # layer & training initialization\n",
        "    layer = model_sigmoid_threshold.layers[index]\n",
        "    layer.apply(init_weights)\n",
        "    # clip gradients to avoid exploding gradients\n",
        "    # torch.nn.utils.clip_grad_norm_(parameters=layer.parameters(), max_norm=1,norm_type=2.0)\n",
        "    # define optimizers\n",
        "    if index < len(model_sigmoid_threshold.layers)-1:\n",
        "      pos_optimizers.append(torch.optim.SGD(layer.parameters(), lr=lrs[index], momentum = momentums[index], weight_decay=weight_decays[index], maximize=True))\n",
        "      neg_optimizers.append(torch.optim.SGD(layer.parameters(), lr=lrs[index], momentum = momentums[index], weight_decay=weight_decays[index], maximize=False))\n",
        "      # pos_schedulers.append(torch.optim.lr_scheduler.CyclicLR(pos_optimizers[index], base_lr=lrs[index]/10, max_lr=lrs[index]))\n",
        "      # neg_schedulers.append(torch.optim.lr_scheduler.CyclicLR(neg_optimizers[index], base_lr=lrs[index]/10, max_lr=lrs[index]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-ZLMO_AQ2Cj5"
      },
      "outputs": [],
      "source": [
        "# train model using criterion used in paper (sigmoid of goodness minus threshold)\n",
        "def train_sigmoid_threshold(num_epochs, model, train_loader, test_loader, final_loss, pos_optimizers, neg_optimizers, final_optimizer, thresholds, records_per_epoch, supervised = True):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # variables for training output\n",
        "    total_pos_goodnesses,total_neg_goodnesses = [],[]\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "    total_step = len(train_loader)\n",
        "    record_period = total_step//records_per_epoch\n",
        "    pos_schedulers, neg_schedulers = [],[]\n",
        "    final_scheduler = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_pos_goodness,total_neg_goodness = [0]*(len(model.layers)-1),[0]*(len(model.layers)-1)\n",
        "        total_loss = 0\n",
        "\n",
        "        for i,(x,y) in enumerate(train_loader): # iterate through dataset\n",
        "            x,y = x.to(device),y.to(device)\n",
        "            if not supervised: # unsupervised training input\n",
        "              pos_imgs = x.squeeze()\n",
        "              neg_imgs = generate_negative_example().squeeze().to(device)\n",
        "            else: # supervised learning input\n",
        "              pos_imgs = model.construct_supervised_example(x,y,True)\n",
        "              neg_imgs = model.construct_supervised_example(x,y,False)\n",
        "\n",
        "            # intermediate variables\n",
        "            pos_interms = []\n",
        "            pos_interm = pos_imgs.clone()\n",
        "            neg_interm = neg_imgs.clone()\n",
        "\n",
        "            # iterate over intermediate layers\n",
        "            for index in range(len(model.layers)-1):\n",
        "              layer = model.layers[index]\n",
        "\n",
        "              # positive pass\n",
        "              pos_output = layer(pos_interm)\n",
        "              pos_goodness = model.criterion(pos_output, threshold=thresholds[index])\n",
        "              pos_interm = model.ln(pos_output).detach()\n",
        "              pos_interms.append(pos_interm.clone())\n",
        "\n",
        "              # update variables\n",
        "              total_pos_goodness[index] += pos_goodness\n",
        "\n",
        "              # clear gradients for this training step\n",
        "              pos_optimizers[index].zero_grad()\n",
        "\n",
        "              # take gradient step\n",
        "              pos_goodness.backward()\n",
        "              pos_optimizers[index].step()\n",
        "\n",
        "              # negative pass\n",
        "              neg_output = layer(neg_interm)\n",
        "              neg_goodness = model.criterion(neg_output, threshold=thresholds[index])\n",
        "              neg_interm = model.ln(neg_output).detach()\n",
        "\n",
        "              total_neg_goodness[index] += neg_goodness\n",
        "              neg_optimizers[index].zero_grad()\n",
        "              neg_goodness.backward()\n",
        "              neg_optimizers[index].step()\n",
        "\n",
        "              # update scheduler\n",
        "              # pos_schedulers[index].step()\n",
        "              # neg_schedulers[index].step()\n",
        "\n",
        "              # check progress\n",
        "              if (i+1) % (record_period) == 0:\n",
        "                  avg_pos_goodness = total_pos_goodness[index]/(record_period)\n",
        "                  avg_neg_goodness = total_neg_goodness[index]/(record_period)\n",
        "                  if len(total_pos_goodnesses)>index:\n",
        "                    total_pos_goodnesses[index].append(avg_pos_goodness)\n",
        "                  else:\n",
        "                    total_pos_goodnesses.append([avg_pos_goodness])\n",
        "                  if len(total_neg_goodnesses)>index:\n",
        "                    total_neg_goodnesses[index].append(avg_neg_goodness)\n",
        "                  else:\n",
        "                    total_neg_goodnesses.append([avg_neg_goodness])\n",
        "\n",
        "                  print ('Layer {}, Epoch [{}/{}], Step [{}/{}], Positive Goodness: {:.4f}, Negative Goodness: {:.4f}'\n",
        "                        .format(index, epoch + 1, num_epochs, i + 1, total_step, avg_pos_goodness, avg_neg_goodness))\n",
        "                  total_pos_goodness[index],total_neg_goodness[index] = 0,0\n",
        "\n",
        "            # output layer\n",
        "            final_input = torch.cat(pos_interms[1:],dim=1)\n",
        "            labels = model.layers[-1](final_input)\n",
        "            loss = ceLoss(labels,y)\n",
        "            total_loss += loss\n",
        "\n",
        "            final_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            final_optimizer.step()\n",
        "            # final_scheduler.step()\n",
        "\n",
        "            if (i+1) % record_period == 0:\n",
        "              avg_loss = total_loss/record_period\n",
        "              train_losses.append(avg_loss)\n",
        "\n",
        "              print ('Final Layer, Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                    .format(epoch + 1, num_epochs, i + 1, total_step, avg_loss))\n",
        "              total_loss = 0\n",
        "\n",
        "              # test model on test set\n",
        "              test_loss, test_acc = test(model, test_loader, final_loss)\n",
        "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Test Accuracy: {:.4f}'\n",
        "                    .format(epoch + 1, num_epochs, i + 1, total_step, test_loss, test_acc))\n",
        "              test_losses.append(test_loss)\n",
        "              test_accuracies.append(test_acc)\n",
        "\n",
        "        # initialize/update linear schedulers\n",
        "        if epoch > (num_epochs//2):\n",
        "          if not final_scheduler:\n",
        "            for index in range(len(model_sigmoid_threshold.layers)-1):\n",
        "              pos_schedulers.append(torch.optim.lr_scheduler.LinearLR(pos_optimizers[index], start_factor=1-1/num_epochs, end_factor=1/num_epochs, total_iters=num_epochs-epoch-1))\n",
        "              neg_schedulers.append(torch.optim.lr_scheduler.LinearLR(neg_optimizers[index], start_factor=1-1/num_epochs, end_factor=1/num_epochs, total_iters=num_epochs-epoch-1))\n",
        "            final_scheduler = torch.optim.lr_scheduler.LinearLR(final_optimizer, start_factor=1-1/num_epochs, end_factor=1/num_epochs, total_iters=num_epochs-epoch-1)\n",
        "          else:\n",
        "            for index in range(len(pos_schedulers)):\n",
        "              pos_schedulers[index].step()\n",
        "              neg_schedulers[index].step()\n",
        "            final_scheduler.step()\n",
        "\n",
        "        # save model every 5 epochs\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            torch.save(model.state_dict(), 'model_sigmoid_threshold_v1.1_epoch{}.pth'.format(epoch+1))\n",
        "        pass\n",
        "    return total_pos_goodnesses, total_neg_goodnesses, train_losses, test_losses, test_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "X2baZqMWtom8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df2a9e1-45ff-42ee-bc01-e367ee553437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0, Epoch [1/100], Step [117/469], Positive Goodness: 0.9037, Negative Goodness: 0.9078\n",
            "Layer 1, Epoch [1/100], Step [117/469], Positive Goodness: 0.9548, Negative Goodness: 0.9708\n",
            "Layer 2, Epoch [1/100], Step [117/469], Positive Goodness: 0.9429, Negative Goodness: 0.9493\n",
            "Layer 3, Epoch [1/100], Step [117/469], Positive Goodness: 0.6723, Negative Goodness: 0.7132\n",
            "Final Layer, Epoch [1/100], Step [117/469], Loss: 0.8166\n",
            "Epoch [1/100], Step [117/469], Loss: 0.5356, Test Accuracy: 0.1463\n",
            "Layer 0, Epoch [1/100], Step [234/469], Positive Goodness: 0.9255, Negative Goodness: 0.9049\n",
            "Layer 1, Epoch [1/100], Step [234/469], Positive Goodness: 0.9987, Negative Goodness: 0.9974\n",
            "Layer 2, Epoch [1/100], Step [234/469], Positive Goodness: 0.9633, Negative Goodness: 0.9349\n",
            "Layer 3, Epoch [1/100], Step [234/469], Positive Goodness: 0.7968, Negative Goodness: 0.8144\n",
            "Final Layer, Epoch [1/100], Step [234/469], Loss: 0.2767\n",
            "Epoch [1/100], Step [234/469], Loss: 0.4466, Test Accuracy: 0.1669\n",
            "Layer 0, Epoch [1/100], Step [351/469], Positive Goodness: 0.9261, Negative Goodness: 0.8879\n",
            "Layer 1, Epoch [1/100], Step [351/469], Positive Goodness: 0.9974, Negative Goodness: 0.9951\n",
            "Layer 2, Epoch [1/100], Step [351/469], Positive Goodness: 0.9684, Negative Goodness: 0.9126\n",
            "Layer 3, Epoch [1/100], Step [351/469], Positive Goodness: 0.9226, Negative Goodness: 0.9295\n",
            "Final Layer, Epoch [1/100], Step [351/469], Loss: 0.2060\n",
            "Epoch [1/100], Step [351/469], Loss: 0.4152, Test Accuracy: 0.1500\n",
            "Layer 0, Epoch [1/100], Step [468/469], Positive Goodness: 0.9174, Negative Goodness: 0.8492\n",
            "Layer 1, Epoch [1/100], Step [468/469], Positive Goodness: 0.9994, Negative Goodness: 0.9975\n",
            "Layer 2, Epoch [1/100], Step [468/469], Positive Goodness: 0.9200, Negative Goodness: 0.8715\n",
            "Layer 3, Epoch [1/100], Step [468/469], Positive Goodness: 0.9027, Negative Goodness: 0.9071\n",
            "Final Layer, Epoch [1/100], Step [468/469], Loss: 0.1713\n",
            "Epoch [1/100], Step [468/469], Loss: 0.3868, Test Accuracy: 0.1732\n",
            "Layer 0, Epoch [2/100], Step [117/469], Positive Goodness: 0.9245, Negative Goodness: 0.8345\n",
            "Layer 1, Epoch [2/100], Step [117/469], Positive Goodness: 0.9961, Negative Goodness: 0.9912\n",
            "Layer 2, Epoch [2/100], Step [117/469], Positive Goodness: 0.9071, Negative Goodness: 0.8337\n",
            "Layer 3, Epoch [2/100], Step [117/469], Positive Goodness: 0.8696, Negative Goodness: 0.8678\n",
            "Final Layer, Epoch [2/100], Step [117/469], Loss: 0.1474\n",
            "Epoch [2/100], Step [117/469], Loss: 0.3737, Test Accuracy: 0.1671\n",
            "Layer 0, Epoch [2/100], Step [234/469], Positive Goodness: 0.8783, Negative Goodness: 0.7812\n",
            "Layer 1, Epoch [2/100], Step [234/469], Positive Goodness: 0.9978, Negative Goodness: 0.9945\n",
            "Layer 2, Epoch [2/100], Step [234/469], Positive Goodness: 0.9094, Negative Goodness: 0.8244\n",
            "Layer 3, Epoch [2/100], Step [234/469], Positive Goodness: 0.8946, Negative Goodness: 0.8812\n",
            "Final Layer, Epoch [2/100], Step [234/469], Loss: 0.1258\n",
            "Epoch [2/100], Step [234/469], Loss: 0.3674, Test Accuracy: 0.1481\n",
            "Layer 0, Epoch [2/100], Step [351/469], Positive Goodness: 0.9009, Negative Goodness: 0.7916\n",
            "Layer 1, Epoch [2/100], Step [351/469], Positive Goodness: 0.9909, Negative Goodness: 0.9836\n",
            "Layer 2, Epoch [2/100], Step [351/469], Positive Goodness: 0.9612, Negative Goodness: 0.8617\n",
            "Layer 3, Epoch [2/100], Step [351/469], Positive Goodness: 0.9431, Negative Goodness: 0.9188\n",
            "Final Layer, Epoch [2/100], Step [351/469], Loss: 0.1151\n",
            "Epoch [2/100], Step [351/469], Loss: 0.3630, Test Accuracy: 0.1586\n",
            "Layer 0, Epoch [2/100], Step [468/469], Positive Goodness: 0.8882, Negative Goodness: 0.7603\n",
            "Layer 1, Epoch [2/100], Step [468/469], Positive Goodness: 0.9879, Negative Goodness: 0.9733\n",
            "Layer 2, Epoch [2/100], Step [468/469], Positive Goodness: 0.9029, Negative Goodness: 0.8030\n",
            "Layer 3, Epoch [2/100], Step [468/469], Positive Goodness: 0.9391, Negative Goodness: 0.9024\n",
            "Final Layer, Epoch [2/100], Step [468/469], Loss: 0.1105\n",
            "Epoch [2/100], Step [468/469], Loss: 0.3696, Test Accuracy: 0.1418\n",
            "Layer 0, Epoch [3/100], Step [117/469], Positive Goodness: 0.8892, Negative Goodness: 0.7402\n",
            "Layer 1, Epoch [3/100], Step [117/469], Positive Goodness: 0.9936, Negative Goodness: 0.9650\n",
            "Layer 2, Epoch [3/100], Step [117/469], Positive Goodness: 0.9380, Negative Goodness: 0.8378\n",
            "Layer 3, Epoch [3/100], Step [117/469], Positive Goodness: 0.8887, Negative Goodness: 0.8416\n",
            "Final Layer, Epoch [3/100], Step [117/469], Loss: 0.0989\n",
            "Epoch [3/100], Step [117/469], Loss: 0.3658, Test Accuracy: 0.1664\n",
            "Layer 0, Epoch [3/100], Step [234/469], Positive Goodness: 0.8986, Negative Goodness: 0.7213\n",
            "Layer 1, Epoch [3/100], Step [234/469], Positive Goodness: 0.9902, Negative Goodness: 0.9197\n",
            "Layer 2, Epoch [3/100], Step [234/469], Positive Goodness: 0.9648, Negative Goodness: 0.8571\n",
            "Layer 3, Epoch [3/100], Step [234/469], Positive Goodness: 0.8912, Negative Goodness: 0.8282\n",
            "Final Layer, Epoch [3/100], Step [234/469], Loss: 0.0939\n",
            "Epoch [3/100], Step [234/469], Loss: 0.3698, Test Accuracy: 0.2101\n",
            "Layer 0, Epoch [3/100], Step [351/469], Positive Goodness: 0.8827, Negative Goodness: 0.6888\n",
            "Layer 1, Epoch [3/100], Step [351/469], Positive Goodness: 0.9754, Negative Goodness: 0.8774\n",
            "Layer 2, Epoch [3/100], Step [351/469], Positive Goodness: 0.9092, Negative Goodness: 0.7897\n",
            "Layer 3, Epoch [3/100], Step [351/469], Positive Goodness: 0.8081, Negative Goodness: 0.7282\n",
            "Final Layer, Epoch [3/100], Step [351/469], Loss: 0.0903\n",
            "Epoch [3/100], Step [351/469], Loss: 0.3711, Test Accuracy: 0.2288\n",
            "Layer 0, Epoch [3/100], Step [468/469], Positive Goodness: 0.8940, Negative Goodness: 0.6851\n",
            "Layer 1, Epoch [3/100], Step [468/469], Positive Goodness: 0.9390, Negative Goodness: 0.8385\n",
            "Layer 2, Epoch [3/100], Step [468/469], Positive Goodness: 0.8671, Negative Goodness: 0.7463\n",
            "Layer 3, Epoch [3/100], Step [468/469], Positive Goodness: 0.8167, Negative Goodness: 0.7226\n",
            "Final Layer, Epoch [3/100], Step [468/469], Loss: 0.0893\n",
            "Epoch [3/100], Step [468/469], Loss: 0.3652, Test Accuracy: 0.2427\n",
            "Layer 0, Epoch [4/100], Step [117/469], Positive Goodness: 0.8878, Negative Goodness: 0.6519\n",
            "Layer 1, Epoch [4/100], Step [117/469], Positive Goodness: 0.9544, Negative Goodness: 0.8272\n",
            "Layer 2, Epoch [4/100], Step [117/469], Positive Goodness: 0.8540, Negative Goodness: 0.7158\n",
            "Layer 3, Epoch [4/100], Step [117/469], Positive Goodness: 0.7701, Negative Goodness: 0.6452\n",
            "Final Layer, Epoch [4/100], Step [117/469], Loss: 0.0762\n",
            "Epoch [4/100], Step [117/469], Loss: 0.3778, Test Accuracy: 0.2727\n",
            "Layer 0, Epoch [4/100], Step [234/469], Positive Goodness: 0.9062, Negative Goodness: 0.6424\n",
            "Layer 1, Epoch [4/100], Step [234/469], Positive Goodness: 0.9468, Negative Goodness: 0.8008\n",
            "Layer 2, Epoch [4/100], Step [234/469], Positive Goodness: 0.8156, Negative Goodness: 0.6727\n",
            "Layer 3, Epoch [4/100], Step [234/469], Positive Goodness: 0.7540, Negative Goodness: 0.6158\n",
            "Final Layer, Epoch [4/100], Step [234/469], Loss: 0.0771\n",
            "Epoch [4/100], Step [234/469], Loss: 0.3810, Test Accuracy: 0.2897\n",
            "Layer 0, Epoch [4/100], Step [351/469], Positive Goodness: 0.9105, Negative Goodness: 0.6332\n",
            "Layer 1, Epoch [4/100], Step [351/469], Positive Goodness: 0.9244, Negative Goodness: 0.7663\n",
            "Layer 2, Epoch [4/100], Step [351/469], Positive Goodness: 0.8665, Negative Goodness: 0.7149\n",
            "Layer 3, Epoch [4/100], Step [351/469], Positive Goodness: 0.7888, Negative Goodness: 0.6457\n",
            "Final Layer, Epoch [4/100], Step [351/469], Loss: 0.0711\n",
            "Epoch [4/100], Step [351/469], Loss: 0.3796, Test Accuracy: 0.3559\n",
            "Layer 0, Epoch [4/100], Step [468/469], Positive Goodness: 0.9158, Negative Goodness: 0.6127\n",
            "Layer 1, Epoch [4/100], Step [468/469], Positive Goodness: 0.9362, Negative Goodness: 0.7525\n",
            "Layer 2, Epoch [4/100], Step [468/469], Positive Goodness: 0.8371, Negative Goodness: 0.6616\n",
            "Layer 3, Epoch [4/100], Step [468/469], Positive Goodness: 0.7255, Negative Goodness: 0.5627\n",
            "Final Layer, Epoch [4/100], Step [468/469], Loss: 0.0693\n",
            "Epoch [4/100], Step [468/469], Loss: 0.3847, Test Accuracy: 0.3726\n",
            "Layer 0, Epoch [5/100], Step [117/469], Positive Goodness: 0.9123, Negative Goodness: 0.5925\n",
            "Layer 1, Epoch [5/100], Step [117/469], Positive Goodness: 0.8943, Negative Goodness: 0.6968\n",
            "Layer 2, Epoch [5/100], Step [117/469], Positive Goodness: 0.8030, Negative Goodness: 0.6021\n",
            "Layer 3, Epoch [5/100], Step [117/469], Positive Goodness: 0.6754, Negative Goodness: 0.4914\n",
            "Final Layer, Epoch [5/100], Step [117/469], Loss: 0.0614\n",
            "Epoch [5/100], Step [117/469], Loss: 0.3919, Test Accuracy: 0.4061\n",
            "Layer 0, Epoch [5/100], Step [234/469], Positive Goodness: 0.9151, Negative Goodness: 0.5810\n",
            "Layer 1, Epoch [5/100], Step [234/469], Positive Goodness: 0.8807, Negative Goodness: 0.6690\n",
            "Layer 2, Epoch [5/100], Step [234/469], Positive Goodness: 0.7566, Negative Goodness: 0.5412\n",
            "Layer 3, Epoch [5/100], Step [234/469], Positive Goodness: 0.6697, Negative Goodness: 0.4730\n",
            "Final Layer, Epoch [5/100], Step [234/469], Loss: 0.0573\n",
            "Epoch [5/100], Step [234/469], Loss: 0.3980, Test Accuracy: 0.4279\n",
            "Layer 0, Epoch [5/100], Step [351/469], Positive Goodness: 0.9153, Negative Goodness: 0.5811\n",
            "Layer 1, Epoch [5/100], Step [351/469], Positive Goodness: 0.8548, Negative Goodness: 0.6193\n",
            "Layer 2, Epoch [5/100], Step [351/469], Positive Goodness: 0.7604, Negative Goodness: 0.5244\n",
            "Layer 3, Epoch [5/100], Step [351/469], Positive Goodness: 0.6607, Negative Goodness: 0.4468\n",
            "Final Layer, Epoch [5/100], Step [351/469], Loss: 0.0606\n",
            "Epoch [5/100], Step [351/469], Loss: 0.4047, Test Accuracy: 0.4378\n",
            "Layer 0, Epoch [5/100], Step [468/469], Positive Goodness: 0.9224, Negative Goodness: 0.5741\n",
            "Layer 1, Epoch [5/100], Step [468/469], Positive Goodness: 0.9061, Negative Goodness: 0.6380\n",
            "Layer 2, Epoch [5/100], Step [468/469], Positive Goodness: 0.8304, Negative Goodness: 0.5632\n",
            "Layer 3, Epoch [5/100], Step [468/469], Positive Goodness: 0.6331, Negative Goodness: 0.4039\n",
            "Final Layer, Epoch [5/100], Step [468/469], Loss: 0.0536\n",
            "Epoch [5/100], Step [468/469], Loss: 0.4107, Test Accuracy: 0.4551\n",
            "Layer 0, Epoch [6/100], Step [117/469], Positive Goodness: 0.9252, Negative Goodness: 0.5507\n",
            "Layer 1, Epoch [6/100], Step [117/469], Positive Goodness: 0.8740, Negative Goodness: 0.5829\n",
            "Layer 2, Epoch [6/100], Step [117/469], Positive Goodness: 0.7659, Negative Goodness: 0.4648\n",
            "Layer 3, Epoch [6/100], Step [117/469], Positive Goodness: 0.6101, Negative Goodness: 0.3618\n",
            "Final Layer, Epoch [6/100], Step [117/469], Loss: 0.0473\n",
            "Epoch [6/100], Step [117/469], Loss: 0.4201, Test Accuracy: 0.4617\n",
            "Layer 0, Epoch [6/100], Step [234/469], Positive Goodness: 0.9186, Negative Goodness: 0.5498\n",
            "Layer 1, Epoch [6/100], Step [234/469], Positive Goodness: 0.8566, Negative Goodness: 0.5635\n",
            "Layer 2, Epoch [6/100], Step [234/469], Positive Goodness: 0.7509, Negative Goodness: 0.4246\n",
            "Layer 3, Epoch [6/100], Step [234/469], Positive Goodness: 0.6204, Negative Goodness: 0.3627\n",
            "Final Layer, Epoch [6/100], Step [234/469], Loss: 0.0501\n",
            "Epoch [6/100], Step [234/469], Loss: 0.3826, Test Accuracy: 0.4795\n",
            "Layer 0, Epoch [6/100], Step [351/469], Positive Goodness: 0.9222, Negative Goodness: 0.5603\n",
            "Layer 1, Epoch [6/100], Step [351/469], Positive Goodness: 0.8606, Negative Goodness: 0.5622\n",
            "Layer 2, Epoch [6/100], Step [351/469], Positive Goodness: 0.6938, Negative Goodness: 0.3575\n",
            "Layer 3, Epoch [6/100], Step [351/469], Positive Goodness: 0.6056, Negative Goodness: 0.3249\n",
            "Final Layer, Epoch [6/100], Step [351/469], Loss: 0.0497\n",
            "Epoch [6/100], Step [351/469], Loss: 0.3873, Test Accuracy: 0.5211\n",
            "Layer 0, Epoch [6/100], Step [468/469], Positive Goodness: 0.9229, Negative Goodness: 0.5409\n",
            "Layer 1, Epoch [6/100], Step [468/469], Positive Goodness: 0.8800, Negative Goodness: 0.5617\n",
            "Layer 2, Epoch [6/100], Step [468/469], Positive Goodness: 0.7340, Negative Goodness: 0.3731\n",
            "Layer 3, Epoch [6/100], Step [468/469], Positive Goodness: 0.5974, Negative Goodness: 0.2909\n",
            "Final Layer, Epoch [6/100], Step [468/469], Loss: 0.0455\n",
            "Epoch [6/100], Step [468/469], Loss: 0.4086, Test Accuracy: 0.5445\n",
            "Layer 0, Epoch [7/100], Step [117/469], Positive Goodness: 0.9239, Negative Goodness: 0.5288\n",
            "Layer 1, Epoch [7/100], Step [117/469], Positive Goodness: 0.8742, Negative Goodness: 0.5433\n",
            "Layer 2, Epoch [7/100], Step [117/469], Positive Goodness: 0.7641, Negative Goodness: 0.3721\n",
            "Layer 3, Epoch [7/100], Step [117/469], Positive Goodness: 0.6293, Negative Goodness: 0.3014\n",
            "Final Layer, Epoch [7/100], Step [117/469], Loss: 0.0399\n",
            "Epoch [7/100], Step [117/469], Loss: 0.4053, Test Accuracy: 0.5741\n",
            "Layer 0, Epoch [7/100], Step [234/469], Positive Goodness: 0.9272, Negative Goodness: 0.5145\n",
            "Layer 1, Epoch [7/100], Step [234/469], Positive Goodness: 0.8479, Negative Goodness: 0.4981\n",
            "Layer 2, Epoch [7/100], Step [234/469], Positive Goodness: 0.7498, Negative Goodness: 0.3355\n",
            "Layer 3, Epoch [7/100], Step [234/469], Positive Goodness: 0.6284, Negative Goodness: 0.2852\n",
            "Final Layer, Epoch [7/100], Step [234/469], Loss: 0.0392\n",
            "Epoch [7/100], Step [234/469], Loss: 0.3858, Test Accuracy: 0.5777\n",
            "Layer 0, Epoch [7/100], Step [351/469], Positive Goodness: 0.9280, Negative Goodness: 0.5166\n",
            "Layer 1, Epoch [7/100], Step [351/469], Positive Goodness: 0.8535, Negative Goodness: 0.4952\n",
            "Layer 2, Epoch [7/100], Step [351/469], Positive Goodness: 0.7453, Negative Goodness: 0.3162\n",
            "Layer 3, Epoch [7/100], Step [351/469], Positive Goodness: 0.6239, Negative Goodness: 0.2697\n",
            "Final Layer, Epoch [7/100], Step [351/469], Loss: 0.0343\n",
            "Epoch [7/100], Step [351/469], Loss: 0.4262, Test Accuracy: 0.6083\n",
            "Layer 0, Epoch [7/100], Step [468/469], Positive Goodness: 0.9326, Negative Goodness: 0.5122\n",
            "Layer 1, Epoch [7/100], Step [468/469], Positive Goodness: 0.8453, Negative Goodness: 0.4861\n",
            "Layer 2, Epoch [7/100], Step [468/469], Positive Goodness: 0.7063, Negative Goodness: 0.2750\n",
            "Layer 3, Epoch [7/100], Step [468/469], Positive Goodness: 0.6240, Negative Goodness: 0.2588\n",
            "Final Layer, Epoch [7/100], Step [468/469], Loss: 0.0401\n",
            "Epoch [7/100], Step [468/469], Loss: 0.3984, Test Accuracy: 0.6137\n",
            "Layer 0, Epoch [8/100], Step [117/469], Positive Goodness: 0.9365, Negative Goodness: 0.4959\n",
            "Layer 1, Epoch [8/100], Step [117/469], Positive Goodness: 0.8492, Negative Goodness: 0.4621\n",
            "Layer 2, Epoch [8/100], Step [117/469], Positive Goodness: 0.7700, Negative Goodness: 0.3201\n",
            "Layer 3, Epoch [8/100], Step [117/469], Positive Goodness: 0.6384, Negative Goodness: 0.2400\n",
            "Final Layer, Epoch [8/100], Step [117/469], Loss: 0.0330\n",
            "Epoch [8/100], Step [117/469], Loss: 0.3939, Test Accuracy: 0.6658\n",
            "Layer 0, Epoch [8/100], Step [234/469], Positive Goodness: 0.9356, Negative Goodness: 0.4951\n",
            "Layer 1, Epoch [8/100], Step [234/469], Positive Goodness: 0.8185, Negative Goodness: 0.4257\n",
            "Layer 2, Epoch [8/100], Step [234/469], Positive Goodness: 0.7286, Negative Goodness: 0.2450\n",
            "Layer 3, Epoch [8/100], Step [234/469], Positive Goodness: 0.6759, Negative Goodness: 0.2727\n",
            "Final Layer, Epoch [8/100], Step [234/469], Loss: 0.0330\n",
            "Epoch [8/100], Step [234/469], Loss: 0.4155, Test Accuracy: 0.6954\n",
            "Layer 0, Epoch [8/100], Step [351/469], Positive Goodness: 0.9383, Negative Goodness: 0.4930\n",
            "Layer 1, Epoch [8/100], Step [351/469], Positive Goodness: 0.7662, Negative Goodness: 0.3732\n",
            "Layer 2, Epoch [8/100], Step [351/469], Positive Goodness: 0.7372, Negative Goodness: 0.2359\n",
            "Layer 3, Epoch [8/100], Step [351/469], Positive Goodness: 0.6789, Negative Goodness: 0.2738\n",
            "Final Layer, Epoch [8/100], Step [351/469], Loss: 0.0308\n",
            "Epoch [8/100], Step [351/469], Loss: 0.4015, Test Accuracy: 0.7148\n",
            "Layer 0, Epoch [8/100], Step [468/469], Positive Goodness: 0.9386, Negative Goodness: 0.4760\n",
            "Layer 1, Epoch [8/100], Step [468/469], Positive Goodness: 0.7915, Negative Goodness: 0.3687\n",
            "Layer 2, Epoch [8/100], Step [468/469], Positive Goodness: 0.7117, Negative Goodness: 0.1925\n",
            "Layer 3, Epoch [8/100], Step [468/469], Positive Goodness: 0.6692, Negative Goodness: 0.2434\n",
            "Final Layer, Epoch [8/100], Step [468/469], Loss: 0.0307\n",
            "Epoch [8/100], Step [468/469], Loss: 0.3944, Test Accuracy: 0.7261\n",
            "Layer 0, Epoch [9/100], Step [117/469], Positive Goodness: 0.9437, Negative Goodness: 0.4923\n",
            "Layer 1, Epoch [9/100], Step [117/469], Positive Goodness: 0.8075, Negative Goodness: 0.3881\n",
            "Layer 2, Epoch [9/100], Step [117/469], Positive Goodness: 0.7333, Negative Goodness: 0.2175\n",
            "Layer 3, Epoch [9/100], Step [117/469], Positive Goodness: 0.6538, Negative Goodness: 0.2243\n",
            "Final Layer, Epoch [9/100], Step [117/469], Loss: 0.0294\n",
            "Epoch [9/100], Step [117/469], Loss: 0.3846, Test Accuracy: 0.7363\n",
            "Layer 0, Epoch [9/100], Step [234/469], Positive Goodness: 0.9437, Negative Goodness: 0.4751\n",
            "Layer 1, Epoch [9/100], Step [234/469], Positive Goodness: 0.7829, Negative Goodness: 0.3320\n",
            "Layer 2, Epoch [9/100], Step [234/469], Positive Goodness: 0.7151, Negative Goodness: 0.1695\n",
            "Layer 3, Epoch [9/100], Step [234/469], Positive Goodness: 0.6765, Negative Goodness: 0.2156\n",
            "Final Layer, Epoch [9/100], Step [234/469], Loss: 0.0258\n",
            "Epoch [9/100], Step [234/469], Loss: 0.3763, Test Accuracy: 0.7647\n",
            "Layer 0, Epoch [9/100], Step [351/469], Positive Goodness: 0.9446, Negative Goodness: 0.4800\n",
            "Layer 1, Epoch [9/100], Step [351/469], Positive Goodness: 0.7922, Negative Goodness: 0.3426\n",
            "Layer 2, Epoch [9/100], Step [351/469], Positive Goodness: 0.7437, Negative Goodness: 0.1787\n",
            "Layer 3, Epoch [9/100], Step [351/469], Positive Goodness: 0.6992, Negative Goodness: 0.2235\n",
            "Final Layer, Epoch [9/100], Step [351/469], Loss: 0.0276\n",
            "Epoch [9/100], Step [351/469], Loss: 0.3618, Test Accuracy: 0.7767\n",
            "Layer 0, Epoch [9/100], Step [468/469], Positive Goodness: 0.9428, Negative Goodness: 0.4839\n",
            "Layer 1, Epoch [9/100], Step [468/469], Positive Goodness: 0.7970, Negative Goodness: 0.3459\n",
            "Layer 2, Epoch [9/100], Step [468/469], Positive Goodness: 0.7203, Negative Goodness: 0.1489\n",
            "Layer 3, Epoch [9/100], Step [468/469], Positive Goodness: 0.7224, Negative Goodness: 0.2536\n",
            "Final Layer, Epoch [9/100], Step [468/469], Loss: 0.0263\n",
            "Epoch [9/100], Step [468/469], Loss: 0.3697, Test Accuracy: 0.7820\n",
            "Layer 0, Epoch [10/100], Step [117/469], Positive Goodness: 0.9461, Negative Goodness: 0.4669\n",
            "Layer 1, Epoch [10/100], Step [117/469], Positive Goodness: 0.7934, Negative Goodness: 0.3116\n",
            "Layer 2, Epoch [10/100], Step [117/469], Positive Goodness: 0.7524, Negative Goodness: 0.1669\n",
            "Layer 3, Epoch [10/100], Step [117/469], Positive Goodness: 0.7065, Negative Goodness: 0.2259\n",
            "Final Layer, Epoch [10/100], Step [117/469], Loss: 0.0263\n",
            "Epoch [10/100], Step [117/469], Loss: 0.4065, Test Accuracy: 0.8039\n",
            "Layer 0, Epoch [10/100], Step [234/469], Positive Goodness: 0.9470, Negative Goodness: 0.4621\n",
            "Layer 1, Epoch [10/100], Step [234/469], Positive Goodness: 0.7838, Negative Goodness: 0.2919\n",
            "Layer 2, Epoch [10/100], Step [234/469], Positive Goodness: 0.7464, Negative Goodness: 0.1481\n",
            "Layer 3, Epoch [10/100], Step [234/469], Positive Goodness: 0.7039, Negative Goodness: 0.2133\n",
            "Final Layer, Epoch [10/100], Step [234/469], Loss: 0.0246\n",
            "Epoch [10/100], Step [234/469], Loss: 0.3884, Test Accuracy: 0.8051\n",
            "Layer 0, Epoch [10/100], Step [351/469], Positive Goodness: 0.9483, Negative Goodness: 0.4638\n",
            "Layer 1, Epoch [10/100], Step [351/469], Positive Goodness: 0.7897, Negative Goodness: 0.2740\n",
            "Layer 2, Epoch [10/100], Step [351/469], Positive Goodness: 0.7351, Negative Goodness: 0.1363\n",
            "Layer 3, Epoch [10/100], Step [351/469], Positive Goodness: 0.7129, Negative Goodness: 0.2091\n",
            "Final Layer, Epoch [10/100], Step [351/469], Loss: 0.0231\n",
            "Epoch [10/100], Step [351/469], Loss: 0.3610, Test Accuracy: 0.8129\n",
            "Layer 0, Epoch [10/100], Step [468/469], Positive Goodness: 0.9432, Negative Goodness: 0.4630\n",
            "Layer 1, Epoch [10/100], Step [468/469], Positive Goodness: 0.7911, Negative Goodness: 0.2839\n",
            "Layer 2, Epoch [10/100], Step [468/469], Positive Goodness: 0.7454, Negative Goodness: 0.1393\n",
            "Layer 3, Epoch [10/100], Step [468/469], Positive Goodness: 0.7148, Negative Goodness: 0.2251\n",
            "Final Layer, Epoch [10/100], Step [468/469], Loss: 0.0216\n",
            "Epoch [10/100], Step [468/469], Loss: 0.3422, Test Accuracy: 0.8267\n",
            "Layer 0, Epoch [11/100], Step [117/469], Positive Goodness: 0.9468, Negative Goodness: 0.4471\n",
            "Layer 1, Epoch [11/100], Step [117/469], Positive Goodness: 0.7661, Negative Goodness: 0.2290\n",
            "Layer 2, Epoch [11/100], Step [117/469], Positive Goodness: 0.7550, Negative Goodness: 0.1330\n",
            "Layer 3, Epoch [11/100], Step [117/469], Positive Goodness: 0.7295, Negative Goodness: 0.2224\n",
            "Final Layer, Epoch [11/100], Step [117/469], Loss: 0.0206\n",
            "Epoch [11/100], Step [117/469], Loss: 0.3667, Test Accuracy: 0.8377\n",
            "Layer 0, Epoch [11/100], Step [234/469], Positive Goodness: 0.9463, Negative Goodness: 0.4507\n",
            "Layer 1, Epoch [11/100], Step [234/469], Positive Goodness: 0.8077, Negative Goodness: 0.2699\n",
            "Layer 2, Epoch [11/100], Step [234/469], Positive Goodness: 0.7589, Negative Goodness: 0.1434\n",
            "Layer 3, Epoch [11/100], Step [234/469], Positive Goodness: 0.7327, Negative Goodness: 0.2265\n",
            "Final Layer, Epoch [11/100], Step [234/469], Loss: 0.0198\n",
            "Epoch [11/100], Step [234/469], Loss: 0.3539, Test Accuracy: 0.8334\n",
            "Layer 0, Epoch [11/100], Step [351/469], Positive Goodness: 0.9490, Negative Goodness: 0.4420\n",
            "Layer 1, Epoch [11/100], Step [351/469], Positive Goodness: 0.7905, Negative Goodness: 0.2418\n",
            "Layer 2, Epoch [11/100], Step [351/469], Positive Goodness: 0.7647, Negative Goodness: 0.1256\n",
            "Layer 3, Epoch [11/100], Step [351/469], Positive Goodness: 0.7270, Negative Goodness: 0.2098\n",
            "Final Layer, Epoch [11/100], Step [351/469], Loss: 0.0204\n",
            "Epoch [11/100], Step [351/469], Loss: 0.3646, Test Accuracy: 0.8282\n",
            "Layer 0, Epoch [11/100], Step [468/469], Positive Goodness: 0.9487, Negative Goodness: 0.4332\n",
            "Layer 1, Epoch [11/100], Step [468/469], Positive Goodness: 0.7747, Negative Goodness: 0.2133\n",
            "Layer 2, Epoch [11/100], Step [468/469], Positive Goodness: 0.7598, Negative Goodness: 0.1059\n",
            "Layer 3, Epoch [11/100], Step [468/469], Positive Goodness: 0.7385, Negative Goodness: 0.2087\n",
            "Final Layer, Epoch [11/100], Step [468/469], Loss: 0.0217\n",
            "Epoch [11/100], Step [468/469], Loss: 0.3577, Test Accuracy: 0.8403\n",
            "Layer 0, Epoch [12/100], Step [117/469], Positive Goodness: 0.9523, Negative Goodness: 0.4293\n",
            "Layer 1, Epoch [12/100], Step [117/469], Positive Goodness: 0.7725, Negative Goodness: 0.1943\n",
            "Layer 2, Epoch [12/100], Step [117/469], Positive Goodness: 0.7820, Negative Goodness: 0.1333\n",
            "Layer 3, Epoch [12/100], Step [117/469], Positive Goodness: 0.7341, Negative Goodness: 0.1903\n",
            "Final Layer, Epoch [12/100], Step [117/469], Loss: 0.0181\n",
            "Epoch [12/100], Step [117/469], Loss: 0.3432, Test Accuracy: 0.8349\n",
            "Layer 0, Epoch [12/100], Step [234/469], Positive Goodness: 0.9509, Negative Goodness: 0.4364\n",
            "Layer 1, Epoch [12/100], Step [234/469], Positive Goodness: 0.7666, Negative Goodness: 0.1883\n",
            "Layer 2, Epoch [12/100], Step [234/469], Positive Goodness: 0.7804, Negative Goodness: 0.1168\n",
            "Layer 3, Epoch [12/100], Step [234/469], Positive Goodness: 0.7299, Negative Goodness: 0.1830\n",
            "Final Layer, Epoch [12/100], Step [234/469], Loss: 0.0179\n",
            "Epoch [12/100], Step [234/469], Loss: 0.3618, Test Accuracy: 0.8473\n",
            "Layer 0, Epoch [12/100], Step [351/469], Positive Goodness: 0.9456, Negative Goodness: 0.4439\n",
            "Layer 1, Epoch [12/100], Step [351/469], Positive Goodness: 0.7891, Negative Goodness: 0.2015\n",
            "Layer 2, Epoch [12/100], Step [351/469], Positive Goodness: 0.7887, Negative Goodness: 0.1259\n",
            "Layer 3, Epoch [12/100], Step [351/469], Positive Goodness: 0.7227, Negative Goodness: 0.1756\n",
            "Final Layer, Epoch [12/100], Step [351/469], Loss: 0.0203\n",
            "Epoch [12/100], Step [351/469], Loss: 0.3309, Test Accuracy: 0.8500\n",
            "Layer 0, Epoch [12/100], Step [468/469], Positive Goodness: 0.9455, Negative Goodness: 0.4423\n",
            "Layer 1, Epoch [12/100], Step [468/469], Positive Goodness: 0.7996, Negative Goodness: 0.1949\n",
            "Layer 2, Epoch [12/100], Step [468/469], Positive Goodness: 0.7916, Negative Goodness: 0.1187\n",
            "Layer 3, Epoch [12/100], Step [468/469], Positive Goodness: 0.7363, Negative Goodness: 0.1691\n",
            "Final Layer, Epoch [12/100], Step [468/469], Loss: 0.0177\n",
            "Epoch [12/100], Step [468/469], Loss: 0.3264, Test Accuracy: 0.8590\n",
            "Layer 0, Epoch [13/100], Step [117/469], Positive Goodness: 0.9474, Negative Goodness: 0.4240\n",
            "Layer 1, Epoch [13/100], Step [117/469], Positive Goodness: 0.8049, Negative Goodness: 0.1992\n",
            "Layer 2, Epoch [13/100], Step [117/469], Positive Goodness: 0.7969, Negative Goodness: 0.1197\n",
            "Layer 3, Epoch [13/100], Step [117/469], Positive Goodness: 0.7722, Negative Goodness: 0.2081\n",
            "Final Layer, Epoch [13/100], Step [117/469], Loss: 0.0151\n",
            "Epoch [13/100], Step [117/469], Loss: 0.3399, Test Accuracy: 0.8562\n",
            "Layer 0, Epoch [13/100], Step [234/469], Positive Goodness: 0.9479, Negative Goodness: 0.4076\n",
            "Layer 1, Epoch [13/100], Step [234/469], Positive Goodness: 0.7612, Negative Goodness: 0.1465\n",
            "Layer 2, Epoch [13/100], Step [234/469], Positive Goodness: 0.8047, Negative Goodness: 0.1197\n",
            "Layer 3, Epoch [13/100], Step [234/469], Positive Goodness: 0.7529, Negative Goodness: 0.1843\n",
            "Final Layer, Epoch [13/100], Step [234/469], Loss: 0.0157\n",
            "Epoch [13/100], Step [234/469], Loss: 0.3413, Test Accuracy: 0.8630\n",
            "Layer 0, Epoch [13/100], Step [351/469], Positive Goodness: 0.9490, Negative Goodness: 0.4126\n",
            "Layer 1, Epoch [13/100], Step [351/469], Positive Goodness: 0.7708, Negative Goodness: 0.1457\n",
            "Layer 2, Epoch [13/100], Step [351/469], Positive Goodness: 0.7985, Negative Goodness: 0.1124\n",
            "Layer 3, Epoch [13/100], Step [351/469], Positive Goodness: 0.7542, Negative Goodness: 0.1792\n",
            "Final Layer, Epoch [13/100], Step [351/469], Loss: 0.0156\n",
            "Epoch [13/100], Step [351/469], Loss: 0.3174, Test Accuracy: 0.8560\n",
            "Layer 0, Epoch [13/100], Step [468/469], Positive Goodness: 0.9425, Negative Goodness: 0.4086\n",
            "Layer 1, Epoch [13/100], Step [468/469], Positive Goodness: 0.7781, Negative Goodness: 0.1547\n",
            "Layer 2, Epoch [13/100], Step [468/469], Positive Goodness: 0.7921, Negative Goodness: 0.1062\n",
            "Layer 3, Epoch [13/100], Step [468/469], Positive Goodness: 0.7751, Negative Goodness: 0.1995\n",
            "Final Layer, Epoch [13/100], Step [468/469], Loss: 0.0175\n",
            "Epoch [13/100], Step [468/469], Loss: 0.3229, Test Accuracy: 0.8675\n",
            "Layer 0, Epoch [14/100], Step [117/469], Positive Goodness: 0.9443, Negative Goodness: 0.3871\n",
            "Layer 1, Epoch [14/100], Step [117/469], Positive Goodness: 0.7904, Negative Goodness: 0.1575\n",
            "Layer 2, Epoch [14/100], Step [117/469], Positive Goodness: 0.8201, Negative Goodness: 0.1282\n",
            "Layer 3, Epoch [14/100], Step [117/469], Positive Goodness: 0.7629, Negative Goodness: 0.1963\n",
            "Final Layer, Epoch [14/100], Step [117/469], Loss: 0.0133\n",
            "Epoch [14/100], Step [117/469], Loss: 0.3297, Test Accuracy: 0.8679\n",
            "Layer 0, Epoch [14/100], Step [234/469], Positive Goodness: 0.9403, Negative Goodness: 0.3828\n",
            "Layer 1, Epoch [14/100], Step [234/469], Positive Goodness: 0.7897, Negative Goodness: 0.1434\n",
            "Layer 2, Epoch [14/100], Step [234/469], Positive Goodness: 0.8032, Negative Goodness: 0.1084\n",
            "Layer 3, Epoch [14/100], Step [234/469], Positive Goodness: 0.7762, Negative Goodness: 0.2006\n",
            "Final Layer, Epoch [14/100], Step [234/469], Loss: 0.0146\n",
            "Epoch [14/100], Step [234/469], Loss: 0.3549, Test Accuracy: 0.8763\n",
            "Layer 0, Epoch [14/100], Step [351/469], Positive Goodness: 0.9423, Negative Goodness: 0.3700\n",
            "Layer 1, Epoch [14/100], Step [351/469], Positive Goodness: 0.8037, Negative Goodness: 0.1507\n",
            "Layer 2, Epoch [14/100], Step [351/469], Positive Goodness: 0.8154, Negative Goodness: 0.1153\n",
            "Layer 3, Epoch [14/100], Step [351/469], Positive Goodness: 0.7825, Negative Goodness: 0.1953\n",
            "Final Layer, Epoch [14/100], Step [351/469], Loss: 0.0154\n",
            "Epoch [14/100], Step [351/469], Loss: 0.3212, Test Accuracy: 0.8761\n",
            "Layer 0, Epoch [14/100], Step [468/469], Positive Goodness: 0.9438, Negative Goodness: 0.3718\n",
            "Layer 1, Epoch [14/100], Step [468/469], Positive Goodness: 0.7960, Negative Goodness: 0.1412\n",
            "Layer 2, Epoch [14/100], Step [468/469], Positive Goodness: 0.8056, Negative Goodness: 0.0972\n",
            "Layer 3, Epoch [14/100], Step [468/469], Positive Goodness: 0.7719, Negative Goodness: 0.1727\n",
            "Final Layer, Epoch [14/100], Step [468/469], Loss: 0.0146\n",
            "Epoch [14/100], Step [468/469], Loss: 0.3182, Test Accuracy: 0.8737\n",
            "Layer 0, Epoch [15/100], Step [117/469], Positive Goodness: 0.9404, Negative Goodness: 0.3661\n",
            "Layer 1, Epoch [15/100], Step [117/469], Positive Goodness: 0.8033, Negative Goodness: 0.1366\n",
            "Layer 2, Epoch [15/100], Step [117/469], Positive Goodness: 0.8239, Negative Goodness: 0.1164\n",
            "Layer 3, Epoch [15/100], Step [117/469], Positive Goodness: 0.7828, Negative Goodness: 0.1772\n",
            "Final Layer, Epoch [15/100], Step [117/469], Loss: 0.0146\n",
            "Epoch [15/100], Step [117/469], Loss: 0.2981, Test Accuracy: 0.8801\n",
            "Layer 0, Epoch [15/100], Step [234/469], Positive Goodness: 0.9439, Negative Goodness: 0.3678\n",
            "Layer 1, Epoch [15/100], Step [234/469], Positive Goodness: 0.7969, Negative Goodness: 0.1192\n",
            "Layer 2, Epoch [15/100], Step [234/469], Positive Goodness: 0.8356, Negative Goodness: 0.1159\n",
            "Layer 3, Epoch [15/100], Step [234/469], Positive Goodness: 0.7879, Negative Goodness: 0.1753\n",
            "Final Layer, Epoch [15/100], Step [234/469], Loss: 0.0120\n",
            "Epoch [15/100], Step [234/469], Loss: 0.3384, Test Accuracy: 0.8900\n",
            "Layer 0, Epoch [15/100], Step [351/469], Positive Goodness: 0.9449, Negative Goodness: 0.3528\n",
            "Layer 1, Epoch [15/100], Step [351/469], Positive Goodness: 0.8099, Negative Goodness: 0.1221\n",
            "Layer 2, Epoch [15/100], Step [351/469], Positive Goodness: 0.8287, Negative Goodness: 0.0978\n",
            "Layer 3, Epoch [15/100], Step [351/469], Positive Goodness: 0.7917, Negative Goodness: 0.1618\n",
            "Final Layer, Epoch [15/100], Step [351/469], Loss: 0.0109\n",
            "Epoch [15/100], Step [351/469], Loss: 0.3245, Test Accuracy: 0.8914\n",
            "Layer 0, Epoch [15/100], Step [468/469], Positive Goodness: 0.9436, Negative Goodness: 0.3654\n",
            "Layer 1, Epoch [15/100], Step [468/469], Positive Goodness: 0.8170, Negative Goodness: 0.1254\n",
            "Layer 2, Epoch [15/100], Step [468/469], Positive Goodness: 0.8345, Negative Goodness: 0.1073\n",
            "Layer 3, Epoch [15/100], Step [468/469], Positive Goodness: 0.7879, Negative Goodness: 0.1641\n",
            "Final Layer, Epoch [15/100], Step [468/469], Loss: 0.0134\n",
            "Epoch [15/100], Step [468/469], Loss: 0.3016, Test Accuracy: 0.8943\n",
            "Layer 0, Epoch [16/100], Step [117/469], Positive Goodness: 0.9482, Negative Goodness: 0.3501\n",
            "Layer 1, Epoch [16/100], Step [117/469], Positive Goodness: 0.8187, Negative Goodness: 0.1196\n",
            "Layer 2, Epoch [16/100], Step [117/469], Positive Goodness: 0.8393, Negative Goodness: 0.0955\n",
            "Layer 3, Epoch [16/100], Step [117/469], Positive Goodness: 0.7942, Negative Goodness: 0.1664\n",
            "Final Layer, Epoch [16/100], Step [117/469], Loss: 0.0113\n",
            "Epoch [16/100], Step [117/469], Loss: 0.2942, Test Accuracy: 0.8960\n",
            "Layer 0, Epoch [16/100], Step [234/469], Positive Goodness: 0.9456, Negative Goodness: 0.3510\n",
            "Layer 1, Epoch [16/100], Step [234/469], Positive Goodness: 0.8203, Negative Goodness: 0.1182\n",
            "Layer 2, Epoch [16/100], Step [234/469], Positive Goodness: 0.8309, Negative Goodness: 0.0892\n",
            "Layer 3, Epoch [16/100], Step [234/469], Positive Goodness: 0.7911, Negative Goodness: 0.1503\n",
            "Final Layer, Epoch [16/100], Step [234/469], Loss: 0.0125\n",
            "Epoch [16/100], Step [234/469], Loss: 0.3008, Test Accuracy: 0.8954\n",
            "Layer 0, Epoch [16/100], Step [351/469], Positive Goodness: 0.9436, Negative Goodness: 0.3441\n",
            "Layer 1, Epoch [16/100], Step [351/469], Positive Goodness: 0.8276, Negative Goodness: 0.1171\n",
            "Layer 2, Epoch [16/100], Step [351/469], Positive Goodness: 0.8413, Negative Goodness: 0.0960\n",
            "Layer 3, Epoch [16/100], Step [351/469], Positive Goodness: 0.8050, Negative Goodness: 0.1605\n",
            "Final Layer, Epoch [16/100], Step [351/469], Loss: 0.0107\n",
            "Epoch [16/100], Step [351/469], Loss: 0.2905, Test Accuracy: 0.9014\n",
            "Layer 0, Epoch [16/100], Step [468/469], Positive Goodness: 0.9420, Negative Goodness: 0.3376\n",
            "Layer 1, Epoch [16/100], Step [468/469], Positive Goodness: 0.8202, Negative Goodness: 0.1046\n",
            "Layer 2, Epoch [16/100], Step [468/469], Positive Goodness: 0.8408, Negative Goodness: 0.0879\n",
            "Layer 3, Epoch [16/100], Step [468/469], Positive Goodness: 0.7966, Negative Goodness: 0.1376\n",
            "Final Layer, Epoch [16/100], Step [468/469], Loss: 0.0088\n",
            "Epoch [16/100], Step [468/469], Loss: 0.2944, Test Accuracy: 0.8988\n",
            "Layer 0, Epoch [17/100], Step [117/469], Positive Goodness: 0.9438, Negative Goodness: 0.3292\n",
            "Layer 1, Epoch [17/100], Step [117/469], Positive Goodness: 0.8306, Negative Goodness: 0.1066\n",
            "Layer 2, Epoch [17/100], Step [117/469], Positive Goodness: 0.8578, Negative Goodness: 0.1000\n",
            "Layer 3, Epoch [17/100], Step [117/469], Positive Goodness: 0.8143, Negative Goodness: 0.1544\n",
            "Final Layer, Epoch [17/100], Step [117/469], Loss: 0.0102\n",
            "Epoch [17/100], Step [117/469], Loss: 0.3178, Test Accuracy: 0.9082\n",
            "Layer 0, Epoch [17/100], Step [234/469], Positive Goodness: 0.9461, Negative Goodness: 0.3332\n",
            "Layer 1, Epoch [17/100], Step [234/469], Positive Goodness: 0.8380, Negative Goodness: 0.1136\n",
            "Layer 2, Epoch [17/100], Step [234/469], Positive Goodness: 0.8320, Negative Goodness: 0.0816\n",
            "Layer 3, Epoch [17/100], Step [234/469], Positive Goodness: 0.8155, Negative Goodness: 0.1572\n",
            "Final Layer, Epoch [17/100], Step [234/469], Loss: 0.0083\n",
            "Epoch [17/100], Step [234/469], Loss: 0.2816, Test Accuracy: 0.9028\n",
            "Layer 0, Epoch [17/100], Step [351/469], Positive Goodness: 0.9417, Negative Goodness: 0.3238\n",
            "Layer 1, Epoch [17/100], Step [351/469], Positive Goodness: 0.8221, Negative Goodness: 0.0957\n",
            "Layer 2, Epoch [17/100], Step [351/469], Positive Goodness: 0.8288, Negative Goodness: 0.0730\n",
            "Layer 3, Epoch [17/100], Step [351/469], Positive Goodness: 0.8163, Negative Goodness: 0.1575\n",
            "Final Layer, Epoch [17/100], Step [351/469], Loss: 0.0081\n",
            "Epoch [17/100], Step [351/469], Loss: 0.2850, Test Accuracy: 0.9048\n",
            "Layer 0, Epoch [17/100], Step [468/469], Positive Goodness: 0.9420, Negative Goodness: 0.3213\n",
            "Layer 1, Epoch [17/100], Step [468/469], Positive Goodness: 0.8396, Negative Goodness: 0.1158\n",
            "Layer 2, Epoch [17/100], Step [468/469], Positive Goodness: 0.8376, Negative Goodness: 0.0724\n",
            "Layer 3, Epoch [17/100], Step [468/469], Positive Goodness: 0.8076, Negative Goodness: 0.1315\n",
            "Final Layer, Epoch [17/100], Step [468/469], Loss: 0.0098\n",
            "Epoch [17/100], Step [468/469], Loss: 0.2838, Test Accuracy: 0.9084\n",
            "Layer 0, Epoch [18/100], Step [117/469], Positive Goodness: 0.9465, Negative Goodness: 0.3126\n",
            "Layer 1, Epoch [18/100], Step [117/469], Positive Goodness: 0.8481, Negative Goodness: 0.1128\n",
            "Layer 2, Epoch [18/100], Step [117/469], Positive Goodness: 0.8648, Negative Goodness: 0.0951\n",
            "Layer 3, Epoch [18/100], Step [117/469], Positive Goodness: 0.8232, Negative Goodness: 0.1399\n",
            "Final Layer, Epoch [18/100], Step [117/469], Loss: 0.0074\n",
            "Epoch [18/100], Step [117/469], Loss: 0.2898, Test Accuracy: 0.9134\n",
            "Layer 0, Epoch [18/100], Step [234/469], Positive Goodness: 0.9445, Negative Goodness: 0.3025\n",
            "Layer 1, Epoch [18/100], Step [234/469], Positive Goodness: 0.8479, Negative Goodness: 0.1076\n",
            "Layer 2, Epoch [18/100], Step [234/469], Positive Goodness: 0.8576, Negative Goodness: 0.0856\n",
            "Layer 3, Epoch [18/100], Step [234/469], Positive Goodness: 0.8192, Negative Goodness: 0.1323\n",
            "Final Layer, Epoch [18/100], Step [234/469], Loss: 0.0072\n",
            "Epoch [18/100], Step [234/469], Loss: 0.2806, Test Accuracy: 0.9144\n",
            "Layer 0, Epoch [18/100], Step [351/469], Positive Goodness: 0.9459, Negative Goodness: 0.3051\n",
            "Layer 1, Epoch [18/100], Step [351/469], Positive Goodness: 0.8436, Negative Goodness: 0.1042\n",
            "Layer 2, Epoch [18/100], Step [351/469], Positive Goodness: 0.8592, Negative Goodness: 0.0831\n",
            "Layer 3, Epoch [18/100], Step [351/469], Positive Goodness: 0.8259, Negative Goodness: 0.1388\n",
            "Final Layer, Epoch [18/100], Step [351/469], Loss: 0.0084\n",
            "Epoch [18/100], Step [351/469], Loss: 0.2853, Test Accuracy: 0.9151\n",
            "Layer 0, Epoch [18/100], Step [468/469], Positive Goodness: 0.9398, Negative Goodness: 0.2993\n",
            "Layer 1, Epoch [18/100], Step [468/469], Positive Goodness: 0.8387, Negative Goodness: 0.1014\n",
            "Layer 2, Epoch [18/100], Step [468/469], Positive Goodness: 0.8695, Negative Goodness: 0.0979\n",
            "Layer 3, Epoch [18/100], Step [468/469], Positive Goodness: 0.8454, Negative Goodness: 0.1499\n",
            "Final Layer, Epoch [18/100], Step [468/469], Loss: 0.0083\n",
            "Epoch [18/100], Step [468/469], Loss: 0.2593, Test Accuracy: 0.9135\n",
            "Layer 0, Epoch [19/100], Step [117/469], Positive Goodness: 0.9467, Negative Goodness: 0.2832\n",
            "Layer 1, Epoch [19/100], Step [117/469], Positive Goodness: 0.8514, Negative Goodness: 0.0910\n",
            "Layer 2, Epoch [19/100], Step [117/469], Positive Goodness: 0.8701, Negative Goodness: 0.0803\n",
            "Layer 3, Epoch [19/100], Step [117/469], Positive Goodness: 0.8410, Negative Goodness: 0.1344\n",
            "Final Layer, Epoch [19/100], Step [117/469], Loss: 0.0061\n",
            "Epoch [19/100], Step [117/469], Loss: 0.2867, Test Accuracy: 0.9186\n",
            "Layer 0, Epoch [19/100], Step [234/469], Positive Goodness: 0.9444, Negative Goodness: 0.2740\n",
            "Layer 1, Epoch [19/100], Step [234/469], Positive Goodness: 0.8547, Negative Goodness: 0.0956\n",
            "Layer 2, Epoch [19/100], Step [234/469], Positive Goodness: 0.8691, Negative Goodness: 0.0875\n",
            "Layer 3, Epoch [19/100], Step [234/469], Positive Goodness: 0.8423, Negative Goodness: 0.1375\n",
            "Final Layer, Epoch [19/100], Step [234/469], Loss: 0.0069\n",
            "Epoch [19/100], Step [234/469], Loss: 0.2697, Test Accuracy: 0.9189\n",
            "Layer 0, Epoch [19/100], Step [351/469], Positive Goodness: 0.9428, Negative Goodness: 0.2869\n",
            "Layer 1, Epoch [19/100], Step [351/469], Positive Goodness: 0.8480, Negative Goodness: 0.0946\n",
            "Layer 2, Epoch [19/100], Step [351/469], Positive Goodness: 0.8761, Negative Goodness: 0.0943\n",
            "Layer 3, Epoch [19/100], Step [351/469], Positive Goodness: 0.8370, Negative Goodness: 0.1314\n",
            "Final Layer, Epoch [19/100], Step [351/469], Loss: 0.0084\n",
            "Epoch [19/100], Step [351/469], Loss: 0.2771, Test Accuracy: 0.9205\n",
            "Layer 0, Epoch [19/100], Step [468/469], Positive Goodness: 0.9474, Negative Goodness: 0.2759\n",
            "Layer 1, Epoch [19/100], Step [468/469], Positive Goodness: 0.8587, Negative Goodness: 0.0983\n",
            "Layer 2, Epoch [19/100], Step [468/469], Positive Goodness: 0.8680, Negative Goodness: 0.0768\n",
            "Layer 3, Epoch [19/100], Step [468/469], Positive Goodness: 0.8340, Negative Goodness: 0.1165\n",
            "Final Layer, Epoch [19/100], Step [468/469], Loss: 0.0070\n",
            "Epoch [19/100], Step [468/469], Loss: 0.2762, Test Accuracy: 0.9230\n",
            "Layer 0, Epoch [20/100], Step [117/469], Positive Goodness: 0.9477, Negative Goodness: 0.2736\n",
            "Layer 1, Epoch [20/100], Step [117/469], Positive Goodness: 0.8565, Negative Goodness: 0.0945\n",
            "Layer 2, Epoch [20/100], Step [117/469], Positive Goodness: 0.8734, Negative Goodness: 0.0782\n",
            "Layer 3, Epoch [20/100], Step [117/469], Positive Goodness: 0.8467, Negative Goodness: 0.1307\n",
            "Final Layer, Epoch [20/100], Step [117/469], Loss: 0.0072\n",
            "Epoch [20/100], Step [117/469], Loss: 0.2831, Test Accuracy: 0.9213\n",
            "Layer 0, Epoch [20/100], Step [234/469], Positive Goodness: 0.9418, Negative Goodness: 0.2652\n",
            "Layer 1, Epoch [20/100], Step [234/469], Positive Goodness: 0.8583, Negative Goodness: 0.0796\n",
            "Layer 2, Epoch [20/100], Step [234/469], Positive Goodness: 0.8749, Negative Goodness: 0.0740\n",
            "Layer 3, Epoch [20/100], Step [234/469], Positive Goodness: 0.8473, Negative Goodness: 0.1225\n",
            "Final Layer, Epoch [20/100], Step [234/469], Loss: 0.0064\n",
            "Epoch [20/100], Step [234/469], Loss: 0.2606, Test Accuracy: 0.9230\n",
            "Layer 0, Epoch [20/100], Step [351/469], Positive Goodness: 0.9475, Negative Goodness: 0.2560\n",
            "Layer 1, Epoch [20/100], Step [351/469], Positive Goodness: 0.8603, Negative Goodness: 0.0875\n",
            "Layer 2, Epoch [20/100], Step [351/469], Positive Goodness: 0.8672, Negative Goodness: 0.0671\n",
            "Layer 3, Epoch [20/100], Step [351/469], Positive Goodness: 0.8435, Negative Goodness: 0.1203\n",
            "Final Layer, Epoch [20/100], Step [351/469], Loss: 0.0064\n",
            "Epoch [20/100], Step [351/469], Loss: 0.2738, Test Accuracy: 0.9254\n",
            "Layer 0, Epoch [20/100], Step [468/469], Positive Goodness: 0.9441, Negative Goodness: 0.2515\n",
            "Layer 1, Epoch [20/100], Step [468/469], Positive Goodness: 0.8687, Negative Goodness: 0.0860\n",
            "Layer 2, Epoch [20/100], Step [468/469], Positive Goodness: 0.8803, Negative Goodness: 0.0725\n",
            "Layer 3, Epoch [20/100], Step [468/469], Positive Goodness: 0.8609, Negative Goodness: 0.1277\n",
            "Final Layer, Epoch [20/100], Step [468/469], Loss: 0.0051\n",
            "Epoch [20/100], Step [468/469], Loss: 0.2750, Test Accuracy: 0.9273\n",
            "Layer 0, Epoch [21/100], Step [117/469], Positive Goodness: 0.9472, Negative Goodness: 0.2512\n",
            "Layer 1, Epoch [21/100], Step [117/469], Positive Goodness: 0.8667, Negative Goodness: 0.0907\n",
            "Layer 2, Epoch [21/100], Step [117/469], Positive Goodness: 0.8789, Negative Goodness: 0.0705\n",
            "Layer 3, Epoch [21/100], Step [117/469], Positive Goodness: 0.8558, Negative Goodness: 0.1200\n",
            "Final Layer, Epoch [21/100], Step [117/469], Loss: 0.0065\n",
            "Epoch [21/100], Step [117/469], Loss: 0.2572, Test Accuracy: 0.9257\n",
            "Layer 0, Epoch [21/100], Step [234/469], Positive Goodness: 0.9476, Negative Goodness: 0.2387\n",
            "Layer 1, Epoch [21/100], Step [234/469], Positive Goodness: 0.8747, Negative Goodness: 0.0794\n",
            "Layer 2, Epoch [21/100], Step [234/469], Positive Goodness: 0.8681, Negative Goodness: 0.0590\n",
            "Layer 3, Epoch [21/100], Step [234/469], Positive Goodness: 0.8725, Negative Goodness: 0.1352\n",
            "Final Layer, Epoch [21/100], Step [234/469], Loss: 0.0050\n",
            "Epoch [21/100], Step [234/469], Loss: 0.2663, Test Accuracy: 0.9288\n",
            "Layer 0, Epoch [21/100], Step [351/469], Positive Goodness: 0.9463, Negative Goodness: 0.2332\n",
            "Layer 1, Epoch [21/100], Step [351/469], Positive Goodness: 0.8661, Negative Goodness: 0.0722\n",
            "Layer 2, Epoch [21/100], Step [351/469], Positive Goodness: 0.8828, Negative Goodness: 0.0743\n",
            "Layer 3, Epoch [21/100], Step [351/469], Positive Goodness: 0.8631, Negative Goodness: 0.1227\n",
            "Final Layer, Epoch [21/100], Step [351/469], Loss: 0.0050\n",
            "Epoch [21/100], Step [351/469], Loss: 0.2495, Test Accuracy: 0.9281\n",
            "Layer 0, Epoch [21/100], Step [468/469], Positive Goodness: 0.9431, Negative Goodness: 0.2418\n",
            "Layer 1, Epoch [21/100], Step [468/469], Positive Goodness: 0.8706, Negative Goodness: 0.0775\n",
            "Layer 2, Epoch [21/100], Step [468/469], Positive Goodness: 0.8791, Negative Goodness: 0.0671\n",
            "Layer 3, Epoch [21/100], Step [468/469], Positive Goodness: 0.8596, Negative Goodness: 0.1209\n",
            "Final Layer, Epoch [21/100], Step [468/469], Loss: 0.0053\n",
            "Epoch [21/100], Step [468/469], Loss: 0.2559, Test Accuracy: 0.9277\n",
            "Layer 0, Epoch [22/100], Step [117/469], Positive Goodness: 0.9464, Negative Goodness: 0.2324\n",
            "Layer 1, Epoch [22/100], Step [117/469], Positive Goodness: 0.8756, Negative Goodness: 0.0809\n",
            "Layer 2, Epoch [22/100], Step [117/469], Positive Goodness: 0.8770, Negative Goodness: 0.0619\n",
            "Layer 3, Epoch [22/100], Step [117/469], Positive Goodness: 0.8584, Negative Goodness: 0.1119\n",
            "Final Layer, Epoch [22/100], Step [117/469], Loss: 0.0069\n",
            "Epoch [22/100], Step [117/469], Loss: 0.2517, Test Accuracy: 0.9299\n",
            "Layer 0, Epoch [22/100], Step [234/469], Positive Goodness: 0.9480, Negative Goodness: 0.2309\n",
            "Layer 1, Epoch [22/100], Step [234/469], Positive Goodness: 0.8650, Negative Goodness: 0.0621\n",
            "Layer 2, Epoch [22/100], Step [234/469], Positive Goodness: 0.8972, Negative Goodness: 0.0747\n",
            "Layer 3, Epoch [22/100], Step [234/469], Positive Goodness: 0.8620, Negative Goodness: 0.1006\n",
            "Final Layer, Epoch [22/100], Step [234/469], Loss: 0.0043\n",
            "Epoch [22/100], Step [234/469], Loss: 0.2414, Test Accuracy: 0.9311\n",
            "Layer 0, Epoch [22/100], Step [351/469], Positive Goodness: 0.9465, Negative Goodness: 0.2341\n",
            "Layer 1, Epoch [22/100], Step [351/469], Positive Goodness: 0.8789, Negative Goodness: 0.0682\n",
            "Layer 2, Epoch [22/100], Step [351/469], Positive Goodness: 0.8958, Negative Goodness: 0.0725\n",
            "Layer 3, Epoch [22/100], Step [351/469], Positive Goodness: 0.8777, Negative Goodness: 0.1147\n",
            "Final Layer, Epoch [22/100], Step [351/469], Loss: 0.0046\n",
            "Epoch [22/100], Step [351/469], Loss: 0.2496, Test Accuracy: 0.9317\n",
            "Layer 0, Epoch [22/100], Step [468/469], Positive Goodness: 0.9453, Negative Goodness: 0.2298\n",
            "Layer 1, Epoch [22/100], Step [468/469], Positive Goodness: 0.8894, Negative Goodness: 0.0842\n",
            "Layer 2, Epoch [22/100], Step [468/469], Positive Goodness: 0.8947, Negative Goodness: 0.0714\n",
            "Layer 3, Epoch [22/100], Step [468/469], Positive Goodness: 0.8766, Negative Goodness: 0.1179\n",
            "Final Layer, Epoch [22/100], Step [468/469], Loss: 0.0035\n",
            "Epoch [22/100], Step [468/469], Loss: 0.2544, Test Accuracy: 0.9329\n",
            "Layer 0, Epoch [23/100], Step [117/469], Positive Goodness: 0.9458, Negative Goodness: 0.2261\n",
            "Layer 1, Epoch [23/100], Step [117/469], Positive Goodness: 0.8892, Negative Goodness: 0.0758\n",
            "Layer 2, Epoch [23/100], Step [117/469], Positive Goodness: 0.8962, Negative Goodness: 0.0657\n",
            "Layer 3, Epoch [23/100], Step [117/469], Positive Goodness: 0.8825, Negative Goodness: 0.1189\n",
            "Final Layer, Epoch [23/100], Step [117/469], Loss: 0.0049\n",
            "Epoch [23/100], Step [117/469], Loss: 0.2529, Test Accuracy: 0.9341\n",
            "Layer 0, Epoch [23/100], Step [234/469], Positive Goodness: 0.9450, Negative Goodness: 0.2238\n",
            "Layer 1, Epoch [23/100], Step [234/469], Positive Goodness: 0.8881, Negative Goodness: 0.0765\n",
            "Layer 2, Epoch [23/100], Step [234/469], Positive Goodness: 0.9009, Negative Goodness: 0.0681\n",
            "Layer 3, Epoch [23/100], Step [234/469], Positive Goodness: 0.8682, Negative Goodness: 0.1053\n",
            "Final Layer, Epoch [23/100], Step [234/469], Loss: 0.0055\n",
            "Epoch [23/100], Step [234/469], Loss: 0.2438, Test Accuracy: 0.9326\n",
            "Layer 0, Epoch [23/100], Step [351/469], Positive Goodness: 0.9486, Negative Goodness: 0.2245\n",
            "Layer 1, Epoch [23/100], Step [351/469], Positive Goodness: 0.8782, Negative Goodness: 0.0690\n",
            "Layer 2, Epoch [23/100], Step [351/469], Positive Goodness: 0.8847, Negative Goodness: 0.0587\n",
            "Layer 3, Epoch [23/100], Step [351/469], Positive Goodness: 0.8760, Negative Goodness: 0.1075\n",
            "Final Layer, Epoch [23/100], Step [351/469], Loss: 0.0040\n",
            "Epoch [23/100], Step [351/469], Loss: 0.2516, Test Accuracy: 0.9330\n",
            "Layer 0, Epoch [23/100], Step [468/469], Positive Goodness: 0.9452, Negative Goodness: 0.2177\n",
            "Layer 1, Epoch [23/100], Step [468/469], Positive Goodness: 0.8873, Negative Goodness: 0.0735\n",
            "Layer 2, Epoch [23/100], Step [468/469], Positive Goodness: 0.8984, Negative Goodness: 0.0640\n",
            "Layer 3, Epoch [23/100], Step [468/469], Positive Goodness: 0.8877, Negative Goodness: 0.1177\n",
            "Final Layer, Epoch [23/100], Step [468/469], Loss: 0.0035\n",
            "Epoch [23/100], Step [468/469], Loss: 0.2447, Test Accuracy: 0.9360\n",
            "Layer 0, Epoch [24/100], Step [117/469], Positive Goodness: 0.9484, Negative Goodness: 0.2217\n",
            "Layer 1, Epoch [24/100], Step [117/469], Positive Goodness: 0.8937, Negative Goodness: 0.0691\n",
            "Layer 2, Epoch [24/100], Step [117/469], Positive Goodness: 0.8969, Negative Goodness: 0.0606\n",
            "Layer 3, Epoch [24/100], Step [117/469], Positive Goodness: 0.8802, Negative Goodness: 0.1003\n",
            "Final Layer, Epoch [24/100], Step [117/469], Loss: 0.0047\n",
            "Epoch [24/100], Step [117/469], Loss: 0.2493, Test Accuracy: 0.9356\n",
            "Layer 0, Epoch [24/100], Step [234/469], Positive Goodness: 0.9457, Negative Goodness: 0.2114\n",
            "Layer 1, Epoch [24/100], Step [234/469], Positive Goodness: 0.9020, Negative Goodness: 0.0791\n",
            "Layer 2, Epoch [24/100], Step [234/469], Positive Goodness: 0.9025, Negative Goodness: 0.0676\n",
            "Layer 3, Epoch [24/100], Step [234/469], Positive Goodness: 0.8537, Negative Goodness: 0.0870\n",
            "Final Layer, Epoch [24/100], Step [234/469], Loss: 0.0037\n",
            "Epoch [24/100], Step [234/469], Loss: 0.2512, Test Accuracy: 0.9371\n",
            "Layer 0, Epoch [24/100], Step [351/469], Positive Goodness: 0.9486, Negative Goodness: 0.2009\n",
            "Layer 1, Epoch [24/100], Step [351/469], Positive Goodness: 0.8981, Negative Goodness: 0.0702\n",
            "Layer 2, Epoch [24/100], Step [351/469], Positive Goodness: 0.9133, Negative Goodness: 0.0660\n",
            "Layer 3, Epoch [24/100], Step [351/469], Positive Goodness: 0.8837, Negative Goodness: 0.1016\n",
            "Final Layer, Epoch [24/100], Step [351/469], Loss: 0.0034\n",
            "Epoch [24/100], Step [351/469], Loss: 0.2419, Test Accuracy: 0.9383\n",
            "Layer 0, Epoch [24/100], Step [468/469], Positive Goodness: 0.9479, Negative Goodness: 0.2085\n",
            "Layer 1, Epoch [24/100], Step [468/469], Positive Goodness: 0.8951, Negative Goodness: 0.0721\n",
            "Layer 2, Epoch [24/100], Step [468/469], Positive Goodness: 0.9028, Negative Goodness: 0.0631\n",
            "Layer 3, Epoch [24/100], Step [468/469], Positive Goodness: 0.8775, Negative Goodness: 0.1021\n",
            "Final Layer, Epoch [24/100], Step [468/469], Loss: 0.0032\n",
            "Epoch [24/100], Step [468/469], Loss: 0.2302, Test Accuracy: 0.9362\n",
            "Layer 0, Epoch [25/100], Step [117/469], Positive Goodness: 0.9506, Negative Goodness: 0.2045\n",
            "Layer 1, Epoch [25/100], Step [117/469], Positive Goodness: 0.8950, Negative Goodness: 0.0630\n",
            "Layer 2, Epoch [25/100], Step [117/469], Positive Goodness: 0.9107, Negative Goodness: 0.0606\n",
            "Layer 3, Epoch [25/100], Step [117/469], Positive Goodness: 0.8810, Negative Goodness: 0.0937\n",
            "Final Layer, Epoch [25/100], Step [117/469], Loss: 0.0035\n",
            "Epoch [25/100], Step [117/469], Loss: 0.2413, Test Accuracy: 0.9363\n",
            "Layer 0, Epoch [25/100], Step [234/469], Positive Goodness: 0.9484, Negative Goodness: 0.2034\n",
            "Layer 1, Epoch [25/100], Step [234/469], Positive Goodness: 0.9123, Negative Goodness: 0.0872\n",
            "Layer 2, Epoch [25/100], Step [234/469], Positive Goodness: 0.9153, Negative Goodness: 0.0722\n",
            "Layer 3, Epoch [25/100], Step [234/469], Positive Goodness: 0.8790, Negative Goodness: 0.0957\n",
            "Final Layer, Epoch [25/100], Step [234/469], Loss: 0.0041\n",
            "Epoch [25/100], Step [234/469], Loss: 0.2389, Test Accuracy: 0.9367\n",
            "Layer 0, Epoch [25/100], Step [351/469], Positive Goodness: 0.9518, Negative Goodness: 0.1990\n",
            "Layer 1, Epoch [25/100], Step [351/469], Positive Goodness: 0.9089, Negative Goodness: 0.0685\n",
            "Layer 2, Epoch [25/100], Step [351/469], Positive Goodness: 0.9141, Negative Goodness: 0.0664\n",
            "Layer 3, Epoch [25/100], Step [351/469], Positive Goodness: 0.8964, Negative Goodness: 0.0989\n",
            "Final Layer, Epoch [25/100], Step [351/469], Loss: 0.0036\n",
            "Epoch [25/100], Step [351/469], Loss: 0.2282, Test Accuracy: 0.9355\n",
            "Layer 0, Epoch [25/100], Step [468/469], Positive Goodness: 0.9452, Negative Goodness: 0.1917\n",
            "Layer 1, Epoch [25/100], Step [468/469], Positive Goodness: 0.9033, Negative Goodness: 0.0738\n",
            "Layer 2, Epoch [25/100], Step [468/469], Positive Goodness: 0.9025, Negative Goodness: 0.0581\n",
            "Layer 3, Epoch [25/100], Step [468/469], Positive Goodness: 0.8961, Negative Goodness: 0.1040\n",
            "Final Layer, Epoch [25/100], Step [468/469], Loss: 0.0039\n",
            "Epoch [25/100], Step [468/469], Loss: 0.2326, Test Accuracy: 0.9364\n",
            "Layer 0, Epoch [26/100], Step [117/469], Positive Goodness: 0.9503, Negative Goodness: 0.1871\n",
            "Layer 1, Epoch [26/100], Step [117/469], Positive Goodness: 0.9051, Negative Goodness: 0.0619\n",
            "Layer 2, Epoch [26/100], Step [117/469], Positive Goodness: 0.9100, Negative Goodness: 0.0540\n",
            "Layer 3, Epoch [26/100], Step [117/469], Positive Goodness: 0.8990, Negative Goodness: 0.0966\n",
            "Final Layer, Epoch [26/100], Step [117/469], Loss: 0.0037\n",
            "Epoch [26/100], Step [117/469], Loss: 0.2247, Test Accuracy: 0.9380\n",
            "Layer 0, Epoch [26/100], Step [234/469], Positive Goodness: 0.9494, Negative Goodness: 0.1934\n",
            "Layer 1, Epoch [26/100], Step [234/469], Positive Goodness: 0.9078, Negative Goodness: 0.0703\n",
            "Layer 2, Epoch [26/100], Step [234/469], Positive Goodness: 0.9119, Negative Goodness: 0.0585\n",
            "Layer 3, Epoch [26/100], Step [234/469], Positive Goodness: 0.8952, Negative Goodness: 0.0982\n",
            "Final Layer, Epoch [26/100], Step [234/469], Loss: 0.0031\n",
            "Epoch [26/100], Step [234/469], Loss: 0.2241, Test Accuracy: 0.9388\n",
            "Layer 0, Epoch [26/100], Step [351/469], Positive Goodness: 0.9497, Negative Goodness: 0.1821\n",
            "Layer 1, Epoch [26/100], Step [351/469], Positive Goodness: 0.9048, Negative Goodness: 0.0698\n",
            "Layer 2, Epoch [26/100], Step [351/469], Positive Goodness: 0.9181, Negative Goodness: 0.0616\n",
            "Layer 3, Epoch [26/100], Step [351/469], Positive Goodness: 0.8930, Negative Goodness: 0.0967\n",
            "Final Layer, Epoch [26/100], Step [351/469], Loss: 0.0038\n",
            "Epoch [26/100], Step [351/469], Loss: 0.2216, Test Accuracy: 0.9371\n",
            "Layer 0, Epoch [26/100], Step [468/469], Positive Goodness: 0.9476, Negative Goodness: 0.1926\n",
            "Layer 1, Epoch [26/100], Step [468/469], Positive Goodness: 0.9115, Negative Goodness: 0.0723\n",
            "Layer 2, Epoch [26/100], Step [468/469], Positive Goodness: 0.9195, Negative Goodness: 0.0649\n",
            "Layer 3, Epoch [26/100], Step [468/469], Positive Goodness: 0.9003, Negative Goodness: 0.1009\n",
            "Final Layer, Epoch [26/100], Step [468/469], Loss: 0.0027\n",
            "Epoch [26/100], Step [468/469], Loss: 0.2264, Test Accuracy: 0.9365\n",
            "Layer 0, Epoch [27/100], Step [117/469], Positive Goodness: 0.9509, Negative Goodness: 0.1803\n",
            "Layer 1, Epoch [27/100], Step [117/469], Positive Goodness: 0.9110, Negative Goodness: 0.0716\n",
            "Layer 2, Epoch [27/100], Step [117/469], Positive Goodness: 0.9175, Negative Goodness: 0.0586\n",
            "Layer 3, Epoch [27/100], Step [117/469], Positive Goodness: 0.8909, Negative Goodness: 0.0879\n",
            "Final Layer, Epoch [27/100], Step [117/469], Loss: 0.0033\n",
            "Epoch [27/100], Step [117/469], Loss: 0.2276, Test Accuracy: 0.9405\n",
            "Layer 0, Epoch [27/100], Step [234/469], Positive Goodness: 0.9474, Negative Goodness: 0.1753\n",
            "Layer 1, Epoch [27/100], Step [234/469], Positive Goodness: 0.9060, Negative Goodness: 0.0662\n",
            "Layer 2, Epoch [27/100], Step [234/469], Positive Goodness: 0.9226, Negative Goodness: 0.0633\n",
            "Layer 3, Epoch [27/100], Step [234/469], Positive Goodness: 0.8955, Negative Goodness: 0.0888\n",
            "Final Layer, Epoch [27/100], Step [234/469], Loss: 0.0030\n",
            "Epoch [27/100], Step [234/469], Loss: 0.2271, Test Accuracy: 0.9409\n",
            "Layer 0, Epoch [27/100], Step [351/469], Positive Goodness: 0.9475, Negative Goodness: 0.1709\n",
            "Layer 1, Epoch [27/100], Step [351/469], Positive Goodness: 0.9144, Negative Goodness: 0.0713\n",
            "Layer 2, Epoch [27/100], Step [351/469], Positive Goodness: 0.9220, Negative Goodness: 0.0621\n",
            "Layer 3, Epoch [27/100], Step [351/469], Positive Goodness: 0.9002, Negative Goodness: 0.0926\n",
            "Final Layer, Epoch [27/100], Step [351/469], Loss: 0.0030\n",
            "Epoch [27/100], Step [351/469], Loss: 0.2195, Test Accuracy: 0.9407\n",
            "Layer 0, Epoch [27/100], Step [468/469], Positive Goodness: 0.9466, Negative Goodness: 0.1704\n",
            "Layer 1, Epoch [27/100], Step [468/469], Positive Goodness: 0.9122, Negative Goodness: 0.0611\n",
            "Layer 2, Epoch [27/100], Step [468/469], Positive Goodness: 0.9221, Negative Goodness: 0.0610\n",
            "Layer 3, Epoch [27/100], Step [468/469], Positive Goodness: 0.9077, Negative Goodness: 0.1030\n",
            "Final Layer, Epoch [27/100], Step [468/469], Loss: 0.0022\n",
            "Epoch [27/100], Step [468/469], Loss: 0.2177, Test Accuracy: 0.9407\n",
            "Layer 0, Epoch [28/100], Step [117/469], Positive Goodness: 0.9485, Negative Goodness: 0.1643\n",
            "Layer 1, Epoch [28/100], Step [117/469], Positive Goodness: 0.9078, Negative Goodness: 0.0613\n",
            "Layer 2, Epoch [28/100], Step [117/469], Positive Goodness: 0.9222, Negative Goodness: 0.0582\n",
            "Layer 3, Epoch [28/100], Step [117/469], Positive Goodness: 0.9031, Negative Goodness: 0.0964\n",
            "Final Layer, Epoch [28/100], Step [117/469], Loss: 0.0032\n",
            "Epoch [28/100], Step [117/469], Loss: 0.2150, Test Accuracy: 0.9400\n",
            "Layer 0, Epoch [28/100], Step [234/469], Positive Goodness: 0.9463, Negative Goodness: 0.1630\n",
            "Layer 1, Epoch [28/100], Step [234/469], Positive Goodness: 0.9120, Negative Goodness: 0.0670\n",
            "Layer 2, Epoch [28/100], Step [234/469], Positive Goodness: 0.9189, Negative Goodness: 0.0580\n",
            "Layer 3, Epoch [28/100], Step [234/469], Positive Goodness: 0.9034, Negative Goodness: 0.1037\n",
            "Final Layer, Epoch [28/100], Step [234/469], Loss: 0.0022\n",
            "Epoch [28/100], Step [234/469], Loss: 0.2112, Test Accuracy: 0.9428\n",
            "Layer 0, Epoch [28/100], Step [351/469], Positive Goodness: 0.9468, Negative Goodness: 0.1623\n",
            "Layer 1, Epoch [28/100], Step [351/469], Positive Goodness: 0.9179, Negative Goodness: 0.0720\n",
            "Layer 2, Epoch [28/100], Step [351/469], Positive Goodness: 0.9170, Negative Goodness: 0.0584\n",
            "Layer 3, Epoch [28/100], Step [351/469], Positive Goodness: 0.8964, Negative Goodness: 0.0946\n",
            "Final Layer, Epoch [28/100], Step [351/469], Loss: 0.0026\n",
            "Epoch [28/100], Step [351/469], Loss: 0.2097, Test Accuracy: 0.9418\n",
            "Layer 0, Epoch [28/100], Step [468/469], Positive Goodness: 0.9461, Negative Goodness: 0.1548\n",
            "Layer 1, Epoch [28/100], Step [468/469], Positive Goodness: 0.9081, Negative Goodness: 0.0544\n",
            "Layer 2, Epoch [28/100], Step [468/469], Positive Goodness: 0.9326, Negative Goodness: 0.0698\n",
            "Layer 3, Epoch [28/100], Step [468/469], Positive Goodness: 0.9040, Negative Goodness: 0.0978\n",
            "Final Layer, Epoch [28/100], Step [468/469], Loss: 0.0021\n",
            "Epoch [28/100], Step [468/469], Loss: 0.2180, Test Accuracy: 0.9399\n",
            "Layer 0, Epoch [29/100], Step [117/469], Positive Goodness: 0.9469, Negative Goodness: 0.1501\n",
            "Layer 1, Epoch [29/100], Step [117/469], Positive Goodness: 0.9112, Negative Goodness: 0.0576\n",
            "Layer 2, Epoch [29/100], Step [117/469], Positive Goodness: 0.9279, Negative Goodness: 0.0626\n",
            "Layer 3, Epoch [29/100], Step [117/469], Positive Goodness: 0.9052, Negative Goodness: 0.0962\n",
            "Final Layer, Epoch [29/100], Step [117/469], Loss: 0.0021\n",
            "Epoch [29/100], Step [117/469], Loss: 0.2105, Test Accuracy: 0.9446\n",
            "Layer 0, Epoch [29/100], Step [234/469], Positive Goodness: 0.9452, Negative Goodness: 0.1385\n",
            "Layer 1, Epoch [29/100], Step [234/469], Positive Goodness: 0.9092, Negative Goodness: 0.0559\n",
            "Layer 2, Epoch [29/100], Step [234/469], Positive Goodness: 0.9220, Negative Goodness: 0.0552\n",
            "Layer 3, Epoch [29/100], Step [234/469], Positive Goodness: 0.9026, Negative Goodness: 0.0937\n",
            "Final Layer, Epoch [29/100], Step [234/469], Loss: 0.0024\n",
            "Epoch [29/100], Step [234/469], Loss: 0.2133, Test Accuracy: 0.9364\n",
            "Layer 0, Epoch [29/100], Step [351/469], Positive Goodness: 0.9471, Negative Goodness: 0.1496\n",
            "Layer 1, Epoch [29/100], Step [351/469], Positive Goodness: 0.9183, Negative Goodness: 0.0577\n",
            "Layer 2, Epoch [29/100], Step [351/469], Positive Goodness: 0.9213, Negative Goodness: 0.0587\n",
            "Layer 3, Epoch [29/100], Step [351/469], Positive Goodness: 0.9143, Negative Goodness: 0.0979\n",
            "Final Layer, Epoch [29/100], Step [351/469], Loss: 0.0024\n",
            "Epoch [29/100], Step [351/469], Loss: 0.2195, Test Accuracy: 0.9422\n",
            "Layer 0, Epoch [29/100], Step [468/469], Positive Goodness: 0.9469, Negative Goodness: 0.1455\n",
            "Layer 1, Epoch [29/100], Step [468/469], Positive Goodness: 0.9222, Negative Goodness: 0.0674\n",
            "Layer 2, Epoch [29/100], Step [468/469], Positive Goodness: 0.9281, Negative Goodness: 0.0602\n",
            "Layer 3, Epoch [29/100], Step [468/469], Positive Goodness: 0.9131, Negative Goodness: 0.0981\n",
            "Final Layer, Epoch [29/100], Step [468/469], Loss: 0.0020\n",
            "Epoch [29/100], Step [468/469], Loss: 0.2078, Test Accuracy: 0.9433\n",
            "Layer 0, Epoch [30/100], Step [117/469], Positive Goodness: 0.9478, Negative Goodness: 0.1413\n",
            "Layer 1, Epoch [30/100], Step [117/469], Positive Goodness: 0.9202, Negative Goodness: 0.0588\n",
            "Layer 2, Epoch [30/100], Step [117/469], Positive Goodness: 0.9278, Negative Goodness: 0.0552\n",
            "Layer 3, Epoch [30/100], Step [117/469], Positive Goodness: 0.9064, Negative Goodness: 0.0873\n",
            "Final Layer, Epoch [30/100], Step [117/469], Loss: 0.0021\n",
            "Epoch [30/100], Step [117/469], Loss: 0.2070, Test Accuracy: 0.9463\n",
            "Layer 0, Epoch [30/100], Step [234/469], Positive Goodness: 0.9494, Negative Goodness: 0.1377\n",
            "Layer 1, Epoch [30/100], Step [234/469], Positive Goodness: 0.9226, Negative Goodness: 0.0541\n",
            "Layer 2, Epoch [30/100], Step [234/469], Positive Goodness: 0.9234, Negative Goodness: 0.0489\n",
            "Layer 3, Epoch [30/100], Step [234/469], Positive Goodness: 0.9083, Negative Goodness: 0.0849\n",
            "Final Layer, Epoch [30/100], Step [234/469], Loss: 0.0020\n",
            "Epoch [30/100], Step [234/469], Loss: 0.2165, Test Accuracy: 0.9466\n",
            "Layer 0, Epoch [30/100], Step [351/469], Positive Goodness: 0.9485, Negative Goodness: 0.1366\n",
            "Layer 1, Epoch [30/100], Step [351/469], Positive Goodness: 0.9264, Negative Goodness: 0.0676\n",
            "Layer 2, Epoch [30/100], Step [351/469], Positive Goodness: 0.9299, Negative Goodness: 0.0623\n",
            "Layer 3, Epoch [30/100], Step [351/469], Positive Goodness: 0.9199, Negative Goodness: 0.1059\n",
            "Final Layer, Epoch [30/100], Step [351/469], Loss: 0.0025\n",
            "Epoch [30/100], Step [351/469], Loss: 0.2134, Test Accuracy: 0.9413\n",
            "Layer 0, Epoch [30/100], Step [468/469], Positive Goodness: 0.9513, Negative Goodness: 0.1299\n",
            "Layer 1, Epoch [30/100], Step [468/469], Positive Goodness: 0.9289, Negative Goodness: 0.0583\n",
            "Layer 2, Epoch [30/100], Step [468/469], Positive Goodness: 0.9332, Negative Goodness: 0.0577\n",
            "Layer 3, Epoch [30/100], Step [468/469], Positive Goodness: 0.9095, Negative Goodness: 0.0877\n",
            "Final Layer, Epoch [30/100], Step [468/469], Loss: 0.0017\n",
            "Epoch [30/100], Step [468/469], Loss: 0.2086, Test Accuracy: 0.9470\n",
            "Layer 0, Epoch [31/100], Step [117/469], Positive Goodness: 0.9514, Negative Goodness: 0.1288\n",
            "Layer 1, Epoch [31/100], Step [117/469], Positive Goodness: 0.9218, Negative Goodness: 0.0561\n",
            "Layer 2, Epoch [31/100], Step [117/469], Positive Goodness: 0.9286, Negative Goodness: 0.0488\n",
            "Layer 3, Epoch [31/100], Step [117/469], Positive Goodness: 0.9162, Negative Goodness: 0.0887\n",
            "Final Layer, Epoch [31/100], Step [117/469], Loss: 0.0016\n",
            "Epoch [31/100], Step [117/469], Loss: 0.2003, Test Accuracy: 0.9410\n",
            "Layer 0, Epoch [31/100], Step [234/469], Positive Goodness: 0.9525, Negative Goodness: 0.1274\n",
            "Layer 1, Epoch [31/100], Step [234/469], Positive Goodness: 0.9254, Negative Goodness: 0.0564\n",
            "Layer 2, Epoch [31/100], Step [234/469], Positive Goodness: 0.9354, Negative Goodness: 0.0494\n",
            "Layer 3, Epoch [31/100], Step [234/469], Positive Goodness: 0.9111, Negative Goodness: 0.0836\n",
            "Final Layer, Epoch [31/100], Step [234/469], Loss: 0.0014\n",
            "Epoch [31/100], Step [234/469], Loss: 0.2045, Test Accuracy: 0.9467\n",
            "Layer 0, Epoch [31/100], Step [351/469], Positive Goodness: 0.9504, Negative Goodness: 0.1272\n",
            "Layer 1, Epoch [31/100], Step [351/469], Positive Goodness: 0.9231, Negative Goodness: 0.0553\n",
            "Layer 2, Epoch [31/100], Step [351/469], Positive Goodness: 0.9326, Negative Goodness: 0.0528\n",
            "Layer 3, Epoch [31/100], Step [351/469], Positive Goodness: 0.9070, Negative Goodness: 0.0829\n",
            "Final Layer, Epoch [31/100], Step [351/469], Loss: 0.0019\n",
            "Epoch [31/100], Step [351/469], Loss: 0.1982, Test Accuracy: 0.9478\n",
            "Layer 0, Epoch [31/100], Step [468/469], Positive Goodness: 0.9488, Negative Goodness: 0.1300\n",
            "Layer 1, Epoch [31/100], Step [468/469], Positive Goodness: 0.9235, Negative Goodness: 0.0535\n",
            "Layer 2, Epoch [31/100], Step [468/469], Positive Goodness: 0.9243, Negative Goodness: 0.0491\n",
            "Layer 3, Epoch [31/100], Step [468/469], Positive Goodness: 0.9080, Negative Goodness: 0.0824\n",
            "Final Layer, Epoch [31/100], Step [468/469], Loss: 0.0022\n",
            "Epoch [31/100], Step [468/469], Loss: 0.2126, Test Accuracy: 0.9471\n",
            "Layer 0, Epoch [32/100], Step [117/469], Positive Goodness: 0.9525, Negative Goodness: 0.1206\n",
            "Layer 1, Epoch [32/100], Step [117/469], Positive Goodness: 0.9244, Negative Goodness: 0.0553\n",
            "Layer 2, Epoch [32/100], Step [117/469], Positive Goodness: 0.9340, Negative Goodness: 0.0550\n",
            "Layer 3, Epoch [32/100], Step [117/469], Positive Goodness: 0.9087, Negative Goodness: 0.0809\n",
            "Final Layer, Epoch [32/100], Step [117/469], Loss: 0.0019\n",
            "Epoch [32/100], Step [117/469], Loss: 0.2087, Test Accuracy: 0.9481\n",
            "Layer 0, Epoch [32/100], Step [234/469], Positive Goodness: 0.9525, Negative Goodness: 0.1259\n",
            "Layer 1, Epoch [32/100], Step [234/469], Positive Goodness: 0.9298, Negative Goodness: 0.0591\n",
            "Layer 2, Epoch [32/100], Step [234/469], Positive Goodness: 0.9330, Negative Goodness: 0.0570\n",
            "Layer 3, Epoch [32/100], Step [234/469], Positive Goodness: 0.9072, Negative Goodness: 0.0774\n",
            "Final Layer, Epoch [32/100], Step [234/469], Loss: 0.0014\n",
            "Epoch [32/100], Step [234/469], Loss: 0.2056, Test Accuracy: 0.9479\n",
            "Layer 0, Epoch [32/100], Step [351/469], Positive Goodness: 0.9521, Negative Goodness: 0.1180\n",
            "Layer 1, Epoch [32/100], Step [351/469], Positive Goodness: 0.9287, Negative Goodness: 0.0498\n",
            "Layer 2, Epoch [32/100], Step [351/469], Positive Goodness: 0.9384, Negative Goodness: 0.0515\n",
            "Layer 3, Epoch [32/100], Step [351/469], Positive Goodness: 0.9136, Negative Goodness: 0.0793\n",
            "Final Layer, Epoch [32/100], Step [351/469], Loss: 0.0011\n",
            "Epoch [32/100], Step [351/469], Loss: 0.2107, Test Accuracy: 0.9465\n",
            "Layer 0, Epoch [32/100], Step [468/469], Positive Goodness: 0.9537, Negative Goodness: 0.1182\n",
            "Layer 1, Epoch [32/100], Step [468/469], Positive Goodness: 0.9363, Negative Goodness: 0.0639\n",
            "Layer 2, Epoch [32/100], Step [468/469], Positive Goodness: 0.9341, Negative Goodness: 0.0515\n",
            "Layer 3, Epoch [32/100], Step [468/469], Positive Goodness: 0.9156, Negative Goodness: 0.0827\n",
            "Final Layer, Epoch [32/100], Step [468/469], Loss: 0.0020\n",
            "Epoch [32/100], Step [468/469], Loss: 0.2120, Test Accuracy: 0.9475\n",
            "Layer 0, Epoch [33/100], Step [117/469], Positive Goodness: 0.9544, Negative Goodness: 0.1148\n",
            "Layer 1, Epoch [33/100], Step [117/469], Positive Goodness: 0.9353, Negative Goodness: 0.0601\n",
            "Layer 2, Epoch [33/100], Step [117/469], Positive Goodness: 0.9377, Negative Goodness: 0.0449\n",
            "Layer 3, Epoch [33/100], Step [117/469], Positive Goodness: 0.9154, Negative Goodness: 0.0755\n",
            "Final Layer, Epoch [33/100], Step [117/469], Loss: 0.0012\n",
            "Epoch [33/100], Step [117/469], Loss: 0.2009, Test Accuracy: 0.9454\n",
            "Layer 0, Epoch [33/100], Step [234/469], Positive Goodness: 0.9502, Negative Goodness: 0.1182\n",
            "Layer 1, Epoch [33/100], Step [234/469], Positive Goodness: 0.9294, Negative Goodness: 0.0571\n",
            "Layer 2, Epoch [33/100], Step [234/469], Positive Goodness: 0.9304, Negative Goodness: 0.0463\n",
            "Layer 3, Epoch [33/100], Step [234/469], Positive Goodness: 0.9158, Negative Goodness: 0.0835\n",
            "Final Layer, Epoch [33/100], Step [234/469], Loss: 0.0014\n",
            "Epoch [33/100], Step [234/469], Loss: 0.1980, Test Accuracy: 0.9494\n",
            "Layer 0, Epoch [33/100], Step [351/469], Positive Goodness: 0.9565, Negative Goodness: 0.1183\n",
            "Layer 1, Epoch [33/100], Step [351/469], Positive Goodness: 0.9325, Negative Goodness: 0.0521\n",
            "Layer 2, Epoch [33/100], Step [351/469], Positive Goodness: 0.9406, Negative Goodness: 0.0511\n",
            "Layer 3, Epoch [33/100], Step [351/469], Positive Goodness: 0.9161, Negative Goodness: 0.0797\n",
            "Final Layer, Epoch [33/100], Step [351/469], Loss: 0.0019\n",
            "Epoch [33/100], Step [351/469], Loss: 0.2059, Test Accuracy: 0.9474\n",
            "Layer 0, Epoch [33/100], Step [468/469], Positive Goodness: 0.9546, Negative Goodness: 0.1100\n",
            "Layer 1, Epoch [33/100], Step [468/469], Positive Goodness: 0.9324, Negative Goodness: 0.0513\n",
            "Layer 2, Epoch [33/100], Step [468/469], Positive Goodness: 0.9430, Negative Goodness: 0.0550\n",
            "Layer 3, Epoch [33/100], Step [468/469], Positive Goodness: 0.9158, Negative Goodness: 0.0800\n",
            "Final Layer, Epoch [33/100], Step [468/469], Loss: 0.0011\n",
            "Epoch [33/100], Step [468/469], Loss: 0.2179, Test Accuracy: 0.9496\n",
            "Layer 0, Epoch [34/100], Step [117/469], Positive Goodness: 0.9578, Negative Goodness: 0.1066\n",
            "Layer 1, Epoch [34/100], Step [117/469], Positive Goodness: 0.9322, Negative Goodness: 0.0458\n",
            "Layer 2, Epoch [34/100], Step [117/469], Positive Goodness: 0.9477, Negative Goodness: 0.0543\n",
            "Layer 3, Epoch [34/100], Step [117/469], Positive Goodness: 0.9232, Negative Goodness: 0.0817\n",
            "Final Layer, Epoch [34/100], Step [117/469], Loss: 0.0012\n",
            "Epoch [34/100], Step [117/469], Loss: 0.2053, Test Accuracy: 0.9487\n",
            "Layer 0, Epoch [34/100], Step [234/469], Positive Goodness: 0.9554, Negative Goodness: 0.1135\n",
            "Layer 1, Epoch [34/100], Step [234/469], Positive Goodness: 0.9369, Negative Goodness: 0.0515\n",
            "Layer 2, Epoch [34/100], Step [234/469], Positive Goodness: 0.9391, Negative Goodness: 0.0502\n",
            "Layer 3, Epoch [34/100], Step [234/469], Positive Goodness: 0.9203, Negative Goodness: 0.0822\n",
            "Final Layer, Epoch [34/100], Step [234/469], Loss: 0.0016\n",
            "Epoch [34/100], Step [234/469], Loss: 0.1999, Test Accuracy: 0.9444\n",
            "Layer 0, Epoch [34/100], Step [351/469], Positive Goodness: 0.9548, Negative Goodness: 0.1036\n",
            "Layer 1, Epoch [34/100], Step [351/469], Positive Goodness: 0.9485, Negative Goodness: 0.0662\n",
            "Layer 2, Epoch [34/100], Step [351/469], Positive Goodness: 0.9401, Negative Goodness: 0.0471\n",
            "Layer 3, Epoch [34/100], Step [351/469], Positive Goodness: 0.9271, Negative Goodness: 0.0839\n",
            "Final Layer, Epoch [34/100], Step [351/469], Loss: 0.0009\n",
            "Epoch [34/100], Step [351/469], Loss: 0.2056, Test Accuracy: 0.9448\n",
            "Layer 0, Epoch [34/100], Step [468/469], Positive Goodness: 0.9582, Negative Goodness: 0.1045\n",
            "Layer 1, Epoch [34/100], Step [468/469], Positive Goodness: 0.9382, Negative Goodness: 0.0551\n",
            "Layer 2, Epoch [34/100], Step [468/469], Positive Goodness: 0.9393, Negative Goodness: 0.0473\n",
            "Layer 3, Epoch [34/100], Step [468/469], Positive Goodness: 0.9228, Negative Goodness: 0.0779\n",
            "Final Layer, Epoch [34/100], Step [468/469], Loss: 0.0010\n",
            "Epoch [34/100], Step [468/469], Loss: 0.1970, Test Accuracy: 0.9499\n",
            "Layer 0, Epoch [35/100], Step [117/469], Positive Goodness: 0.9587, Negative Goodness: 0.1018\n",
            "Layer 1, Epoch [35/100], Step [117/469], Positive Goodness: 0.9361, Negative Goodness: 0.0485\n",
            "Layer 2, Epoch [35/100], Step [117/469], Positive Goodness: 0.9458, Negative Goodness: 0.0464\n",
            "Layer 3, Epoch [35/100], Step [117/469], Positive Goodness: 0.9292, Negative Goodness: 0.0844\n",
            "Final Layer, Epoch [35/100], Step [117/469], Loss: 0.0011\n",
            "Epoch [35/100], Step [117/469], Loss: 0.2164, Test Accuracy: 0.9501\n",
            "Layer 0, Epoch [35/100], Step [234/469], Positive Goodness: 0.9589, Negative Goodness: 0.1040\n",
            "Layer 1, Epoch [35/100], Step [234/469], Positive Goodness: 0.9343, Negative Goodness: 0.0446\n",
            "Layer 2, Epoch [35/100], Step [234/469], Positive Goodness: 0.9435, Negative Goodness: 0.0466\n",
            "Layer 3, Epoch [35/100], Step [234/469], Positive Goodness: 0.9274, Negative Goodness: 0.0828\n",
            "Final Layer, Epoch [35/100], Step [234/469], Loss: 0.0014\n",
            "Epoch [35/100], Step [234/469], Loss: 0.2054, Test Accuracy: 0.9503\n",
            "Layer 0, Epoch [35/100], Step [351/469], Positive Goodness: 0.9573, Negative Goodness: 0.0981\n",
            "Layer 1, Epoch [35/100], Step [351/469], Positive Goodness: 0.9351, Negative Goodness: 0.0467\n",
            "Layer 2, Epoch [35/100], Step [351/469], Positive Goodness: 0.9434, Negative Goodness: 0.0448\n",
            "Layer 3, Epoch [35/100], Step [351/469], Positive Goodness: 0.9358, Negative Goodness: 0.0907\n",
            "Final Layer, Epoch [35/100], Step [351/469], Loss: 0.0009\n",
            "Epoch [35/100], Step [351/469], Loss: 0.2076, Test Accuracy: 0.9482\n",
            "Layer 0, Epoch [35/100], Step [468/469], Positive Goodness: 0.9579, Negative Goodness: 0.0967\n",
            "Layer 1, Epoch [35/100], Step [468/469], Positive Goodness: 0.9370, Negative Goodness: 0.0461\n",
            "Layer 2, Epoch [35/100], Step [468/469], Positive Goodness: 0.9380, Negative Goodness: 0.0428\n",
            "Layer 3, Epoch [35/100], Step [468/469], Positive Goodness: 0.9278, Negative Goodness: 0.0858\n",
            "Final Layer, Epoch [35/100], Step [468/469], Loss: 0.0010\n",
            "Epoch [35/100], Step [468/469], Loss: 0.2086, Test Accuracy: 0.9515\n",
            "Layer 0, Epoch [36/100], Step [117/469], Positive Goodness: 0.9591, Negative Goodness: 0.0975\n",
            "Layer 1, Epoch [36/100], Step [117/469], Positive Goodness: 0.9404, Negative Goodness: 0.0496\n",
            "Layer 2, Epoch [36/100], Step [117/469], Positive Goodness: 0.9411, Negative Goodness: 0.0468\n",
            "Layer 3, Epoch [36/100], Step [117/469], Positive Goodness: 0.9238, Negative Goodness: 0.0778\n",
            "Final Layer, Epoch [36/100], Step [117/469], Loss: 0.0010\n",
            "Epoch [36/100], Step [117/469], Loss: 0.1915, Test Accuracy: 0.9503\n",
            "Layer 0, Epoch [36/100], Step [234/469], Positive Goodness: 0.9583, Negative Goodness: 0.0986\n",
            "Layer 1, Epoch [36/100], Step [234/469], Positive Goodness: 0.9398, Negative Goodness: 0.0475\n",
            "Layer 2, Epoch [36/100], Step [234/469], Positive Goodness: 0.9390, Negative Goodness: 0.0421\n",
            "Layer 3, Epoch [36/100], Step [234/469], Positive Goodness: 0.9253, Negative Goodness: 0.0809\n",
            "Final Layer, Epoch [36/100], Step [234/469], Loss: 0.0010\n",
            "Epoch [36/100], Step [234/469], Loss: 0.2099, Test Accuracy: 0.9431\n",
            "Layer 0, Epoch [36/100], Step [351/469], Positive Goodness: 0.9566, Negative Goodness: 0.0952\n",
            "Layer 1, Epoch [36/100], Step [351/469], Positive Goodness: 0.9407, Negative Goodness: 0.0472\n",
            "Layer 2, Epoch [36/100], Step [351/469], Positive Goodness: 0.9479, Negative Goodness: 0.0455\n",
            "Layer 3, Epoch [36/100], Step [351/469], Positive Goodness: 0.9243, Negative Goodness: 0.0741\n",
            "Final Layer, Epoch [36/100], Step [351/469], Loss: 0.0009\n",
            "Epoch [36/100], Step [351/469], Loss: 0.2092, Test Accuracy: 0.9495\n",
            "Layer 0, Epoch [36/100], Step [468/469], Positive Goodness: 0.9603, Negative Goodness: 0.0882\n",
            "Layer 1, Epoch [36/100], Step [468/469], Positive Goodness: 0.9405, Negative Goodness: 0.0484\n",
            "Layer 2, Epoch [36/100], Step [468/469], Positive Goodness: 0.9493, Negative Goodness: 0.0438\n",
            "Layer 3, Epoch [36/100], Step [468/469], Positive Goodness: 0.9223, Negative Goodness: 0.0692\n",
            "Final Layer, Epoch [36/100], Step [468/469], Loss: 0.0013\n",
            "Epoch [36/100], Step [468/469], Loss: 0.2079, Test Accuracy: 0.9523\n",
            "Layer 0, Epoch [37/100], Step [117/469], Positive Goodness: 0.9590, Negative Goodness: 0.0842\n",
            "Layer 1, Epoch [37/100], Step [117/469], Positive Goodness: 0.9470, Negative Goodness: 0.0533\n",
            "Layer 2, Epoch [37/100], Step [117/469], Positive Goodness: 0.9440, Negative Goodness: 0.0418\n",
            "Layer 3, Epoch [37/100], Step [117/469], Positive Goodness: 0.9297, Negative Goodness: 0.0808\n",
            "Final Layer, Epoch [37/100], Step [117/469], Loss: 0.0012\n",
            "Epoch [37/100], Step [117/469], Loss: 0.2022, Test Accuracy: 0.9510\n",
            "Layer 0, Epoch [37/100], Step [234/469], Positive Goodness: 0.9615, Negative Goodness: 0.0843\n",
            "Layer 1, Epoch [37/100], Step [234/469], Positive Goodness: 0.9438, Negative Goodness: 0.0461\n",
            "Layer 2, Epoch [37/100], Step [234/469], Positive Goodness: 0.9477, Negative Goodness: 0.0416\n",
            "Layer 3, Epoch [37/100], Step [234/469], Positive Goodness: 0.9281, Negative Goodness: 0.0727\n",
            "Final Layer, Epoch [37/100], Step [234/469], Loss: 0.0006\n",
            "Epoch [37/100], Step [234/469], Loss: 0.2090, Test Accuracy: 0.9511\n",
            "Layer 0, Epoch [37/100], Step [351/469], Positive Goodness: 0.9615, Negative Goodness: 0.0906\n",
            "Layer 1, Epoch [37/100], Step [351/469], Positive Goodness: 0.9446, Negative Goodness: 0.0499\n",
            "Layer 2, Epoch [37/100], Step [351/469], Positive Goodness: 0.9435, Negative Goodness: 0.0402\n",
            "Layer 3, Epoch [37/100], Step [351/469], Positive Goodness: 0.9311, Negative Goodness: 0.0790\n",
            "Final Layer, Epoch [37/100], Step [351/469], Loss: 0.0010\n",
            "Epoch [37/100], Step [351/469], Loss: 0.1980, Test Accuracy: 0.9516\n",
            "Layer 0, Epoch [37/100], Step [468/469], Positive Goodness: 0.9602, Negative Goodness: 0.0903\n",
            "Layer 1, Epoch [37/100], Step [468/469], Positive Goodness: 0.9444, Negative Goodness: 0.0461\n",
            "Layer 2, Epoch [37/100], Step [468/469], Positive Goodness: 0.9480, Negative Goodness: 0.0441\n",
            "Layer 3, Epoch [37/100], Step [468/469], Positive Goodness: 0.9270, Negative Goodness: 0.0725\n",
            "Final Layer, Epoch [37/100], Step [468/469], Loss: 0.0009\n",
            "Epoch [37/100], Step [468/469], Loss: 0.2010, Test Accuracy: 0.9509\n",
            "Layer 0, Epoch [38/100], Step [117/469], Positive Goodness: 0.9630, Negative Goodness: 0.0817\n",
            "Layer 1, Epoch [38/100], Step [117/469], Positive Goodness: 0.9427, Negative Goodness: 0.0407\n",
            "Layer 2, Epoch [38/100], Step [117/469], Positive Goodness: 0.9539, Negative Goodness: 0.0443\n",
            "Layer 3, Epoch [38/100], Step [117/469], Positive Goodness: 0.9376, Negative Goodness: 0.0790\n",
            "Final Layer, Epoch [38/100], Step [117/469], Loss: 0.0010\n",
            "Epoch [38/100], Step [117/469], Loss: 0.1977, Test Accuracy: 0.9524\n",
            "Layer 0, Epoch [38/100], Step [234/469], Positive Goodness: 0.9617, Negative Goodness: 0.0843\n",
            "Layer 1, Epoch [38/100], Step [234/469], Positive Goodness: 0.9457, Negative Goodness: 0.0483\n",
            "Layer 2, Epoch [38/100], Step [234/469], Positive Goodness: 0.9449, Negative Goodness: 0.0412\n",
            "Layer 3, Epoch [38/100], Step [234/469], Positive Goodness: 0.9268, Negative Goodness: 0.0715\n",
            "Final Layer, Epoch [38/100], Step [234/469], Loss: 0.0012\n",
            "Epoch [38/100], Step [234/469], Loss: 0.2193, Test Accuracy: 0.9526\n",
            "Layer 0, Epoch [38/100], Step [351/469], Positive Goodness: 0.9625, Negative Goodness: 0.0842\n",
            "Layer 1, Epoch [38/100], Step [351/469], Positive Goodness: 0.9420, Negative Goodness: 0.0407\n",
            "Layer 2, Epoch [38/100], Step [351/469], Positive Goodness: 0.9499, Negative Goodness: 0.0458\n",
            "Layer 3, Epoch [38/100], Step [351/469], Positive Goodness: 0.9301, Negative Goodness: 0.0707\n",
            "Final Layer, Epoch [38/100], Step [351/469], Loss: 0.0006\n",
            "Epoch [38/100], Step [351/469], Loss: 0.2094, Test Accuracy: 0.9522\n",
            "Layer 0, Epoch [38/100], Step [468/469], Positive Goodness: 0.9601, Negative Goodness: 0.0817\n",
            "Layer 1, Epoch [38/100], Step [468/469], Positive Goodness: 0.9482, Negative Goodness: 0.0491\n",
            "Layer 2, Epoch [38/100], Step [468/469], Positive Goodness: 0.9488, Negative Goodness: 0.0433\n",
            "Layer 3, Epoch [38/100], Step [468/469], Positive Goodness: 0.9337, Negative Goodness: 0.0765\n",
            "Final Layer, Epoch [38/100], Step [468/469], Loss: 0.0007\n",
            "Epoch [38/100], Step [468/469], Loss: 0.2088, Test Accuracy: 0.9538\n",
            "Layer 0, Epoch [39/100], Step [117/469], Positive Goodness: 0.9617, Negative Goodness: 0.0848\n",
            "Layer 1, Epoch [39/100], Step [117/469], Positive Goodness: 0.9490, Negative Goodness: 0.0495\n",
            "Layer 2, Epoch [39/100], Step [117/469], Positive Goodness: 0.9470, Negative Goodness: 0.0392\n",
            "Layer 3, Epoch [39/100], Step [117/469], Positive Goodness: 0.9363, Negative Goodness: 0.0759\n",
            "Final Layer, Epoch [39/100], Step [117/469], Loss: 0.0010\n",
            "Epoch [39/100], Step [117/469], Loss: 0.2081, Test Accuracy: 0.9500\n",
            "Layer 0, Epoch [39/100], Step [234/469], Positive Goodness: 0.9635, Negative Goodness: 0.0864\n",
            "Layer 1, Epoch [39/100], Step [234/469], Positive Goodness: 0.9482, Negative Goodness: 0.0483\n",
            "Layer 2, Epoch [39/100], Step [234/469], Positive Goodness: 0.9505, Negative Goodness: 0.0429\n",
            "Layer 3, Epoch [39/100], Step [234/469], Positive Goodness: 0.9343, Negative Goodness: 0.0754\n",
            "Final Layer, Epoch [39/100], Step [234/469], Loss: 0.0006\n",
            "Epoch [39/100], Step [234/469], Loss: 0.2208, Test Accuracy: 0.9511\n",
            "Layer 0, Epoch [39/100], Step [351/469], Positive Goodness: 0.9636, Negative Goodness: 0.0825\n",
            "Layer 1, Epoch [39/100], Step [351/469], Positive Goodness: 0.9505, Negative Goodness: 0.0473\n",
            "Layer 2, Epoch [39/100], Step [351/469], Positive Goodness: 0.9524, Negative Goodness: 0.0444\n",
            "Layer 3, Epoch [39/100], Step [351/469], Positive Goodness: 0.9362, Negative Goodness: 0.0768\n",
            "Final Layer, Epoch [39/100], Step [351/469], Loss: 0.0005\n",
            "Epoch [39/100], Step [351/469], Loss: 0.2070, Test Accuracy: 0.9543\n",
            "Layer 0, Epoch [39/100], Step [468/469], Positive Goodness: 0.9632, Negative Goodness: 0.0825\n",
            "Layer 1, Epoch [39/100], Step [468/469], Positive Goodness: 0.9523, Negative Goodness: 0.0505\n",
            "Layer 2, Epoch [39/100], Step [468/469], Positive Goodness: 0.9521, Negative Goodness: 0.0437\n",
            "Layer 3, Epoch [39/100], Step [468/469], Positive Goodness: 0.9307, Negative Goodness: 0.0687\n",
            "Final Layer, Epoch [39/100], Step [468/469], Loss: 0.0009\n",
            "Epoch [39/100], Step [468/469], Loss: 0.2069, Test Accuracy: 0.9527\n",
            "Layer 0, Epoch [40/100], Step [117/469], Positive Goodness: 0.9649, Negative Goodness: 0.0816\n",
            "Layer 1, Epoch [40/100], Step [117/469], Positive Goodness: 0.9459, Negative Goodness: 0.0449\n",
            "Layer 2, Epoch [40/100], Step [117/469], Positive Goodness: 0.9495, Negative Goodness: 0.0387\n",
            "Layer 3, Epoch [40/100], Step [117/469], Positive Goodness: 0.9351, Negative Goodness: 0.0704\n",
            "Final Layer, Epoch [40/100], Step [117/469], Loss: 0.0005\n",
            "Epoch [40/100], Step [117/469], Loss: 0.2094, Test Accuracy: 0.9515\n",
            "Layer 0, Epoch [40/100], Step [234/469], Positive Goodness: 0.9622, Negative Goodness: 0.0757\n",
            "Layer 1, Epoch [40/100], Step [234/469], Positive Goodness: 0.9469, Negative Goodness: 0.0459\n",
            "Layer 2, Epoch [40/100], Step [234/469], Positive Goodness: 0.9523, Negative Goodness: 0.0458\n",
            "Layer 3, Epoch [40/100], Step [234/469], Positive Goodness: 0.9318, Negative Goodness: 0.0700\n",
            "Final Layer, Epoch [40/100], Step [234/469], Loss: 0.0009\n",
            "Epoch [40/100], Step [234/469], Loss: 0.2073, Test Accuracy: 0.9543\n",
            "Layer 0, Epoch [40/100], Step [351/469], Positive Goodness: 0.9623, Negative Goodness: 0.0834\n",
            "Layer 1, Epoch [40/100], Step [351/469], Positive Goodness: 0.9415, Negative Goodness: 0.0374\n",
            "Layer 2, Epoch [40/100], Step [351/469], Positive Goodness: 0.9498, Negative Goodness: 0.0448\n",
            "Layer 3, Epoch [40/100], Step [351/469], Positive Goodness: 0.9412, Negative Goodness: 0.0755\n",
            "Final Layer, Epoch [40/100], Step [351/469], Loss: 0.0006\n",
            "Epoch [40/100], Step [351/469], Loss: 0.2190, Test Accuracy: 0.9526\n",
            "Layer 0, Epoch [40/100], Step [468/469], Positive Goodness: 0.9649, Negative Goodness: 0.0796\n",
            "Layer 1, Epoch [40/100], Step [468/469], Positive Goodness: 0.9488, Negative Goodness: 0.0410\n",
            "Layer 2, Epoch [40/100], Step [468/469], Positive Goodness: 0.9529, Negative Goodness: 0.0376\n",
            "Layer 3, Epoch [40/100], Step [468/469], Positive Goodness: 0.9330, Negative Goodness: 0.0631\n",
            "Final Layer, Epoch [40/100], Step [468/469], Loss: 0.0006\n",
            "Epoch [40/100], Step [468/469], Loss: 0.1988, Test Accuracy: 0.9523\n",
            "Layer 0, Epoch [41/100], Step [117/469], Positive Goodness: 0.9614, Negative Goodness: 0.0784\n",
            "Layer 1, Epoch [41/100], Step [117/469], Positive Goodness: 0.9470, Negative Goodness: 0.0409\n",
            "Layer 2, Epoch [41/100], Step [117/469], Positive Goodness: 0.9470, Negative Goodness: 0.0375\n",
            "Layer 3, Epoch [41/100], Step [117/469], Positive Goodness: 0.9315, Negative Goodness: 0.0715\n",
            "Final Layer, Epoch [41/100], Step [117/469], Loss: 0.0006\n",
            "Epoch [41/100], Step [117/469], Loss: 0.2115, Test Accuracy: 0.9538\n",
            "Layer 0, Epoch [41/100], Step [234/469], Positive Goodness: 0.9646, Negative Goodness: 0.0781\n",
            "Layer 1, Epoch [41/100], Step [234/469], Positive Goodness: 0.9491, Negative Goodness: 0.0423\n",
            "Layer 2, Epoch [41/100], Step [234/469], Positive Goodness: 0.9566, Negative Goodness: 0.0504\n",
            "Layer 3, Epoch [41/100], Step [234/469], Positive Goodness: 0.9404, Negative Goodness: 0.0739\n",
            "Final Layer, Epoch [41/100], Step [234/469], Loss: 0.0005\n",
            "Epoch [41/100], Step [234/469], Loss: 0.2130, Test Accuracy: 0.9482\n",
            "Layer 0, Epoch [41/100], Step [351/469], Positive Goodness: 0.9663, Negative Goodness: 0.0735\n",
            "Layer 1, Epoch [41/100], Step [351/469], Positive Goodness: 0.9475, Negative Goodness: 0.0373\n",
            "Layer 2, Epoch [41/100], Step [351/469], Positive Goodness: 0.9576, Negative Goodness: 0.0463\n",
            "Layer 3, Epoch [41/100], Step [351/469], Positive Goodness: 0.9408, Negative Goodness: 0.0688\n",
            "Final Layer, Epoch [41/100], Step [351/469], Loss: 0.0007\n",
            "Epoch [41/100], Step [351/469], Loss: 0.2042, Test Accuracy: 0.9527\n",
            "Layer 0, Epoch [41/100], Step [468/469], Positive Goodness: 0.9637, Negative Goodness: 0.0775\n",
            "Layer 1, Epoch [41/100], Step [468/469], Positive Goodness: 0.9441, Negative Goodness: 0.0426\n",
            "Layer 2, Epoch [41/100], Step [468/469], Positive Goodness: 0.9573, Negative Goodness: 0.0451\n",
            "Layer 3, Epoch [41/100], Step [468/469], Positive Goodness: 0.9357, Negative Goodness: 0.0680\n",
            "Final Layer, Epoch [41/100], Step [468/469], Loss: 0.0008\n",
            "Epoch [41/100], Step [468/469], Loss: 0.2040, Test Accuracy: 0.9538\n",
            "Layer 0, Epoch [42/100], Step [117/469], Positive Goodness: 0.9645, Negative Goodness: 0.0712\n",
            "Layer 1, Epoch [42/100], Step [117/469], Positive Goodness: 0.9517, Negative Goodness: 0.0382\n",
            "Layer 2, Epoch [42/100], Step [117/469], Positive Goodness: 0.9585, Negative Goodness: 0.0425\n",
            "Layer 3, Epoch [42/100], Step [117/469], Positive Goodness: 0.9396, Negative Goodness: 0.0657\n",
            "Final Layer, Epoch [42/100], Step [117/469], Loss: 0.0006\n",
            "Epoch [42/100], Step [117/469], Loss: 0.2000, Test Accuracy: 0.9553\n",
            "Layer 0, Epoch [42/100], Step [234/469], Positive Goodness: 0.9637, Negative Goodness: 0.0730\n",
            "Layer 1, Epoch [42/100], Step [234/469], Positive Goodness: 0.9465, Negative Goodness: 0.0376\n",
            "Layer 2, Epoch [42/100], Step [234/469], Positive Goodness: 0.9570, Negative Goodness: 0.0408\n",
            "Layer 3, Epoch [42/100], Step [234/469], Positive Goodness: 0.9414, Negative Goodness: 0.0701\n",
            "Final Layer, Epoch [42/100], Step [234/469], Loss: 0.0007\n",
            "Epoch [42/100], Step [234/469], Loss: 0.2054, Test Accuracy: 0.9539\n",
            "Layer 0, Epoch [42/100], Step [351/469], Positive Goodness: 0.9660, Negative Goodness: 0.0791\n",
            "Layer 1, Epoch [42/100], Step [351/469], Positive Goodness: 0.9491, Negative Goodness: 0.0417\n",
            "Layer 2, Epoch [42/100], Step [351/469], Positive Goodness: 0.9549, Negative Goodness: 0.0448\n",
            "Layer 3, Epoch [42/100], Step [351/469], Positive Goodness: 0.9388, Negative Goodness: 0.0689\n",
            "Final Layer, Epoch [42/100], Step [351/469], Loss: 0.0005\n",
            "Epoch [42/100], Step [351/469], Loss: 0.2014, Test Accuracy: 0.9537\n",
            "Layer 0, Epoch [42/100], Step [468/469], Positive Goodness: 0.9655, Negative Goodness: 0.0791\n",
            "Layer 1, Epoch [42/100], Step [468/469], Positive Goodness: 0.9526, Negative Goodness: 0.0439\n",
            "Layer 2, Epoch [42/100], Step [468/469], Positive Goodness: 0.9570, Negative Goodness: 0.0411\n",
            "Layer 3, Epoch [42/100], Step [468/469], Positive Goodness: 0.9391, Negative Goodness: 0.0710\n",
            "Final Layer, Epoch [42/100], Step [468/469], Loss: 0.0005\n",
            "Epoch [42/100], Step [468/469], Loss: 0.1992, Test Accuracy: 0.9568\n",
            "Layer 0, Epoch [43/100], Step [117/469], Positive Goodness: 0.9672, Negative Goodness: 0.0705\n",
            "Layer 1, Epoch [43/100], Step [117/469], Positive Goodness: 0.9534, Negative Goodness: 0.0432\n",
            "Layer 2, Epoch [43/100], Step [117/469], Positive Goodness: 0.9538, Negative Goodness: 0.0349\n",
            "Layer 3, Epoch [43/100], Step [117/469], Positive Goodness: 0.9457, Negative Goodness: 0.0719\n",
            "Final Layer, Epoch [43/100], Step [117/469], Loss: 0.0006\n",
            "Epoch [43/100], Step [117/469], Loss: 0.1976, Test Accuracy: 0.9565\n",
            "Layer 0, Epoch [43/100], Step [234/469], Positive Goodness: 0.9673, Negative Goodness: 0.0741\n",
            "Layer 1, Epoch [43/100], Step [234/469], Positive Goodness: 0.9523, Negative Goodness: 0.0379\n",
            "Layer 2, Epoch [43/100], Step [234/469], Positive Goodness: 0.9596, Negative Goodness: 0.0415\n",
            "Layer 3, Epoch [43/100], Step [234/469], Positive Goodness: 0.9410, Negative Goodness: 0.0676\n",
            "Final Layer, Epoch [43/100], Step [234/469], Loss: 0.0004\n",
            "Epoch [43/100], Step [234/469], Loss: 0.2050, Test Accuracy: 0.9564\n",
            "Layer 0, Epoch [43/100], Step [351/469], Positive Goodness: 0.9652, Negative Goodness: 0.0731\n",
            "Layer 1, Epoch [43/100], Step [351/469], Positive Goodness: 0.9547, Negative Goodness: 0.0477\n",
            "Layer 2, Epoch [43/100], Step [351/469], Positive Goodness: 0.9546, Negative Goodness: 0.0372\n",
            "Layer 3, Epoch [43/100], Step [351/469], Positive Goodness: 0.9437, Negative Goodness: 0.0676\n",
            "Final Layer, Epoch [43/100], Step [351/469], Loss: 0.0005\n",
            "Epoch [43/100], Step [351/469], Loss: 0.1956, Test Accuracy: 0.9548\n",
            "Layer 0, Epoch [43/100], Step [468/469], Positive Goodness: 0.9630, Negative Goodness: 0.0725\n",
            "Layer 1, Epoch [43/100], Step [468/469], Positive Goodness: 0.9532, Negative Goodness: 0.0388\n",
            "Layer 2, Epoch [43/100], Step [468/469], Positive Goodness: 0.9570, Negative Goodness: 0.0376\n",
            "Layer 3, Epoch [43/100], Step [468/469], Positive Goodness: 0.9424, Negative Goodness: 0.0666\n",
            "Final Layer, Epoch [43/100], Step [468/469], Loss: 0.0006\n",
            "Epoch [43/100], Step [468/469], Loss: 0.2000, Test Accuracy: 0.9552\n",
            "Layer 0, Epoch [44/100], Step [117/469], Positive Goodness: 0.9665, Negative Goodness: 0.0706\n",
            "Layer 1, Epoch [44/100], Step [117/469], Positive Goodness: 0.9504, Negative Goodness: 0.0363\n",
            "Layer 2, Epoch [44/100], Step [117/469], Positive Goodness: 0.9572, Negative Goodness: 0.0374\n",
            "Layer 3, Epoch [44/100], Step [117/469], Positive Goodness: 0.9437, Negative Goodness: 0.0692\n",
            "Final Layer, Epoch [44/100], Step [117/469], Loss: 0.0006\n",
            "Epoch [44/100], Step [117/469], Loss: 0.2011, Test Accuracy: 0.9548\n",
            "Layer 0, Epoch [44/100], Step [234/469], Positive Goodness: 0.9652, Negative Goodness: 0.0711\n",
            "Layer 1, Epoch [44/100], Step [234/469], Positive Goodness: 0.9517, Negative Goodness: 0.0386\n",
            "Layer 2, Epoch [44/100], Step [234/469], Positive Goodness: 0.9622, Negative Goodness: 0.0461\n",
            "Layer 3, Epoch [44/100], Step [234/469], Positive Goodness: 0.9401, Negative Goodness: 0.0650\n",
            "Final Layer, Epoch [44/100], Step [234/469], Loss: 0.0006\n",
            "Epoch [44/100], Step [234/469], Loss: 0.1904, Test Accuracy: 0.9559\n",
            "Layer 0, Epoch [44/100], Step [351/469], Positive Goodness: 0.9652, Negative Goodness: 0.0739\n",
            "Layer 1, Epoch [44/100], Step [351/469], Positive Goodness: 0.9493, Negative Goodness: 0.0354\n",
            "Layer 2, Epoch [44/100], Step [351/469], Positive Goodness: 0.9533, Negative Goodness: 0.0340\n",
            "Layer 3, Epoch [44/100], Step [351/469], Positive Goodness: 0.9369, Negative Goodness: 0.0645\n",
            "Final Layer, Epoch [44/100], Step [351/469], Loss: 0.0004\n",
            "Epoch [44/100], Step [351/469], Loss: 0.1878, Test Accuracy: 0.9542\n",
            "Layer 0, Epoch [44/100], Step [468/469], Positive Goodness: 0.9683, Negative Goodness: 0.0678\n",
            "Layer 1, Epoch [44/100], Step [468/469], Positive Goodness: 0.9550, Negative Goodness: 0.0377\n",
            "Layer 2, Epoch [44/100], Step [468/469], Positive Goodness: 0.9617, Negative Goodness: 0.0397\n",
            "Layer 3, Epoch [44/100], Step [468/469], Positive Goodness: 0.9425, Negative Goodness: 0.0616\n",
            "Final Layer, Epoch [44/100], Step [468/469], Loss: 0.0004\n",
            "Epoch [44/100], Step [468/469], Loss: 0.1958, Test Accuracy: 0.9550\n",
            "Layer 0, Epoch [45/100], Step [117/469], Positive Goodness: 0.9664, Negative Goodness: 0.0699\n",
            "Layer 1, Epoch [45/100], Step [117/469], Positive Goodness: 0.9582, Negative Goodness: 0.0427\n",
            "Layer 2, Epoch [45/100], Step [117/469], Positive Goodness: 0.9596, Negative Goodness: 0.0392\n",
            "Layer 3, Epoch [45/100], Step [117/469], Positive Goodness: 0.9425, Negative Goodness: 0.0684\n",
            "Final Layer, Epoch [45/100], Step [117/469], Loss: 0.0005\n",
            "Epoch [45/100], Step [117/469], Loss: 0.1931, Test Accuracy: 0.9558\n",
            "Layer 0, Epoch [45/100], Step [234/469], Positive Goodness: 0.9676, Negative Goodness: 0.0697\n",
            "Layer 1, Epoch [45/100], Step [234/469], Positive Goodness: 0.9520, Negative Goodness: 0.0333\n",
            "Layer 2, Epoch [45/100], Step [234/469], Positive Goodness: 0.9567, Negative Goodness: 0.0332\n",
            "Layer 3, Epoch [45/100], Step [234/469], Positive Goodness: 0.9446, Negative Goodness: 0.0631\n",
            "Final Layer, Epoch [45/100], Step [234/469], Loss: 0.0005\n",
            "Epoch [45/100], Step [234/469], Loss: 0.2033, Test Accuracy: 0.9563\n",
            "Layer 0, Epoch [45/100], Step [351/469], Positive Goodness: 0.9674, Negative Goodness: 0.0677\n",
            "Layer 1, Epoch [45/100], Step [351/469], Positive Goodness: 0.9537, Negative Goodness: 0.0342\n",
            "Layer 2, Epoch [45/100], Step [351/469], Positive Goodness: 0.9600, Negative Goodness: 0.0355\n",
            "Layer 3, Epoch [45/100], Step [351/469], Positive Goodness: 0.9479, Negative Goodness: 0.0698\n",
            "Final Layer, Epoch [45/100], Step [351/469], Loss: 0.0003\n",
            "Epoch [45/100], Step [351/469], Loss: 0.1966, Test Accuracy: 0.9555\n",
            "Layer 0, Epoch [45/100], Step [468/469], Positive Goodness: 0.9646, Negative Goodness: 0.0694\n",
            "Layer 1, Epoch [45/100], Step [468/469], Positive Goodness: 0.9528, Negative Goodness: 0.0389\n",
            "Layer 2, Epoch [45/100], Step [468/469], Positive Goodness: 0.9592, Negative Goodness: 0.0390\n",
            "Layer 3, Epoch [45/100], Step [468/469], Positive Goodness: 0.9434, Negative Goodness: 0.0638\n",
            "Final Layer, Epoch [45/100], Step [468/469], Loss: 0.0007\n",
            "Epoch [45/100], Step [468/469], Loss: 0.2008, Test Accuracy: 0.9580\n",
            "Layer 0, Epoch [46/100], Step [117/469], Positive Goodness: 0.9661, Negative Goodness: 0.0654\n",
            "Layer 1, Epoch [46/100], Step [117/469], Positive Goodness: 0.9525, Negative Goodness: 0.0315\n",
            "Layer 2, Epoch [46/100], Step [117/469], Positive Goodness: 0.9596, Negative Goodness: 0.0329\n",
            "Layer 3, Epoch [46/100], Step [117/469], Positive Goodness: 0.9479, Negative Goodness: 0.0660\n",
            "Final Layer, Epoch [46/100], Step [117/469], Loss: 0.0004\n",
            "Epoch [46/100], Step [117/469], Loss: 0.1937, Test Accuracy: 0.9562\n",
            "Layer 0, Epoch [46/100], Step [234/469], Positive Goodness: 0.9682, Negative Goodness: 0.0670\n",
            "Layer 1, Epoch [46/100], Step [234/469], Positive Goodness: 0.9597, Negative Goodness: 0.0375\n",
            "Layer 2, Epoch [46/100], Step [234/469], Positive Goodness: 0.9622, Negative Goodness: 0.0370\n",
            "Layer 3, Epoch [46/100], Step [234/469], Positive Goodness: 0.9482, Negative Goodness: 0.0698\n",
            "Final Layer, Epoch [46/100], Step [234/469], Loss: 0.0004\n",
            "Epoch [46/100], Step [234/469], Loss: 0.1939, Test Accuracy: 0.9579\n",
            "Layer 0, Epoch [46/100], Step [351/469], Positive Goodness: 0.9699, Negative Goodness: 0.0678\n",
            "Layer 1, Epoch [46/100], Step [351/469], Positive Goodness: 0.9579, Negative Goodness: 0.0362\n",
            "Layer 2, Epoch [46/100], Step [351/469], Positive Goodness: 0.9632, Negative Goodness: 0.0380\n",
            "Layer 3, Epoch [46/100], Step [351/469], Positive Goodness: 0.9453, Negative Goodness: 0.0667\n",
            "Final Layer, Epoch [46/100], Step [351/469], Loss: 0.0004\n",
            "Epoch [46/100], Step [351/469], Loss: 0.2002, Test Accuracy: 0.9569\n",
            "Layer 0, Epoch [46/100], Step [468/469], Positive Goodness: 0.9661, Negative Goodness: 0.0650\n",
            "Layer 1, Epoch [46/100], Step [468/469], Positive Goodness: 0.9586, Negative Goodness: 0.0388\n",
            "Layer 2, Epoch [46/100], Step [468/469], Positive Goodness: 0.9606, Negative Goodness: 0.0371\n",
            "Layer 3, Epoch [46/100], Step [468/469], Positive Goodness: 0.9438, Negative Goodness: 0.0693\n",
            "Final Layer, Epoch [46/100], Step [468/469], Loss: 0.0006\n",
            "Epoch [46/100], Step [468/469], Loss: 0.2095, Test Accuracy: 0.9553\n",
            "Layer 0, Epoch [47/100], Step [117/469], Positive Goodness: 0.9666, Negative Goodness: 0.0669\n",
            "Layer 1, Epoch [47/100], Step [117/469], Positive Goodness: 0.9562, Negative Goodness: 0.0355\n",
            "Layer 2, Epoch [47/100], Step [117/469], Positive Goodness: 0.9601, Negative Goodness: 0.0383\n",
            "Layer 3, Epoch [47/100], Step [117/469], Positive Goodness: 0.9433, Negative Goodness: 0.0668\n",
            "Final Layer, Epoch [47/100], Step [117/469], Loss: 0.0004\n",
            "Epoch [47/100], Step [117/469], Loss: 0.1994, Test Accuracy: 0.9578\n",
            "Layer 0, Epoch [47/100], Step [234/469], Positive Goodness: 0.9693, Negative Goodness: 0.0606\n",
            "Layer 1, Epoch [47/100], Step [234/469], Positive Goodness: 0.9554, Negative Goodness: 0.0376\n",
            "Layer 2, Epoch [47/100], Step [234/469], Positive Goodness: 0.9618, Negative Goodness: 0.0392\n",
            "Layer 3, Epoch [47/100], Step [234/469], Positive Goodness: 0.9462, Negative Goodness: 0.0642\n",
            "Final Layer, Epoch [47/100], Step [234/469], Loss: 0.0004\n",
            "Epoch [47/100], Step [234/469], Loss: 0.2079, Test Accuracy: 0.9574\n",
            "Layer 0, Epoch [47/100], Step [351/469], Positive Goodness: 0.9680, Negative Goodness: 0.0624\n",
            "Layer 1, Epoch [47/100], Step [351/469], Positive Goodness: 0.9562, Negative Goodness: 0.0355\n",
            "Layer 2, Epoch [47/100], Step [351/469], Positive Goodness: 0.9641, Negative Goodness: 0.0383\n",
            "Layer 3, Epoch [47/100], Step [351/469], Positive Goodness: 0.9505, Negative Goodness: 0.0664\n",
            "Final Layer, Epoch [47/100], Step [351/469], Loss: 0.0005\n",
            "Epoch [47/100], Step [351/469], Loss: 0.2099, Test Accuracy: 0.9567\n",
            "Layer 0, Epoch [47/100], Step [468/469], Positive Goodness: 0.9662, Negative Goodness: 0.0578\n",
            "Layer 1, Epoch [47/100], Step [468/469], Positive Goodness: 0.9543, Negative Goodness: 0.0323\n",
            "Layer 2, Epoch [47/100], Step [468/469], Positive Goodness: 0.9633, Negative Goodness: 0.0396\n",
            "Layer 3, Epoch [47/100], Step [468/469], Positive Goodness: 0.9486, Negative Goodness: 0.0607\n",
            "Final Layer, Epoch [47/100], Step [468/469], Loss: 0.0004\n",
            "Epoch [47/100], Step [468/469], Loss: 0.2046, Test Accuracy: 0.9572\n",
            "Layer 0, Epoch [48/100], Step [117/469], Positive Goodness: 0.9692, Negative Goodness: 0.0609\n",
            "Layer 1, Epoch [48/100], Step [117/469], Positive Goodness: 0.9560, Negative Goodness: 0.0332\n",
            "Layer 2, Epoch [48/100], Step [117/469], Positive Goodness: 0.9617, Negative Goodness: 0.0346\n",
            "Layer 3, Epoch [48/100], Step [117/469], Positive Goodness: 0.9466, Negative Goodness: 0.0612\n",
            "Final Layer, Epoch [48/100], Step [117/469], Loss: 0.0004\n",
            "Epoch [48/100], Step [117/469], Loss: 0.1981, Test Accuracy: 0.9574\n",
            "Layer 0, Epoch [48/100], Step [234/469], Positive Goodness: 0.9669, Negative Goodness: 0.0607\n",
            "Layer 1, Epoch [48/100], Step [234/469], Positive Goodness: 0.9603, Negative Goodness: 0.0374\n",
            "Layer 2, Epoch [48/100], Step [234/469], Positive Goodness: 0.9605, Negative Goodness: 0.0353\n",
            "Layer 3, Epoch [48/100], Step [234/469], Positive Goodness: 0.9467, Negative Goodness: 0.0628\n",
            "Final Layer, Epoch [48/100], Step [234/469], Loss: 0.0006\n",
            "Epoch [48/100], Step [234/469], Loss: 0.2115, Test Accuracy: 0.9562\n",
            "Layer 0, Epoch [48/100], Step [351/469], Positive Goodness: 0.9681, Negative Goodness: 0.0604\n",
            "Layer 1, Epoch [48/100], Step [351/469], Positive Goodness: 0.9546, Negative Goodness: 0.0332\n",
            "Layer 2, Epoch [48/100], Step [351/469], Positive Goodness: 0.9635, Negative Goodness: 0.0411\n",
            "Layer 3, Epoch [48/100], Step [351/469], Positive Goodness: 0.9436, Negative Goodness: 0.0629\n",
            "Final Layer, Epoch [48/100], Step [351/469], Loss: 0.0003\n",
            "Epoch [48/100], Step [351/469], Loss: 0.2004, Test Accuracy: 0.9563\n",
            "Layer 0, Epoch [48/100], Step [468/469], Positive Goodness: 0.9657, Negative Goodness: 0.0586\n",
            "Layer 1, Epoch [48/100], Step [468/469], Positive Goodness: 0.9576, Negative Goodness: 0.0346\n",
            "Layer 2, Epoch [48/100], Step [468/469], Positive Goodness: 0.9630, Negative Goodness: 0.0374\n",
            "Layer 3, Epoch [48/100], Step [468/469], Positive Goodness: 0.9497, Negative Goodness: 0.0636\n",
            "Final Layer, Epoch [48/100], Step [468/469], Loss: 0.0004\n",
            "Epoch [48/100], Step [468/469], Loss: 0.2038, Test Accuracy: 0.9550\n",
            "Layer 0, Epoch [49/100], Step [117/469], Positive Goodness: 0.9681, Negative Goodness: 0.0537\n",
            "Layer 1, Epoch [49/100], Step [117/469], Positive Goodness: 0.9602, Negative Goodness: 0.0368\n",
            "Layer 2, Epoch [49/100], Step [117/469], Positive Goodness: 0.9615, Negative Goodness: 0.0318\n",
            "Layer 3, Epoch [49/100], Step [117/469], Positive Goodness: 0.9512, Negative Goodness: 0.0651\n",
            "Final Layer, Epoch [49/100], Step [117/469], Loss: 0.0004\n",
            "Epoch [49/100], Step [117/469], Loss: 0.1985, Test Accuracy: 0.9581\n",
            "Layer 0, Epoch [49/100], Step [234/469], Positive Goodness: 0.9685, Negative Goodness: 0.0517\n",
            "Layer 1, Epoch [49/100], Step [234/469], Positive Goodness: 0.9608, Negative Goodness: 0.0372\n",
            "Layer 2, Epoch [49/100], Step [234/469], Positive Goodness: 0.9634, Negative Goodness: 0.0347\n",
            "Layer 3, Epoch [49/100], Step [234/469], Positive Goodness: 0.9460, Negative Goodness: 0.0625\n",
            "Final Layer, Epoch [49/100], Step [234/469], Loss: 0.0005\n",
            "Epoch [49/100], Step [234/469], Loss: 0.2049, Test Accuracy: 0.9533\n",
            "Layer 0, Epoch [49/100], Step [351/469], Positive Goodness: 0.9680, Negative Goodness: 0.0549\n",
            "Layer 1, Epoch [49/100], Step [351/469], Positive Goodness: 0.9588, Negative Goodness: 0.0368\n",
            "Layer 2, Epoch [49/100], Step [351/469], Positive Goodness: 0.9593, Negative Goodness: 0.0335\n",
            "Layer 3, Epoch [49/100], Step [351/469], Positive Goodness: 0.9488, Negative Goodness: 0.0675\n",
            "Final Layer, Epoch [49/100], Step [351/469], Loss: 0.0003\n",
            "Epoch [49/100], Step [351/469], Loss: 0.2080, Test Accuracy: 0.9592\n",
            "Layer 0, Epoch [49/100], Step [468/469], Positive Goodness: 0.9669, Negative Goodness: 0.0553\n",
            "Layer 1, Epoch [49/100], Step [468/469], Positive Goodness: 0.9598, Negative Goodness: 0.0378\n",
            "Layer 2, Epoch [49/100], Step [468/469], Positive Goodness: 0.9635, Negative Goodness: 0.0357\n",
            "Layer 3, Epoch [49/100], Step [468/469], Positive Goodness: 0.9493, Negative Goodness: 0.0630\n",
            "Final Layer, Epoch [49/100], Step [468/469], Loss: 0.0003\n",
            "Epoch [49/100], Step [468/469], Loss: 0.1936, Test Accuracy: 0.9588\n",
            "Layer 0, Epoch [50/100], Step [117/469], Positive Goodness: 0.9668, Negative Goodness: 0.0514\n",
            "Layer 1, Epoch [50/100], Step [117/469], Positive Goodness: 0.9627, Negative Goodness: 0.0365\n",
            "Layer 2, Epoch [50/100], Step [117/469], Positive Goodness: 0.9671, Negative Goodness: 0.0363\n",
            "Layer 3, Epoch [50/100], Step [117/469], Positive Goodness: 0.9497, Negative Goodness: 0.0607\n",
            "Final Layer, Epoch [50/100], Step [117/469], Loss: 0.0003\n",
            "Epoch [50/100], Step [117/469], Loss: 0.2128, Test Accuracy: 0.9576\n",
            "Layer 0, Epoch [50/100], Step [234/469], Positive Goodness: 0.9691, Negative Goodness: 0.0552\n",
            "Layer 1, Epoch [50/100], Step [234/469], Positive Goodness: 0.9605, Negative Goodness: 0.0356\n",
            "Layer 2, Epoch [50/100], Step [234/469], Positive Goodness: 0.9591, Negative Goodness: 0.0333\n",
            "Layer 3, Epoch [50/100], Step [234/469], Positive Goodness: 0.9445, Negative Goodness: 0.0536\n",
            "Final Layer, Epoch [50/100], Step [234/469], Loss: 0.0005\n",
            "Epoch [50/100], Step [234/469], Loss: 0.2043, Test Accuracy: 0.9580\n",
            "Layer 0, Epoch [50/100], Step [351/469], Positive Goodness: 0.9691, Negative Goodness: 0.0517\n",
            "Layer 1, Epoch [50/100], Step [351/469], Positive Goodness: 0.9594, Negative Goodness: 0.0352\n",
            "Layer 2, Epoch [50/100], Step [351/469], Positive Goodness: 0.9633, Negative Goodness: 0.0338\n",
            "Layer 3, Epoch [50/100], Step [351/469], Positive Goodness: 0.9482, Negative Goodness: 0.0594\n",
            "Final Layer, Epoch [50/100], Step [351/469], Loss: 0.0004\n",
            "Epoch [50/100], Step [351/469], Loss: 0.2055, Test Accuracy: 0.9590\n",
            "Layer 0, Epoch [50/100], Step [468/469], Positive Goodness: 0.9657, Negative Goodness: 0.0492\n",
            "Layer 1, Epoch [50/100], Step [468/469], Positive Goodness: 0.9616, Negative Goodness: 0.0334\n",
            "Layer 2, Epoch [50/100], Step [468/469], Positive Goodness: 0.9620, Negative Goodness: 0.0329\n",
            "Layer 3, Epoch [50/100], Step [468/469], Positive Goodness: 0.9495, Negative Goodness: 0.0610\n",
            "Final Layer, Epoch [50/100], Step [468/469], Loss: 0.0003\n",
            "Epoch [50/100], Step [468/469], Loss: 0.1970, Test Accuracy: 0.9567\n",
            "Layer 0, Epoch [51/100], Step [117/469], Positive Goodness: 0.9700, Negative Goodness: 0.0506\n",
            "Layer 1, Epoch [51/100], Step [117/469], Positive Goodness: 0.9621, Negative Goodness: 0.0373\n",
            "Layer 2, Epoch [51/100], Step [117/469], Positive Goodness: 0.9587, Negative Goodness: 0.0310\n",
            "Layer 3, Epoch [51/100], Step [117/469], Positive Goodness: 0.9502, Negative Goodness: 0.0620\n",
            "Final Layer, Epoch [51/100], Step [117/469], Loss: 0.0004\n",
            "Epoch [51/100], Step [117/469], Loss: 0.2068, Test Accuracy: 0.9567\n",
            "Layer 0, Epoch [51/100], Step [234/469], Positive Goodness: 0.9680, Negative Goodness: 0.0505\n",
            "Layer 1, Epoch [51/100], Step [234/469], Positive Goodness: 0.9598, Negative Goodness: 0.0336\n",
            "Layer 2, Epoch [51/100], Step [234/469], Positive Goodness: 0.9646, Negative Goodness: 0.0362\n",
            "Layer 3, Epoch [51/100], Step [234/469], Positive Goodness: 0.9447, Negative Goodness: 0.0572\n",
            "Final Layer, Epoch [51/100], Step [234/469], Loss: 0.0003\n",
            "Epoch [51/100], Step [234/469], Loss: 0.2008, Test Accuracy: 0.9593\n",
            "Layer 0, Epoch [51/100], Step [351/469], Positive Goodness: 0.9687, Negative Goodness: 0.0464\n",
            "Layer 1, Epoch [51/100], Step [351/469], Positive Goodness: 0.9640, Negative Goodness: 0.0355\n",
            "Layer 2, Epoch [51/100], Step [351/469], Positive Goodness: 0.9598, Negative Goodness: 0.0288\n",
            "Layer 3, Epoch [51/100], Step [351/469], Positive Goodness: 0.9520, Negative Goodness: 0.0625\n",
            "Final Layer, Epoch [51/100], Step [351/469], Loss: 0.0004\n",
            "Epoch [51/100], Step [351/469], Loss: 0.1978, Test Accuracy: 0.9563\n",
            "Layer 0, Epoch [51/100], Step [468/469], Positive Goodness: 0.9669, Negative Goodness: 0.0510\n",
            "Layer 1, Epoch [51/100], Step [468/469], Positive Goodness: 0.9603, Negative Goodness: 0.0326\n",
            "Layer 2, Epoch [51/100], Step [468/469], Positive Goodness: 0.9607, Negative Goodness: 0.0326\n",
            "Layer 3, Epoch [51/100], Step [468/469], Positive Goodness: 0.9505, Negative Goodness: 0.0620\n",
            "Final Layer, Epoch [51/100], Step [468/469], Loss: 0.0003\n",
            "Epoch [51/100], Step [468/469], Loss: 0.1977, Test Accuracy: 0.9601\n",
            "Layer 0, Epoch [52/100], Step [117/469], Positive Goodness: 0.9676, Negative Goodness: 0.0468\n",
            "Layer 1, Epoch [52/100], Step [117/469], Positive Goodness: 0.9616, Negative Goodness: 0.0330\n",
            "Layer 2, Epoch [52/100], Step [117/469], Positive Goodness: 0.9691, Negative Goodness: 0.0378\n",
            "Layer 3, Epoch [52/100], Step [117/469], Positive Goodness: 0.9488, Negative Goodness: 0.0610\n",
            "Final Layer, Epoch [52/100], Step [117/469], Loss: 0.0004\n",
            "Epoch [52/100], Step [117/469], Loss: 0.1964, Test Accuracy: 0.9571\n",
            "Layer 0, Epoch [52/100], Step [234/469], Positive Goodness: 0.9695, Negative Goodness: 0.0472\n",
            "Layer 1, Epoch [52/100], Step [234/469], Positive Goodness: 0.9597, Negative Goodness: 0.0299\n",
            "Layer 2, Epoch [52/100], Step [234/469], Positive Goodness: 0.9666, Negative Goodness: 0.0318\n",
            "Layer 3, Epoch [52/100], Step [234/469], Positive Goodness: 0.9520, Negative Goodness: 0.0599\n",
            "Final Layer, Epoch [52/100], Step [234/469], Loss: 0.0003\n",
            "Epoch [52/100], Step [234/469], Loss: 0.2009, Test Accuracy: 0.9582\n",
            "Layer 0, Epoch [52/100], Step [351/469], Positive Goodness: 0.9692, Negative Goodness: 0.0452\n",
            "Layer 1, Epoch [52/100], Step [351/469], Positive Goodness: 0.9539, Negative Goodness: 0.0303\n",
            "Layer 2, Epoch [52/100], Step [351/469], Positive Goodness: 0.9640, Negative Goodness: 0.0353\n",
            "Layer 3, Epoch [52/100], Step [351/469], Positive Goodness: 0.9473, Negative Goodness: 0.0579\n",
            "Final Layer, Epoch [52/100], Step [351/469], Loss: 0.0003\n",
            "Epoch [52/100], Step [351/469], Loss: 0.1916, Test Accuracy: 0.9559\n",
            "Layer 0, Epoch [52/100], Step [468/469], Positive Goodness: 0.9678, Negative Goodness: 0.0471\n",
            "Layer 1, Epoch [52/100], Step [468/469], Positive Goodness: 0.9631, Negative Goodness: 0.0351\n",
            "Layer 2, Epoch [52/100], Step [468/469], Positive Goodness: 0.9671, Negative Goodness: 0.0371\n",
            "Layer 3, Epoch [52/100], Step [468/469], Positive Goodness: 0.9496, Negative Goodness: 0.0591\n",
            "Final Layer, Epoch [52/100], Step [468/469], Loss: 0.0003\n",
            "Epoch [52/100], Step [468/469], Loss: 0.1962, Test Accuracy: 0.9568\n",
            "Layer 0, Epoch [53/100], Step [117/469], Positive Goodness: 0.9700, Negative Goodness: 0.0473\n",
            "Layer 1, Epoch [53/100], Step [117/469], Positive Goodness: 0.9599, Negative Goodness: 0.0293\n",
            "Layer 2, Epoch [53/100], Step [117/469], Positive Goodness: 0.9664, Negative Goodness: 0.0335\n",
            "Layer 3, Epoch [53/100], Step [117/469], Positive Goodness: 0.9512, Negative Goodness: 0.0595\n",
            "Final Layer, Epoch [53/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [53/100], Step [117/469], Loss: 0.1922, Test Accuracy: 0.9579\n",
            "Layer 0, Epoch [53/100], Step [234/469], Positive Goodness: 0.9693, Negative Goodness: 0.0426\n",
            "Layer 1, Epoch [53/100], Step [234/469], Positive Goodness: 0.9660, Negative Goodness: 0.0331\n",
            "Layer 2, Epoch [53/100], Step [234/469], Positive Goodness: 0.9690, Negative Goodness: 0.0335\n",
            "Layer 3, Epoch [53/100], Step [234/469], Positive Goodness: 0.9507, Negative Goodness: 0.0574\n",
            "Final Layer, Epoch [53/100], Step [234/469], Loss: 0.0005\n",
            "Epoch [53/100], Step [234/469], Loss: 0.2000, Test Accuracy: 0.9574\n",
            "Layer 0, Epoch [53/100], Step [351/469], Positive Goodness: 0.9674, Negative Goodness: 0.0448\n",
            "Layer 1, Epoch [53/100], Step [351/469], Positive Goodness: 0.9619, Negative Goodness: 0.0330\n",
            "Layer 2, Epoch [53/100], Step [351/469], Positive Goodness: 0.9614, Negative Goodness: 0.0309\n",
            "Layer 3, Epoch [53/100], Step [351/469], Positive Goodness: 0.9484, Negative Goodness: 0.0586\n",
            "Final Layer, Epoch [53/100], Step [351/469], Loss: 0.0004\n",
            "Epoch [53/100], Step [351/469], Loss: 0.1966, Test Accuracy: 0.9571\n",
            "Layer 0, Epoch [53/100], Step [468/469], Positive Goodness: 0.9698, Negative Goodness: 0.0445\n",
            "Layer 1, Epoch [53/100], Step [468/469], Positive Goodness: 0.9581, Negative Goodness: 0.0304\n",
            "Layer 2, Epoch [53/100], Step [468/469], Positive Goodness: 0.9648, Negative Goodness: 0.0339\n",
            "Layer 3, Epoch [53/100], Step [468/469], Positive Goodness: 0.9465, Negative Goodness: 0.0565\n",
            "Final Layer, Epoch [53/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [53/100], Step [468/469], Loss: 0.2064, Test Accuracy: 0.9577\n",
            "Layer 0, Epoch [54/100], Step [117/469], Positive Goodness: 0.9675, Negative Goodness: 0.0413\n",
            "Layer 1, Epoch [54/100], Step [117/469], Positive Goodness: 0.9634, Negative Goodness: 0.0318\n",
            "Layer 2, Epoch [54/100], Step [117/469], Positive Goodness: 0.9668, Negative Goodness: 0.0323\n",
            "Layer 3, Epoch [54/100], Step [117/469], Positive Goodness: 0.9453, Negative Goodness: 0.0543\n",
            "Final Layer, Epoch [54/100], Step [117/469], Loss: 0.0005\n",
            "Epoch [54/100], Step [117/469], Loss: 0.1943, Test Accuracy: 0.9580\n",
            "Layer 0, Epoch [54/100], Step [234/469], Positive Goodness: 0.9712, Negative Goodness: 0.0439\n",
            "Layer 1, Epoch [54/100], Step [234/469], Positive Goodness: 0.9630, Negative Goodness: 0.0301\n",
            "Layer 2, Epoch [54/100], Step [234/469], Positive Goodness: 0.9695, Negative Goodness: 0.0322\n",
            "Layer 3, Epoch [54/100], Step [234/469], Positive Goodness: 0.9556, Negative Goodness: 0.0587\n",
            "Final Layer, Epoch [54/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [54/100], Step [234/469], Loss: 0.1952, Test Accuracy: 0.9582\n",
            "Layer 0, Epoch [54/100], Step [351/469], Positive Goodness: 0.9684, Negative Goodness: 0.0421\n",
            "Layer 1, Epoch [54/100], Step [351/469], Positive Goodness: 0.9607, Negative Goodness: 0.0289\n",
            "Layer 2, Epoch [54/100], Step [351/469], Positive Goodness: 0.9641, Negative Goodness: 0.0280\n",
            "Layer 3, Epoch [54/100], Step [351/469], Positive Goodness: 0.9494, Negative Goodness: 0.0567\n",
            "Final Layer, Epoch [54/100], Step [351/469], Loss: 0.0003\n",
            "Epoch [54/100], Step [351/469], Loss: 0.1936, Test Accuracy: 0.9584\n",
            "Layer 0, Epoch [54/100], Step [468/469], Positive Goodness: 0.9691, Negative Goodness: 0.0443\n",
            "Layer 1, Epoch [54/100], Step [468/469], Positive Goodness: 0.9590, Negative Goodness: 0.0284\n",
            "Layer 2, Epoch [54/100], Step [468/469], Positive Goodness: 0.9644, Negative Goodness: 0.0308\n",
            "Layer 3, Epoch [54/100], Step [468/469], Positive Goodness: 0.9551, Negative Goodness: 0.0599\n",
            "Final Layer, Epoch [54/100], Step [468/469], Loss: 0.0003\n",
            "Epoch [54/100], Step [468/469], Loss: 0.1951, Test Accuracy: 0.9595\n",
            "Layer 0, Epoch [55/100], Step [117/469], Positive Goodness: 0.9709, Negative Goodness: 0.0418\n",
            "Layer 1, Epoch [55/100], Step [117/469], Positive Goodness: 0.9663, Negative Goodness: 0.0318\n",
            "Layer 2, Epoch [55/100], Step [117/469], Positive Goodness: 0.9678, Negative Goodness: 0.0269\n",
            "Layer 3, Epoch [55/100], Step [117/469], Positive Goodness: 0.9516, Negative Goodness: 0.0575\n",
            "Final Layer, Epoch [55/100], Step [117/469], Loss: 0.0004\n",
            "Epoch [55/100], Step [117/469], Loss: 0.1935, Test Accuracy: 0.9588\n",
            "Layer 0, Epoch [55/100], Step [234/469], Positive Goodness: 0.9697, Negative Goodness: 0.0433\n",
            "Layer 1, Epoch [55/100], Step [234/469], Positive Goodness: 0.9621, Negative Goodness: 0.0294\n",
            "Layer 2, Epoch [55/100], Step [234/469], Positive Goodness: 0.9678, Negative Goodness: 0.0354\n",
            "Layer 3, Epoch [55/100], Step [234/469], Positive Goodness: 0.9507, Negative Goodness: 0.0571\n",
            "Final Layer, Epoch [55/100], Step [234/469], Loss: 0.0003\n",
            "Epoch [55/100], Step [234/469], Loss: 0.2038, Test Accuracy: 0.9573\n",
            "Layer 0, Epoch [55/100], Step [351/469], Positive Goodness: 0.9709, Negative Goodness: 0.0403\n",
            "Layer 1, Epoch [55/100], Step [351/469], Positive Goodness: 0.9605, Negative Goodness: 0.0271\n",
            "Layer 2, Epoch [55/100], Step [351/469], Positive Goodness: 0.9637, Negative Goodness: 0.0318\n",
            "Layer 3, Epoch [55/100], Step [351/469], Positive Goodness: 0.9524, Negative Goodness: 0.0559\n",
            "Final Layer, Epoch [55/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [55/100], Step [351/469], Loss: 0.1953, Test Accuracy: 0.9577\n",
            "Layer 0, Epoch [55/100], Step [468/469], Positive Goodness: 0.9684, Negative Goodness: 0.0411\n",
            "Layer 1, Epoch [55/100], Step [468/469], Positive Goodness: 0.9604, Negative Goodness: 0.0282\n",
            "Layer 2, Epoch [55/100], Step [468/469], Positive Goodness: 0.9655, Negative Goodness: 0.0321\n",
            "Layer 3, Epoch [55/100], Step [468/469], Positive Goodness: 0.9515, Negative Goodness: 0.0574\n",
            "Final Layer, Epoch [55/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [55/100], Step [468/469], Loss: 0.1962, Test Accuracy: 0.9570\n",
            "Layer 0, Epoch [56/100], Step [117/469], Positive Goodness: 0.9703, Negative Goodness: 0.0432\n",
            "Layer 1, Epoch [56/100], Step [117/469], Positive Goodness: 0.9628, Negative Goodness: 0.0311\n",
            "Layer 2, Epoch [56/100], Step [117/469], Positive Goodness: 0.9625, Negative Goodness: 0.0304\n",
            "Layer 3, Epoch [56/100], Step [117/469], Positive Goodness: 0.9528, Negative Goodness: 0.0607\n",
            "Final Layer, Epoch [56/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [56/100], Step [117/469], Loss: 0.1895, Test Accuracy: 0.9574\n",
            "Layer 0, Epoch [56/100], Step [234/469], Positive Goodness: 0.9707, Negative Goodness: 0.0392\n",
            "Layer 1, Epoch [56/100], Step [234/469], Positive Goodness: 0.9650, Negative Goodness: 0.0317\n",
            "Layer 2, Epoch [56/100], Step [234/469], Positive Goodness: 0.9641, Negative Goodness: 0.0290\n",
            "Layer 3, Epoch [56/100], Step [234/469], Positive Goodness: 0.9577, Negative Goodness: 0.0606\n",
            "Final Layer, Epoch [56/100], Step [234/469], Loss: 0.0004\n",
            "Epoch [56/100], Step [234/469], Loss: 0.1949, Test Accuracy: 0.9577\n",
            "Layer 0, Epoch [56/100], Step [351/469], Positive Goodness: 0.9697, Negative Goodness: 0.0401\n",
            "Layer 1, Epoch [56/100], Step [351/469], Positive Goodness: 0.9609, Negative Goodness: 0.0309\n",
            "Layer 2, Epoch [56/100], Step [351/469], Positive Goodness: 0.9654, Negative Goodness: 0.0300\n",
            "Layer 3, Epoch [56/100], Step [351/469], Positive Goodness: 0.9472, Negative Goodness: 0.0545\n",
            "Final Layer, Epoch [56/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [56/100], Step [351/469], Loss: 0.1966, Test Accuracy: 0.9587\n",
            "Layer 0, Epoch [56/100], Step [468/469], Positive Goodness: 0.9704, Negative Goodness: 0.0424\n",
            "Layer 1, Epoch [56/100], Step [468/469], Positive Goodness: 0.9650, Negative Goodness: 0.0302\n",
            "Layer 2, Epoch [56/100], Step [468/469], Positive Goodness: 0.9714, Negative Goodness: 0.0319\n",
            "Layer 3, Epoch [56/100], Step [468/469], Positive Goodness: 0.9487, Negative Goodness: 0.0525\n",
            "Final Layer, Epoch [56/100], Step [468/469], Loss: 0.0003\n",
            "Epoch [56/100], Step [468/469], Loss: 0.1944, Test Accuracy: 0.9590\n",
            "Layer 0, Epoch [57/100], Step [117/469], Positive Goodness: 0.9719, Negative Goodness: 0.0371\n",
            "Layer 1, Epoch [57/100], Step [117/469], Positive Goodness: 0.9641, Negative Goodness: 0.0278\n",
            "Layer 2, Epoch [57/100], Step [117/469], Positive Goodness: 0.9694, Negative Goodness: 0.0283\n",
            "Layer 3, Epoch [57/100], Step [117/469], Positive Goodness: 0.9523, Negative Goodness: 0.0508\n",
            "Final Layer, Epoch [57/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [57/100], Step [117/469], Loss: 0.1951, Test Accuracy: 0.9597\n",
            "Layer 0, Epoch [57/100], Step [234/469], Positive Goodness: 0.9707, Negative Goodness: 0.0421\n",
            "Layer 1, Epoch [57/100], Step [234/469], Positive Goodness: 0.9682, Negative Goodness: 0.0355\n",
            "Layer 2, Epoch [57/100], Step [234/469], Positive Goodness: 0.9720, Negative Goodness: 0.0338\n",
            "Layer 3, Epoch [57/100], Step [234/469], Positive Goodness: 0.9580, Negative Goodness: 0.0583\n",
            "Final Layer, Epoch [57/100], Step [234/469], Loss: 0.0003\n",
            "Epoch [57/100], Step [234/469], Loss: 0.2003, Test Accuracy: 0.9616\n",
            "Layer 0, Epoch [57/100], Step [351/469], Positive Goodness: 0.9698, Negative Goodness: 0.0417\n",
            "Layer 1, Epoch [57/100], Step [351/469], Positive Goodness: 0.9622, Negative Goodness: 0.0305\n",
            "Layer 2, Epoch [57/100], Step [351/469], Positive Goodness: 0.9650, Negative Goodness: 0.0310\n",
            "Layer 3, Epoch [57/100], Step [351/469], Positive Goodness: 0.9515, Negative Goodness: 0.0602\n",
            "Final Layer, Epoch [57/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [57/100], Step [351/469], Loss: 0.1917, Test Accuracy: 0.9600\n",
            "Layer 0, Epoch [57/100], Step [468/469], Positive Goodness: 0.9707, Negative Goodness: 0.0383\n",
            "Layer 1, Epoch [57/100], Step [468/469], Positive Goodness: 0.9636, Negative Goodness: 0.0286\n",
            "Layer 2, Epoch [57/100], Step [468/469], Positive Goodness: 0.9650, Negative Goodness: 0.0295\n",
            "Layer 3, Epoch [57/100], Step [468/469], Positive Goodness: 0.9498, Negative Goodness: 0.0566\n",
            "Final Layer, Epoch [57/100], Step [468/469], Loss: 0.0003\n",
            "Epoch [57/100], Step [468/469], Loss: 0.1933, Test Accuracy: 0.9600\n",
            "Layer 0, Epoch [58/100], Step [117/469], Positive Goodness: 0.9712, Negative Goodness: 0.0412\n",
            "Layer 1, Epoch [58/100], Step [117/469], Positive Goodness: 0.9673, Negative Goodness: 0.0310\n",
            "Layer 2, Epoch [58/100], Step [117/469], Positive Goodness: 0.9693, Negative Goodness: 0.0305\n",
            "Layer 3, Epoch [58/100], Step [117/469], Positive Goodness: 0.9535, Negative Goodness: 0.0551\n",
            "Final Layer, Epoch [58/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [58/100], Step [117/469], Loss: 0.1879, Test Accuracy: 0.9587\n",
            "Layer 0, Epoch [58/100], Step [234/469], Positive Goodness: 0.9709, Negative Goodness: 0.0347\n",
            "Layer 1, Epoch [58/100], Step [234/469], Positive Goodness: 0.9661, Negative Goodness: 0.0280\n",
            "Layer 2, Epoch [58/100], Step [234/469], Positive Goodness: 0.9678, Negative Goodness: 0.0264\n",
            "Layer 3, Epoch [58/100], Step [234/469], Positive Goodness: 0.9577, Negative Goodness: 0.0559\n",
            "Final Layer, Epoch [58/100], Step [234/469], Loss: 0.0003\n",
            "Epoch [58/100], Step [234/469], Loss: 0.1944, Test Accuracy: 0.9610\n",
            "Layer 0, Epoch [58/100], Step [351/469], Positive Goodness: 0.9711, Negative Goodness: 0.0394\n",
            "Layer 1, Epoch [58/100], Step [351/469], Positive Goodness: 0.9621, Negative Goodness: 0.0254\n",
            "Layer 2, Epoch [58/100], Step [351/469], Positive Goodness: 0.9682, Negative Goodness: 0.0297\n",
            "Layer 3, Epoch [58/100], Step [351/469], Positive Goodness: 0.9541, Negative Goodness: 0.0584\n",
            "Final Layer, Epoch [58/100], Step [351/469], Loss: 0.0003\n",
            "Epoch [58/100], Step [351/469], Loss: 0.1981, Test Accuracy: 0.9615\n",
            "Layer 0, Epoch [58/100], Step [468/469], Positive Goodness: 0.9725, Negative Goodness: 0.0387\n",
            "Layer 1, Epoch [58/100], Step [468/469], Positive Goodness: 0.9663, Negative Goodness: 0.0280\n",
            "Layer 2, Epoch [58/100], Step [468/469], Positive Goodness: 0.9686, Negative Goodness: 0.0300\n",
            "Layer 3, Epoch [58/100], Step [468/469], Positive Goodness: 0.9553, Negative Goodness: 0.0551\n",
            "Final Layer, Epoch [58/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [58/100], Step [468/469], Loss: 0.1863, Test Accuracy: 0.9604\n",
            "Layer 0, Epoch [59/100], Step [117/469], Positive Goodness: 0.9702, Negative Goodness: 0.0375\n",
            "Layer 1, Epoch [59/100], Step [117/469], Positive Goodness: 0.9647, Negative Goodness: 0.0259\n",
            "Layer 2, Epoch [59/100], Step [117/469], Positive Goodness: 0.9687, Negative Goodness: 0.0275\n",
            "Layer 3, Epoch [59/100], Step [117/469], Positive Goodness: 0.9543, Negative Goodness: 0.0584\n",
            "Final Layer, Epoch [59/100], Step [117/469], Loss: 0.0003\n",
            "Epoch [59/100], Step [117/469], Loss: 0.1906, Test Accuracy: 0.9595\n",
            "Layer 0, Epoch [59/100], Step [234/469], Positive Goodness: 0.9731, Negative Goodness: 0.0376\n",
            "Layer 1, Epoch [59/100], Step [234/469], Positive Goodness: 0.9693, Negative Goodness: 0.0275\n",
            "Layer 2, Epoch [59/100], Step [234/469], Positive Goodness: 0.9699, Negative Goodness: 0.0246\n",
            "Layer 3, Epoch [59/100], Step [234/469], Positive Goodness: 0.9544, Negative Goodness: 0.0514\n",
            "Final Layer, Epoch [59/100], Step [234/469], Loss: 0.0003\n",
            "Epoch [59/100], Step [234/469], Loss: 0.1832, Test Accuracy: 0.9597\n",
            "Layer 0, Epoch [59/100], Step [351/469], Positive Goodness: 0.9721, Negative Goodness: 0.0384\n",
            "Layer 1, Epoch [59/100], Step [351/469], Positive Goodness: 0.9664, Negative Goodness: 0.0259\n",
            "Layer 2, Epoch [59/100], Step [351/469], Positive Goodness: 0.9699, Negative Goodness: 0.0313\n",
            "Layer 3, Epoch [59/100], Step [351/469], Positive Goodness: 0.9550, Negative Goodness: 0.0525\n",
            "Final Layer, Epoch [59/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [59/100], Step [351/469], Loss: 0.1891, Test Accuracy: 0.9595\n",
            "Layer 0, Epoch [59/100], Step [468/469], Positive Goodness: 0.9718, Negative Goodness: 0.0378\n",
            "Layer 1, Epoch [59/100], Step [468/469], Positive Goodness: 0.9666, Negative Goodness: 0.0295\n",
            "Layer 2, Epoch [59/100], Step [468/469], Positive Goodness: 0.9697, Negative Goodness: 0.0266\n",
            "Layer 3, Epoch [59/100], Step [468/469], Positive Goodness: 0.9557, Negative Goodness: 0.0535\n",
            "Final Layer, Epoch [59/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [59/100], Step [468/469], Loss: 0.1873, Test Accuracy: 0.9595\n",
            "Layer 0, Epoch [60/100], Step [117/469], Positive Goodness: 0.9713, Negative Goodness: 0.0345\n",
            "Layer 1, Epoch [60/100], Step [117/469], Positive Goodness: 0.9668, Negative Goodness: 0.0286\n",
            "Layer 2, Epoch [60/100], Step [117/469], Positive Goodness: 0.9685, Negative Goodness: 0.0258\n",
            "Layer 3, Epoch [60/100], Step [117/469], Positive Goodness: 0.9537, Negative Goodness: 0.0508\n",
            "Final Layer, Epoch [60/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [60/100], Step [117/469], Loss: 0.1909, Test Accuracy: 0.9614\n",
            "Layer 0, Epoch [60/100], Step [234/469], Positive Goodness: 0.9715, Negative Goodness: 0.0353\n",
            "Layer 1, Epoch [60/100], Step [234/469], Positive Goodness: 0.9647, Negative Goodness: 0.0241\n",
            "Layer 2, Epoch [60/100], Step [234/469], Positive Goodness: 0.9687, Negative Goodness: 0.0277\n",
            "Layer 3, Epoch [60/100], Step [234/469], Positive Goodness: 0.9564, Negative Goodness: 0.0504\n",
            "Final Layer, Epoch [60/100], Step [234/469], Loss: 0.0004\n",
            "Epoch [60/100], Step [234/469], Loss: 0.1954, Test Accuracy: 0.9617\n",
            "Layer 0, Epoch [60/100], Step [351/469], Positive Goodness: 0.9736, Negative Goodness: 0.0395\n",
            "Layer 1, Epoch [60/100], Step [351/469], Positive Goodness: 0.9664, Negative Goodness: 0.0280\n",
            "Layer 2, Epoch [60/100], Step [351/469], Positive Goodness: 0.9679, Negative Goodness: 0.0241\n",
            "Layer 3, Epoch [60/100], Step [351/469], Positive Goodness: 0.9545, Negative Goodness: 0.0525\n",
            "Final Layer, Epoch [60/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [60/100], Step [351/469], Loss: 0.1956, Test Accuracy: 0.9625\n",
            "Layer 0, Epoch [60/100], Step [468/469], Positive Goodness: 0.9716, Negative Goodness: 0.0372\n",
            "Layer 1, Epoch [60/100], Step [468/469], Positive Goodness: 0.9666, Negative Goodness: 0.0264\n",
            "Layer 2, Epoch [60/100], Step [468/469], Positive Goodness: 0.9711, Negative Goodness: 0.0267\n",
            "Layer 3, Epoch [60/100], Step [468/469], Positive Goodness: 0.9568, Negative Goodness: 0.0545\n",
            "Final Layer, Epoch [60/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [60/100], Step [468/469], Loss: 0.1909, Test Accuracy: 0.9602\n",
            "Layer 0, Epoch [61/100], Step [117/469], Positive Goodness: 0.9721, Negative Goodness: 0.0351\n",
            "Layer 1, Epoch [61/100], Step [117/469], Positive Goodness: 0.9667, Negative Goodness: 0.0217\n",
            "Layer 2, Epoch [61/100], Step [117/469], Positive Goodness: 0.9692, Negative Goodness: 0.0264\n",
            "Layer 3, Epoch [61/100], Step [117/469], Positive Goodness: 0.9581, Negative Goodness: 0.0525\n",
            "Final Layer, Epoch [61/100], Step [117/469], Loss: 0.0003\n",
            "Epoch [61/100], Step [117/469], Loss: 0.1860, Test Accuracy: 0.9614\n",
            "Layer 0, Epoch [61/100], Step [234/469], Positive Goodness: 0.9729, Negative Goodness: 0.0319\n",
            "Layer 1, Epoch [61/100], Step [234/469], Positive Goodness: 0.9656, Negative Goodness: 0.0236\n",
            "Layer 2, Epoch [61/100], Step [234/469], Positive Goodness: 0.9706, Negative Goodness: 0.0277\n",
            "Layer 3, Epoch [61/100], Step [234/469], Positive Goodness: 0.9534, Negative Goodness: 0.0499\n",
            "Final Layer, Epoch [61/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [61/100], Step [234/469], Loss: 0.1861, Test Accuracy: 0.9626\n",
            "Layer 0, Epoch [61/100], Step [351/469], Positive Goodness: 0.9725, Negative Goodness: 0.0372\n",
            "Layer 1, Epoch [61/100], Step [351/469], Positive Goodness: 0.9675, Negative Goodness: 0.0285\n",
            "Layer 2, Epoch [61/100], Step [351/469], Positive Goodness: 0.9708, Negative Goodness: 0.0293\n",
            "Layer 3, Epoch [61/100], Step [351/469], Positive Goodness: 0.9554, Negative Goodness: 0.0513\n",
            "Final Layer, Epoch [61/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [61/100], Step [351/469], Loss: 0.1846, Test Accuracy: 0.9620\n",
            "Layer 0, Epoch [61/100], Step [468/469], Positive Goodness: 0.9723, Negative Goodness: 0.0369\n",
            "Layer 1, Epoch [61/100], Step [468/469], Positive Goodness: 0.9687, Negative Goodness: 0.0262\n",
            "Layer 2, Epoch [61/100], Step [468/469], Positive Goodness: 0.9703, Negative Goodness: 0.0259\n",
            "Layer 3, Epoch [61/100], Step [468/469], Positive Goodness: 0.9560, Negative Goodness: 0.0503\n",
            "Final Layer, Epoch [61/100], Step [468/469], Loss: 0.0003\n",
            "Epoch [61/100], Step [468/469], Loss: 0.1858, Test Accuracy: 0.9624\n",
            "Layer 0, Epoch [62/100], Step [117/469], Positive Goodness: 0.9714, Negative Goodness: 0.0350\n",
            "Layer 1, Epoch [62/100], Step [117/469], Positive Goodness: 0.9646, Negative Goodness: 0.0239\n",
            "Layer 2, Epoch [62/100], Step [117/469], Positive Goodness: 0.9696, Negative Goodness: 0.0263\n",
            "Layer 3, Epoch [62/100], Step [117/469], Positive Goodness: 0.9589, Negative Goodness: 0.0510\n",
            "Final Layer, Epoch [62/100], Step [117/469], Loss: 0.0003\n",
            "Epoch [62/100], Step [117/469], Loss: 0.1864, Test Accuracy: 0.9613\n",
            "Layer 0, Epoch [62/100], Step [234/469], Positive Goodness: 0.9732, Negative Goodness: 0.0327\n",
            "Layer 1, Epoch [62/100], Step [234/469], Positive Goodness: 0.9698, Negative Goodness: 0.0278\n",
            "Layer 2, Epoch [62/100], Step [234/469], Positive Goodness: 0.9745, Negative Goodness: 0.0266\n",
            "Layer 3, Epoch [62/100], Step [234/469], Positive Goodness: 0.9594, Negative Goodness: 0.0491\n",
            "Final Layer, Epoch [62/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [62/100], Step [234/469], Loss: 0.1894, Test Accuracy: 0.9627\n",
            "Layer 0, Epoch [62/100], Step [351/469], Positive Goodness: 0.9732, Negative Goodness: 0.0368\n",
            "Layer 1, Epoch [62/100], Step [351/469], Positive Goodness: 0.9667, Negative Goodness: 0.0253\n",
            "Layer 2, Epoch [62/100], Step [351/469], Positive Goodness: 0.9676, Negative Goodness: 0.0246\n",
            "Layer 3, Epoch [62/100], Step [351/469], Positive Goodness: 0.9555, Negative Goodness: 0.0507\n",
            "Final Layer, Epoch [62/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [62/100], Step [351/469], Loss: 0.1975, Test Accuracy: 0.9625\n",
            "Layer 0, Epoch [62/100], Step [468/469], Positive Goodness: 0.9729, Negative Goodness: 0.0361\n",
            "Layer 1, Epoch [62/100], Step [468/469], Positive Goodness: 0.9652, Negative Goodness: 0.0242\n",
            "Layer 2, Epoch [62/100], Step [468/469], Positive Goodness: 0.9722, Negative Goodness: 0.0297\n",
            "Layer 3, Epoch [62/100], Step [468/469], Positive Goodness: 0.9542, Negative Goodness: 0.0498\n",
            "Final Layer, Epoch [62/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [62/100], Step [468/469], Loss: 0.1961, Test Accuracy: 0.9614\n",
            "Layer 0, Epoch [63/100], Step [117/469], Positive Goodness: 0.9728, Negative Goodness: 0.0324\n",
            "Layer 1, Epoch [63/100], Step [117/469], Positive Goodness: 0.9675, Negative Goodness: 0.0261\n",
            "Layer 2, Epoch [63/100], Step [117/469], Positive Goodness: 0.9728, Negative Goodness: 0.0248\n",
            "Layer 3, Epoch [63/100], Step [117/469], Positive Goodness: 0.9558, Negative Goodness: 0.0472\n",
            "Final Layer, Epoch [63/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [63/100], Step [117/469], Loss: 0.1942, Test Accuracy: 0.9617\n",
            "Layer 0, Epoch [63/100], Step [234/469], Positive Goodness: 0.9750, Negative Goodness: 0.0337\n",
            "Layer 1, Epoch [63/100], Step [234/469], Positive Goodness: 0.9684, Negative Goodness: 0.0241\n",
            "Layer 2, Epoch [63/100], Step [234/469], Positive Goodness: 0.9706, Negative Goodness: 0.0237\n",
            "Layer 3, Epoch [63/100], Step [234/469], Positive Goodness: 0.9623, Negative Goodness: 0.0488\n",
            "Final Layer, Epoch [63/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [63/100], Step [234/469], Loss: 0.1937, Test Accuracy: 0.9615\n",
            "Layer 0, Epoch [63/100], Step [351/469], Positive Goodness: 0.9738, Negative Goodness: 0.0348\n",
            "Layer 1, Epoch [63/100], Step [351/469], Positive Goodness: 0.9678, Negative Goodness: 0.0245\n",
            "Layer 2, Epoch [63/100], Step [351/469], Positive Goodness: 0.9718, Negative Goodness: 0.0293\n",
            "Layer 3, Epoch [63/100], Step [351/469], Positive Goodness: 0.9587, Negative Goodness: 0.0487\n",
            "Final Layer, Epoch [63/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [63/100], Step [351/469], Loss: 0.1985, Test Accuracy: 0.9622\n",
            "Layer 0, Epoch [63/100], Step [468/469], Positive Goodness: 0.9715, Negative Goodness: 0.0340\n",
            "Layer 1, Epoch [63/100], Step [468/469], Positive Goodness: 0.9669, Negative Goodness: 0.0256\n",
            "Layer 2, Epoch [63/100], Step [468/469], Positive Goodness: 0.9705, Negative Goodness: 0.0253\n",
            "Layer 3, Epoch [63/100], Step [468/469], Positive Goodness: 0.9562, Negative Goodness: 0.0517\n",
            "Final Layer, Epoch [63/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [63/100], Step [468/469], Loss: 0.1893, Test Accuracy: 0.9621\n",
            "Layer 0, Epoch [64/100], Step [117/469], Positive Goodness: 0.9735, Negative Goodness: 0.0322\n",
            "Layer 1, Epoch [64/100], Step [117/469], Positive Goodness: 0.9694, Negative Goodness: 0.0259\n",
            "Layer 2, Epoch [64/100], Step [117/469], Positive Goodness: 0.9725, Negative Goodness: 0.0263\n",
            "Layer 3, Epoch [64/100], Step [117/469], Positive Goodness: 0.9594, Negative Goodness: 0.0506\n",
            "Final Layer, Epoch [64/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [64/100], Step [117/469], Loss: 0.1892, Test Accuracy: 0.9620\n",
            "Layer 0, Epoch [64/100], Step [234/469], Positive Goodness: 0.9739, Negative Goodness: 0.0350\n",
            "Layer 1, Epoch [64/100], Step [234/469], Positive Goodness: 0.9695, Negative Goodness: 0.0257\n",
            "Layer 2, Epoch [64/100], Step [234/469], Positive Goodness: 0.9698, Negative Goodness: 0.0267\n",
            "Layer 3, Epoch [64/100], Step [234/469], Positive Goodness: 0.9600, Negative Goodness: 0.0512\n",
            "Final Layer, Epoch [64/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [64/100], Step [234/469], Loss: 0.1912, Test Accuracy: 0.9633\n",
            "Layer 0, Epoch [64/100], Step [351/469], Positive Goodness: 0.9719, Negative Goodness: 0.0319\n",
            "Layer 1, Epoch [64/100], Step [351/469], Positive Goodness: 0.9711, Negative Goodness: 0.0245\n",
            "Layer 2, Epoch [64/100], Step [351/469], Positive Goodness: 0.9700, Negative Goodness: 0.0214\n",
            "Layer 3, Epoch [64/100], Step [351/469], Positive Goodness: 0.9551, Negative Goodness: 0.0444\n",
            "Final Layer, Epoch [64/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [64/100], Step [351/469], Loss: 0.1944, Test Accuracy: 0.9621\n",
            "Layer 0, Epoch [64/100], Step [468/469], Positive Goodness: 0.9743, Negative Goodness: 0.0328\n",
            "Layer 1, Epoch [64/100], Step [468/469], Positive Goodness: 0.9683, Negative Goodness: 0.0258\n",
            "Layer 2, Epoch [64/100], Step [468/469], Positive Goodness: 0.9701, Negative Goodness: 0.0263\n",
            "Layer 3, Epoch [64/100], Step [468/469], Positive Goodness: 0.9577, Negative Goodness: 0.0529\n",
            "Final Layer, Epoch [64/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [64/100], Step [468/469], Loss: 0.1897, Test Accuracy: 0.9607\n",
            "Layer 0, Epoch [65/100], Step [117/469], Positive Goodness: 0.9758, Negative Goodness: 0.0298\n",
            "Layer 1, Epoch [65/100], Step [117/469], Positive Goodness: 0.9690, Negative Goodness: 0.0226\n",
            "Layer 2, Epoch [65/100], Step [117/469], Positive Goodness: 0.9720, Negative Goodness: 0.0199\n",
            "Layer 3, Epoch [65/100], Step [117/469], Positive Goodness: 0.9603, Negative Goodness: 0.0471\n",
            "Final Layer, Epoch [65/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [65/100], Step [117/469], Loss: 0.1881, Test Accuracy: 0.9620\n",
            "Layer 0, Epoch [65/100], Step [234/469], Positive Goodness: 0.9754, Negative Goodness: 0.0295\n",
            "Layer 1, Epoch [65/100], Step [234/469], Positive Goodness: 0.9700, Negative Goodness: 0.0213\n",
            "Layer 2, Epoch [65/100], Step [234/469], Positive Goodness: 0.9745, Negative Goodness: 0.0238\n",
            "Layer 3, Epoch [65/100], Step [234/469], Positive Goodness: 0.9620, Negative Goodness: 0.0490\n",
            "Final Layer, Epoch [65/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [65/100], Step [234/469], Loss: 0.1924, Test Accuracy: 0.9619\n",
            "Layer 0, Epoch [65/100], Step [351/469], Positive Goodness: 0.9737, Negative Goodness: 0.0318\n",
            "Layer 1, Epoch [65/100], Step [351/469], Positive Goodness: 0.9714, Negative Goodness: 0.0232\n",
            "Layer 2, Epoch [65/100], Step [351/469], Positive Goodness: 0.9757, Negative Goodness: 0.0265\n",
            "Layer 3, Epoch [65/100], Step [351/469], Positive Goodness: 0.9576, Negative Goodness: 0.0488\n",
            "Final Layer, Epoch [65/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [65/100], Step [351/469], Loss: 0.1867, Test Accuracy: 0.9618\n",
            "Layer 0, Epoch [65/100], Step [468/469], Positive Goodness: 0.9699, Negative Goodness: 0.0348\n",
            "Layer 1, Epoch [65/100], Step [468/469], Positive Goodness: 0.9657, Negative Goodness: 0.0235\n",
            "Layer 2, Epoch [65/100], Step [468/469], Positive Goodness: 0.9690, Negative Goodness: 0.0260\n",
            "Layer 3, Epoch [65/100], Step [468/469], Positive Goodness: 0.9540, Negative Goodness: 0.0512\n",
            "Final Layer, Epoch [65/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [65/100], Step [468/469], Loss: 0.1909, Test Accuracy: 0.9622\n",
            "Layer 0, Epoch [66/100], Step [117/469], Positive Goodness: 0.9763, Negative Goodness: 0.0320\n",
            "Layer 1, Epoch [66/100], Step [117/469], Positive Goodness: 0.9727, Negative Goodness: 0.0233\n",
            "Layer 2, Epoch [66/100], Step [117/469], Positive Goodness: 0.9744, Negative Goodness: 0.0233\n",
            "Layer 3, Epoch [66/100], Step [117/469], Positive Goodness: 0.9614, Negative Goodness: 0.0459\n",
            "Final Layer, Epoch [66/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [66/100], Step [117/469], Loss: 0.1891, Test Accuracy: 0.9624\n",
            "Layer 0, Epoch [66/100], Step [234/469], Positive Goodness: 0.9738, Negative Goodness: 0.0305\n",
            "Layer 1, Epoch [66/100], Step [234/469], Positive Goodness: 0.9704, Negative Goodness: 0.0238\n",
            "Layer 2, Epoch [66/100], Step [234/469], Positive Goodness: 0.9732, Negative Goodness: 0.0238\n",
            "Layer 3, Epoch [66/100], Step [234/469], Positive Goodness: 0.9587, Negative Goodness: 0.0471\n",
            "Final Layer, Epoch [66/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [66/100], Step [234/469], Loss: 0.1928, Test Accuracy: 0.9618\n",
            "Layer 0, Epoch [66/100], Step [351/469], Positive Goodness: 0.9728, Negative Goodness: 0.0312\n",
            "Layer 1, Epoch [66/100], Step [351/469], Positive Goodness: 0.9667, Negative Goodness: 0.0221\n",
            "Layer 2, Epoch [66/100], Step [351/469], Positive Goodness: 0.9695, Negative Goodness: 0.0239\n",
            "Layer 3, Epoch [66/100], Step [351/469], Positive Goodness: 0.9579, Negative Goodness: 0.0512\n",
            "Final Layer, Epoch [66/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [66/100], Step [351/469], Loss: 0.1864, Test Accuracy: 0.9629\n",
            "Layer 0, Epoch [66/100], Step [468/469], Positive Goodness: 0.9728, Negative Goodness: 0.0312\n",
            "Layer 1, Epoch [66/100], Step [468/469], Positive Goodness: 0.9724, Negative Goodness: 0.0241\n",
            "Layer 2, Epoch [66/100], Step [468/469], Positive Goodness: 0.9736, Negative Goodness: 0.0250\n",
            "Layer 3, Epoch [66/100], Step [468/469], Positive Goodness: 0.9583, Negative Goodness: 0.0502\n",
            "Final Layer, Epoch [66/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [66/100], Step [468/469], Loss: 0.1895, Test Accuracy: 0.9630\n",
            "Layer 0, Epoch [67/100], Step [117/469], Positive Goodness: 0.9726, Negative Goodness: 0.0310\n",
            "Layer 1, Epoch [67/100], Step [117/469], Positive Goodness: 0.9694, Negative Goodness: 0.0205\n",
            "Layer 2, Epoch [67/100], Step [117/469], Positive Goodness: 0.9741, Negative Goodness: 0.0259\n",
            "Layer 3, Epoch [67/100], Step [117/469], Positive Goodness: 0.9607, Negative Goodness: 0.0480\n",
            "Final Layer, Epoch [67/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [67/100], Step [117/469], Loss: 0.1952, Test Accuracy: 0.9623\n",
            "Layer 0, Epoch [67/100], Step [234/469], Positive Goodness: 0.9756, Negative Goodness: 0.0297\n",
            "Layer 1, Epoch [67/100], Step [234/469], Positive Goodness: 0.9712, Negative Goodness: 0.0246\n",
            "Layer 2, Epoch [67/100], Step [234/469], Positive Goodness: 0.9736, Negative Goodness: 0.0230\n",
            "Layer 3, Epoch [67/100], Step [234/469], Positive Goodness: 0.9595, Negative Goodness: 0.0472\n",
            "Final Layer, Epoch [67/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [67/100], Step [234/469], Loss: 0.1936, Test Accuracy: 0.9618\n",
            "Layer 0, Epoch [67/100], Step [351/469], Positive Goodness: 0.9753, Negative Goodness: 0.0301\n",
            "Layer 1, Epoch [67/100], Step [351/469], Positive Goodness: 0.9719, Negative Goodness: 0.0249\n",
            "Layer 2, Epoch [67/100], Step [351/469], Positive Goodness: 0.9737, Negative Goodness: 0.0231\n",
            "Layer 3, Epoch [67/100], Step [351/469], Positive Goodness: 0.9621, Negative Goodness: 0.0481\n",
            "Final Layer, Epoch [67/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [67/100], Step [351/469], Loss: 0.1879, Test Accuracy: 0.9600\n",
            "Layer 0, Epoch [67/100], Step [468/469], Positive Goodness: 0.9721, Negative Goodness: 0.0321\n",
            "Layer 1, Epoch [67/100], Step [468/469], Positive Goodness: 0.9725, Negative Goodness: 0.0264\n",
            "Layer 2, Epoch [67/100], Step [468/469], Positive Goodness: 0.9709, Negative Goodness: 0.0242\n",
            "Layer 3, Epoch [67/100], Step [468/469], Positive Goodness: 0.9572, Negative Goodness: 0.0518\n",
            "Final Layer, Epoch [67/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [67/100], Step [468/469], Loss: 0.1905, Test Accuracy: 0.9631\n",
            "Layer 0, Epoch [68/100], Step [117/469], Positive Goodness: 0.9754, Negative Goodness: 0.0307\n",
            "Layer 1, Epoch [68/100], Step [117/469], Positive Goodness: 0.9701, Negative Goodness: 0.0218\n",
            "Layer 2, Epoch [68/100], Step [117/469], Positive Goodness: 0.9727, Negative Goodness: 0.0225\n",
            "Layer 3, Epoch [68/100], Step [117/469], Positive Goodness: 0.9596, Negative Goodness: 0.0470\n",
            "Final Layer, Epoch [68/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [68/100], Step [117/469], Loss: 0.1899, Test Accuracy: 0.9624\n",
            "Layer 0, Epoch [68/100], Step [234/469], Positive Goodness: 0.9744, Negative Goodness: 0.0317\n",
            "Layer 1, Epoch [68/100], Step [234/469], Positive Goodness: 0.9691, Negative Goodness: 0.0223\n",
            "Layer 2, Epoch [68/100], Step [234/469], Positive Goodness: 0.9745, Negative Goodness: 0.0221\n",
            "Layer 3, Epoch [68/100], Step [234/469], Positive Goodness: 0.9564, Negative Goodness: 0.0471\n",
            "Final Layer, Epoch [68/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [68/100], Step [234/469], Loss: 0.1906, Test Accuracy: 0.9634\n",
            "Layer 0, Epoch [68/100], Step [351/469], Positive Goodness: 0.9754, Negative Goodness: 0.0327\n",
            "Layer 1, Epoch [68/100], Step [351/469], Positive Goodness: 0.9715, Negative Goodness: 0.0237\n",
            "Layer 2, Epoch [68/100], Step [351/469], Positive Goodness: 0.9738, Negative Goodness: 0.0229\n",
            "Layer 3, Epoch [68/100], Step [351/469], Positive Goodness: 0.9582, Negative Goodness: 0.0504\n",
            "Final Layer, Epoch [68/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [68/100], Step [351/469], Loss: 0.1932, Test Accuracy: 0.9633\n",
            "Layer 0, Epoch [68/100], Step [468/469], Positive Goodness: 0.9712, Negative Goodness: 0.0298\n",
            "Layer 1, Epoch [68/100], Step [468/469], Positive Goodness: 0.9723, Negative Goodness: 0.0247\n",
            "Layer 2, Epoch [68/100], Step [468/469], Positive Goodness: 0.9704, Negative Goodness: 0.0228\n",
            "Layer 3, Epoch [68/100], Step [468/469], Positive Goodness: 0.9614, Negative Goodness: 0.0498\n",
            "Final Layer, Epoch [68/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [68/100], Step [468/469], Loss: 0.1971, Test Accuracy: 0.9619\n",
            "Layer 0, Epoch [69/100], Step [117/469], Positive Goodness: 0.9744, Negative Goodness: 0.0311\n",
            "Layer 1, Epoch [69/100], Step [117/469], Positive Goodness: 0.9709, Negative Goodness: 0.0218\n",
            "Layer 2, Epoch [69/100], Step [117/469], Positive Goodness: 0.9753, Negative Goodness: 0.0241\n",
            "Layer 3, Epoch [69/100], Step [117/469], Positive Goodness: 0.9614, Negative Goodness: 0.0470\n",
            "Final Layer, Epoch [69/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [69/100], Step [117/469], Loss: 0.1930, Test Accuracy: 0.9638\n",
            "Layer 0, Epoch [69/100], Step [234/469], Positive Goodness: 0.9738, Negative Goodness: 0.0308\n",
            "Layer 1, Epoch [69/100], Step [234/469], Positive Goodness: 0.9708, Negative Goodness: 0.0237\n",
            "Layer 2, Epoch [69/100], Step [234/469], Positive Goodness: 0.9721, Negative Goodness: 0.0223\n",
            "Layer 3, Epoch [69/100], Step [234/469], Positive Goodness: 0.9583, Negative Goodness: 0.0440\n",
            "Final Layer, Epoch [69/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [69/100], Step [234/469], Loss: 0.1975, Test Accuracy: 0.9632\n",
            "Layer 0, Epoch [69/100], Step [351/469], Positive Goodness: 0.9757, Negative Goodness: 0.0306\n",
            "Layer 1, Epoch [69/100], Step [351/469], Positive Goodness: 0.9728, Negative Goodness: 0.0218\n",
            "Layer 2, Epoch [69/100], Step [351/469], Positive Goodness: 0.9736, Negative Goodness: 0.0220\n",
            "Layer 3, Epoch [69/100], Step [351/469], Positive Goodness: 0.9598, Negative Goodness: 0.0454\n",
            "Final Layer, Epoch [69/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [69/100], Step [351/469], Loss: 0.1952, Test Accuracy: 0.9627\n",
            "Layer 0, Epoch [69/100], Step [468/469], Positive Goodness: 0.9741, Negative Goodness: 0.0291\n",
            "Layer 1, Epoch [69/100], Step [468/469], Positive Goodness: 0.9709, Negative Goodness: 0.0230\n",
            "Layer 2, Epoch [69/100], Step [468/469], Positive Goodness: 0.9755, Negative Goodness: 0.0244\n",
            "Layer 3, Epoch [69/100], Step [468/469], Positive Goodness: 0.9598, Negative Goodness: 0.0487\n",
            "Final Layer, Epoch [69/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [69/100], Step [468/469], Loss: 0.1928, Test Accuracy: 0.9632\n",
            "Layer 0, Epoch [70/100], Step [117/469], Positive Goodness: 0.9756, Negative Goodness: 0.0293\n",
            "Layer 1, Epoch [70/100], Step [117/469], Positive Goodness: 0.9703, Negative Goodness: 0.0195\n",
            "Layer 2, Epoch [70/100], Step [117/469], Positive Goodness: 0.9744, Negative Goodness: 0.0200\n",
            "Layer 3, Epoch [70/100], Step [117/469], Positive Goodness: 0.9595, Negative Goodness: 0.0482\n",
            "Final Layer, Epoch [70/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [70/100], Step [117/469], Loss: 0.1942, Test Accuracy: 0.9647\n",
            "Layer 0, Epoch [70/100], Step [234/469], Positive Goodness: 0.9739, Negative Goodness: 0.0281\n",
            "Layer 1, Epoch [70/100], Step [234/469], Positive Goodness: 0.9701, Negative Goodness: 0.0209\n",
            "Layer 2, Epoch [70/100], Step [234/469], Positive Goodness: 0.9730, Negative Goodness: 0.0225\n",
            "Layer 3, Epoch [70/100], Step [234/469], Positive Goodness: 0.9595, Negative Goodness: 0.0445\n",
            "Final Layer, Epoch [70/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [70/100], Step [234/469], Loss: 0.1942, Test Accuracy: 0.9643\n",
            "Layer 0, Epoch [70/100], Step [351/469], Positive Goodness: 0.9753, Negative Goodness: 0.0306\n",
            "Layer 1, Epoch [70/100], Step [351/469], Positive Goodness: 0.9718, Negative Goodness: 0.0230\n",
            "Layer 2, Epoch [70/100], Step [351/469], Positive Goodness: 0.9697, Negative Goodness: 0.0210\n",
            "Layer 3, Epoch [70/100], Step [351/469], Positive Goodness: 0.9612, Negative Goodness: 0.0454\n",
            "Final Layer, Epoch [70/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [70/100], Step [351/469], Loss: 0.1922, Test Accuracy: 0.9644\n",
            "Layer 0, Epoch [70/100], Step [468/469], Positive Goodness: 0.9740, Negative Goodness: 0.0308\n",
            "Layer 1, Epoch [70/100], Step [468/469], Positive Goodness: 0.9706, Negative Goodness: 0.0220\n",
            "Layer 2, Epoch [70/100], Step [468/469], Positive Goodness: 0.9727, Negative Goodness: 0.0229\n",
            "Layer 3, Epoch [70/100], Step [468/469], Positive Goodness: 0.9620, Negative Goodness: 0.0460\n",
            "Final Layer, Epoch [70/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [70/100], Step [468/469], Loss: 0.1951, Test Accuracy: 0.9628\n",
            "Layer 0, Epoch [71/100], Step [117/469], Positive Goodness: 0.9769, Negative Goodness: 0.0267\n",
            "Layer 1, Epoch [71/100], Step [117/469], Positive Goodness: 0.9728, Negative Goodness: 0.0177\n",
            "Layer 2, Epoch [71/100], Step [117/469], Positive Goodness: 0.9782, Negative Goodness: 0.0230\n",
            "Layer 3, Epoch [71/100], Step [117/469], Positive Goodness: 0.9636, Negative Goodness: 0.0432\n",
            "Final Layer, Epoch [71/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [71/100], Step [117/469], Loss: 0.1978, Test Accuracy: 0.9649\n",
            "Layer 0, Epoch [71/100], Step [234/469], Positive Goodness: 0.9733, Negative Goodness: 0.0275\n",
            "Layer 1, Epoch [71/100], Step [234/469], Positive Goodness: 0.9713, Negative Goodness: 0.0234\n",
            "Layer 2, Epoch [71/100], Step [234/469], Positive Goodness: 0.9750, Negative Goodness: 0.0235\n",
            "Layer 3, Epoch [71/100], Step [234/469], Positive Goodness: 0.9632, Negative Goodness: 0.0478\n",
            "Final Layer, Epoch [71/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [71/100], Step [234/469], Loss: 0.1904, Test Accuracy: 0.9637\n",
            "Layer 0, Epoch [71/100], Step [351/469], Positive Goodness: 0.9755, Negative Goodness: 0.0299\n",
            "Layer 1, Epoch [71/100], Step [351/469], Positive Goodness: 0.9709, Negative Goodness: 0.0237\n",
            "Layer 2, Epoch [71/100], Step [351/469], Positive Goodness: 0.9755, Negative Goodness: 0.0226\n",
            "Layer 3, Epoch [71/100], Step [351/469], Positive Goodness: 0.9619, Negative Goodness: 0.0456\n",
            "Final Layer, Epoch [71/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [71/100], Step [351/469], Loss: 0.1953, Test Accuracy: 0.9638\n",
            "Layer 0, Epoch [71/100], Step [468/469], Positive Goodness: 0.9736, Negative Goodness: 0.0294\n",
            "Layer 1, Epoch [71/100], Step [468/469], Positive Goodness: 0.9728, Negative Goodness: 0.0220\n",
            "Layer 2, Epoch [71/100], Step [468/469], Positive Goodness: 0.9730, Negative Goodness: 0.0199\n",
            "Layer 3, Epoch [71/100], Step [468/469], Positive Goodness: 0.9570, Negative Goodness: 0.0440\n",
            "Final Layer, Epoch [71/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [71/100], Step [468/469], Loss: 0.1966, Test Accuracy: 0.9642\n",
            "Layer 0, Epoch [72/100], Step [117/469], Positive Goodness: 0.9762, Negative Goodness: 0.0283\n",
            "Layer 1, Epoch [72/100], Step [117/469], Positive Goodness: 0.9735, Negative Goodness: 0.0219\n",
            "Layer 2, Epoch [72/100], Step [117/469], Positive Goodness: 0.9751, Negative Goodness: 0.0220\n",
            "Layer 3, Epoch [72/100], Step [117/469], Positive Goodness: 0.9603, Negative Goodness: 0.0452\n",
            "Final Layer, Epoch [72/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [72/100], Step [117/469], Loss: 0.1908, Test Accuracy: 0.9633\n",
            "Layer 0, Epoch [72/100], Step [234/469], Positive Goodness: 0.9734, Negative Goodness: 0.0282\n",
            "Layer 1, Epoch [72/100], Step [234/469], Positive Goodness: 0.9682, Negative Goodness: 0.0212\n",
            "Layer 2, Epoch [72/100], Step [234/469], Positive Goodness: 0.9729, Negative Goodness: 0.0203\n",
            "Layer 3, Epoch [72/100], Step [234/469], Positive Goodness: 0.9586, Negative Goodness: 0.0432\n",
            "Final Layer, Epoch [72/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [72/100], Step [234/469], Loss: 0.1930, Test Accuracy: 0.9637\n",
            "Layer 0, Epoch [72/100], Step [351/469], Positive Goodness: 0.9753, Negative Goodness: 0.0276\n",
            "Layer 1, Epoch [72/100], Step [351/469], Positive Goodness: 0.9715, Negative Goodness: 0.0199\n",
            "Layer 2, Epoch [72/100], Step [351/469], Positive Goodness: 0.9753, Negative Goodness: 0.0199\n",
            "Layer 3, Epoch [72/100], Step [351/469], Positive Goodness: 0.9592, Negative Goodness: 0.0434\n",
            "Final Layer, Epoch [72/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [72/100], Step [351/469], Loss: 0.2009, Test Accuracy: 0.9633\n",
            "Layer 0, Epoch [72/100], Step [468/469], Positive Goodness: 0.9762, Negative Goodness: 0.0257\n",
            "Layer 1, Epoch [72/100], Step [468/469], Positive Goodness: 0.9738, Negative Goodness: 0.0193\n",
            "Layer 2, Epoch [72/100], Step [468/469], Positive Goodness: 0.9749, Negative Goodness: 0.0181\n",
            "Layer 3, Epoch [72/100], Step [468/469], Positive Goodness: 0.9652, Negative Goodness: 0.0443\n",
            "Final Layer, Epoch [72/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [72/100], Step [468/469], Loss: 0.1991, Test Accuracy: 0.9641\n",
            "Layer 0, Epoch [73/100], Step [117/469], Positive Goodness: 0.9761, Negative Goodness: 0.0298\n",
            "Layer 1, Epoch [73/100], Step [117/469], Positive Goodness: 0.9721, Negative Goodness: 0.0211\n",
            "Layer 2, Epoch [73/100], Step [117/469], Positive Goodness: 0.9755, Negative Goodness: 0.0206\n",
            "Layer 3, Epoch [73/100], Step [117/469], Positive Goodness: 0.9616, Negative Goodness: 0.0430\n",
            "Final Layer, Epoch [73/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [73/100], Step [117/469], Loss: 0.2013, Test Accuracy: 0.9643\n",
            "Layer 0, Epoch [73/100], Step [234/469], Positive Goodness: 0.9774, Negative Goodness: 0.0265\n",
            "Layer 1, Epoch [73/100], Step [234/469], Positive Goodness: 0.9730, Negative Goodness: 0.0192\n",
            "Layer 2, Epoch [73/100], Step [234/469], Positive Goodness: 0.9752, Negative Goodness: 0.0186\n",
            "Layer 3, Epoch [73/100], Step [234/469], Positive Goodness: 0.9594, Negative Goodness: 0.0424\n",
            "Final Layer, Epoch [73/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [73/100], Step [234/469], Loss: 0.1997, Test Accuracy: 0.9638\n",
            "Layer 0, Epoch [73/100], Step [351/469], Positive Goodness: 0.9752, Negative Goodness: 0.0291\n",
            "Layer 1, Epoch [73/100], Step [351/469], Positive Goodness: 0.9745, Negative Goodness: 0.0218\n",
            "Layer 2, Epoch [73/100], Step [351/469], Positive Goodness: 0.9748, Negative Goodness: 0.0195\n",
            "Layer 3, Epoch [73/100], Step [351/469], Positive Goodness: 0.9614, Negative Goodness: 0.0451\n",
            "Final Layer, Epoch [73/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [73/100], Step [351/469], Loss: 0.1990, Test Accuracy: 0.9631\n",
            "Layer 0, Epoch [73/100], Step [468/469], Positive Goodness: 0.9738, Negative Goodness: 0.0276\n",
            "Layer 1, Epoch [73/100], Step [468/469], Positive Goodness: 0.9695, Negative Goodness: 0.0215\n",
            "Layer 2, Epoch [73/100], Step [468/469], Positive Goodness: 0.9721, Negative Goodness: 0.0222\n",
            "Layer 3, Epoch [73/100], Step [468/469], Positive Goodness: 0.9597, Negative Goodness: 0.0454\n",
            "Final Layer, Epoch [73/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [73/100], Step [468/469], Loss: 0.1967, Test Accuracy: 0.9647\n",
            "Layer 0, Epoch [74/100], Step [117/469], Positive Goodness: 0.9736, Negative Goodness: 0.0261\n",
            "Layer 1, Epoch [74/100], Step [117/469], Positive Goodness: 0.9695, Negative Goodness: 0.0201\n",
            "Layer 2, Epoch [74/100], Step [117/469], Positive Goodness: 0.9771, Negative Goodness: 0.0226\n",
            "Layer 3, Epoch [74/100], Step [117/469], Positive Goodness: 0.9600, Negative Goodness: 0.0444\n",
            "Final Layer, Epoch [74/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [74/100], Step [117/469], Loss: 0.1934, Test Accuracy: 0.9637\n",
            "Layer 0, Epoch [74/100], Step [234/469], Positive Goodness: 0.9761, Negative Goodness: 0.0276\n",
            "Layer 1, Epoch [74/100], Step [234/469], Positive Goodness: 0.9732, Negative Goodness: 0.0221\n",
            "Layer 2, Epoch [74/100], Step [234/469], Positive Goodness: 0.9744, Negative Goodness: 0.0213\n",
            "Layer 3, Epoch [74/100], Step [234/469], Positive Goodness: 0.9601, Negative Goodness: 0.0426\n",
            "Final Layer, Epoch [74/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [74/100], Step [234/469], Loss: 0.1937, Test Accuracy: 0.9639\n",
            "Layer 0, Epoch [74/100], Step [351/469], Positive Goodness: 0.9759, Negative Goodness: 0.0287\n",
            "Layer 1, Epoch [74/100], Step [351/469], Positive Goodness: 0.9742, Negative Goodness: 0.0218\n",
            "Layer 2, Epoch [74/100], Step [351/469], Positive Goodness: 0.9759, Negative Goodness: 0.0214\n",
            "Layer 3, Epoch [74/100], Step [351/469], Positive Goodness: 0.9617, Negative Goodness: 0.0457\n",
            "Final Layer, Epoch [74/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [74/100], Step [351/469], Loss: 0.1942, Test Accuracy: 0.9640\n",
            "Layer 0, Epoch [74/100], Step [468/469], Positive Goodness: 0.9766, Negative Goodness: 0.0273\n",
            "Layer 1, Epoch [74/100], Step [468/469], Positive Goodness: 0.9739, Negative Goodness: 0.0174\n",
            "Layer 2, Epoch [74/100], Step [468/469], Positive Goodness: 0.9777, Negative Goodness: 0.0197\n",
            "Layer 3, Epoch [74/100], Step [468/469], Positive Goodness: 0.9624, Negative Goodness: 0.0464\n",
            "Final Layer, Epoch [74/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [74/100], Step [468/469], Loss: 0.1948, Test Accuracy: 0.9634\n",
            "Layer 0, Epoch [75/100], Step [117/469], Positive Goodness: 0.9768, Negative Goodness: 0.0264\n",
            "Layer 1, Epoch [75/100], Step [117/469], Positive Goodness: 0.9721, Negative Goodness: 0.0184\n",
            "Layer 2, Epoch [75/100], Step [117/469], Positive Goodness: 0.9748, Negative Goodness: 0.0203\n",
            "Layer 3, Epoch [75/100], Step [117/469], Positive Goodness: 0.9634, Negative Goodness: 0.0472\n",
            "Final Layer, Epoch [75/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [75/100], Step [117/469], Loss: 0.1989, Test Accuracy: 0.9647\n",
            "Layer 0, Epoch [75/100], Step [234/469], Positive Goodness: 0.9741, Negative Goodness: 0.0281\n",
            "Layer 1, Epoch [75/100], Step [234/469], Positive Goodness: 0.9723, Negative Goodness: 0.0214\n",
            "Layer 2, Epoch [75/100], Step [234/469], Positive Goodness: 0.9750, Negative Goodness: 0.0212\n",
            "Layer 3, Epoch [75/100], Step [234/469], Positive Goodness: 0.9634, Negative Goodness: 0.0459\n",
            "Final Layer, Epoch [75/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [75/100], Step [234/469], Loss: 0.1957, Test Accuracy: 0.9637\n",
            "Layer 0, Epoch [75/100], Step [351/469], Positive Goodness: 0.9753, Negative Goodness: 0.0284\n",
            "Layer 1, Epoch [75/100], Step [351/469], Positive Goodness: 0.9740, Negative Goodness: 0.0203\n",
            "Layer 2, Epoch [75/100], Step [351/469], Positive Goodness: 0.9747, Negative Goodness: 0.0180\n",
            "Layer 3, Epoch [75/100], Step [351/469], Positive Goodness: 0.9605, Negative Goodness: 0.0417\n",
            "Final Layer, Epoch [75/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [75/100], Step [351/469], Loss: 0.1932, Test Accuracy: 0.9625\n",
            "Layer 0, Epoch [75/100], Step [468/469], Positive Goodness: 0.9762, Negative Goodness: 0.0251\n",
            "Layer 1, Epoch [75/100], Step [468/469], Positive Goodness: 0.9733, Negative Goodness: 0.0187\n",
            "Layer 2, Epoch [75/100], Step [468/469], Positive Goodness: 0.9783, Negative Goodness: 0.0203\n",
            "Layer 3, Epoch [75/100], Step [468/469], Positive Goodness: 0.9615, Negative Goodness: 0.0407\n",
            "Final Layer, Epoch [75/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [75/100], Step [468/469], Loss: 0.1953, Test Accuracy: 0.9647\n",
            "Layer 0, Epoch [76/100], Step [117/469], Positive Goodness: 0.9758, Negative Goodness: 0.0252\n",
            "Layer 1, Epoch [76/100], Step [117/469], Positive Goodness: 0.9725, Negative Goodness: 0.0183\n",
            "Layer 2, Epoch [76/100], Step [117/469], Positive Goodness: 0.9773, Negative Goodness: 0.0183\n",
            "Layer 3, Epoch [76/100], Step [117/469], Positive Goodness: 0.9624, Negative Goodness: 0.0425\n",
            "Final Layer, Epoch [76/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [76/100], Step [117/469], Loss: 0.1977, Test Accuracy: 0.9653\n",
            "Layer 0, Epoch [76/100], Step [234/469], Positive Goodness: 0.9761, Negative Goodness: 0.0252\n",
            "Layer 1, Epoch [76/100], Step [234/469], Positive Goodness: 0.9745, Negative Goodness: 0.0171\n",
            "Layer 2, Epoch [76/100], Step [234/469], Positive Goodness: 0.9766, Negative Goodness: 0.0183\n",
            "Layer 3, Epoch [76/100], Step [234/469], Positive Goodness: 0.9628, Negative Goodness: 0.0411\n",
            "Final Layer, Epoch [76/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [76/100], Step [234/469], Loss: 0.1945, Test Accuracy: 0.9643\n",
            "Layer 0, Epoch [76/100], Step [351/469], Positive Goodness: 0.9748, Negative Goodness: 0.0282\n",
            "Layer 1, Epoch [76/100], Step [351/469], Positive Goodness: 0.9712, Negative Goodness: 0.0182\n",
            "Layer 2, Epoch [76/100], Step [351/469], Positive Goodness: 0.9764, Negative Goodness: 0.0180\n",
            "Layer 3, Epoch [76/100], Step [351/469], Positive Goodness: 0.9629, Negative Goodness: 0.0446\n",
            "Final Layer, Epoch [76/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [76/100], Step [351/469], Loss: 0.1971, Test Accuracy: 0.9650\n",
            "Layer 0, Epoch [76/100], Step [468/469], Positive Goodness: 0.9768, Negative Goodness: 0.0256\n",
            "Layer 1, Epoch [76/100], Step [468/469], Positive Goodness: 0.9746, Negative Goodness: 0.0205\n",
            "Layer 2, Epoch [76/100], Step [468/469], Positive Goodness: 0.9751, Negative Goodness: 0.0179\n",
            "Layer 3, Epoch [76/100], Step [468/469], Positive Goodness: 0.9607, Negative Goodness: 0.0426\n",
            "Final Layer, Epoch [76/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [76/100], Step [468/469], Loss: 0.1944, Test Accuracy: 0.9653\n",
            "Layer 0, Epoch [77/100], Step [117/469], Positive Goodness: 0.9767, Negative Goodness: 0.0255\n",
            "Layer 1, Epoch [77/100], Step [117/469], Positive Goodness: 0.9740, Negative Goodness: 0.0180\n",
            "Layer 2, Epoch [77/100], Step [117/469], Positive Goodness: 0.9774, Negative Goodness: 0.0176\n",
            "Layer 3, Epoch [77/100], Step [117/469], Positive Goodness: 0.9634, Negative Goodness: 0.0426\n",
            "Final Layer, Epoch [77/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [77/100], Step [117/469], Loss: 0.1912, Test Accuracy: 0.9645\n",
            "Layer 0, Epoch [77/100], Step [234/469], Positive Goodness: 0.9761, Negative Goodness: 0.0263\n",
            "Layer 1, Epoch [77/100], Step [234/469], Positive Goodness: 0.9738, Negative Goodness: 0.0200\n",
            "Layer 2, Epoch [77/100], Step [234/469], Positive Goodness: 0.9766, Negative Goodness: 0.0178\n",
            "Layer 3, Epoch [77/100], Step [234/469], Positive Goodness: 0.9615, Negative Goodness: 0.0434\n",
            "Final Layer, Epoch [77/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [77/100], Step [234/469], Loss: 0.1949, Test Accuracy: 0.9651\n",
            "Layer 0, Epoch [77/100], Step [351/469], Positive Goodness: 0.9743, Negative Goodness: 0.0252\n",
            "Layer 1, Epoch [77/100], Step [351/469], Positive Goodness: 0.9722, Negative Goodness: 0.0194\n",
            "Layer 2, Epoch [77/100], Step [351/469], Positive Goodness: 0.9752, Negative Goodness: 0.0189\n",
            "Layer 3, Epoch [77/100], Step [351/469], Positive Goodness: 0.9615, Negative Goodness: 0.0444\n",
            "Final Layer, Epoch [77/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [77/100], Step [351/469], Loss: 0.1947, Test Accuracy: 0.9649\n",
            "Layer 0, Epoch [77/100], Step [468/469], Positive Goodness: 0.9769, Negative Goodness: 0.0266\n",
            "Layer 1, Epoch [77/100], Step [468/469], Positive Goodness: 0.9754, Negative Goodness: 0.0191\n",
            "Layer 2, Epoch [77/100], Step [468/469], Positive Goodness: 0.9772, Negative Goodness: 0.0192\n",
            "Layer 3, Epoch [77/100], Step [468/469], Positive Goodness: 0.9631, Negative Goodness: 0.0422\n",
            "Final Layer, Epoch [77/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [77/100], Step [468/469], Loss: 0.1960, Test Accuracy: 0.9653\n",
            "Layer 0, Epoch [78/100], Step [117/469], Positive Goodness: 0.9761, Negative Goodness: 0.0255\n",
            "Layer 1, Epoch [78/100], Step [117/469], Positive Goodness: 0.9729, Negative Goodness: 0.0182\n",
            "Layer 2, Epoch [78/100], Step [117/469], Positive Goodness: 0.9767, Negative Goodness: 0.0189\n",
            "Layer 3, Epoch [78/100], Step [117/469], Positive Goodness: 0.9641, Negative Goodness: 0.0421\n",
            "Final Layer, Epoch [78/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [78/100], Step [117/469], Loss: 0.1954, Test Accuracy: 0.9646\n",
            "Layer 0, Epoch [78/100], Step [234/469], Positive Goodness: 0.9769, Negative Goodness: 0.0280\n",
            "Layer 1, Epoch [78/100], Step [234/469], Positive Goodness: 0.9735, Negative Goodness: 0.0195\n",
            "Layer 2, Epoch [78/100], Step [234/469], Positive Goodness: 0.9760, Negative Goodness: 0.0196\n",
            "Layer 3, Epoch [78/100], Step [234/469], Positive Goodness: 0.9624, Negative Goodness: 0.0422\n",
            "Final Layer, Epoch [78/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [78/100], Step [234/469], Loss: 0.1974, Test Accuracy: 0.9657\n",
            "Layer 0, Epoch [78/100], Step [351/469], Positive Goodness: 0.9755, Negative Goodness: 0.0243\n",
            "Layer 1, Epoch [78/100], Step [351/469], Positive Goodness: 0.9747, Negative Goodness: 0.0194\n",
            "Layer 2, Epoch [78/100], Step [351/469], Positive Goodness: 0.9774, Negative Goodness: 0.0201\n",
            "Layer 3, Epoch [78/100], Step [351/469], Positive Goodness: 0.9611, Negative Goodness: 0.0428\n",
            "Final Layer, Epoch [78/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [78/100], Step [351/469], Loss: 0.1922, Test Accuracy: 0.9649\n",
            "Layer 0, Epoch [78/100], Step [468/469], Positive Goodness: 0.9766, Negative Goodness: 0.0251\n",
            "Layer 1, Epoch [78/100], Step [468/469], Positive Goodness: 0.9742, Negative Goodness: 0.0178\n",
            "Layer 2, Epoch [78/100], Step [468/469], Positive Goodness: 0.9784, Negative Goodness: 0.0177\n",
            "Layer 3, Epoch [78/100], Step [468/469], Positive Goodness: 0.9644, Negative Goodness: 0.0433\n",
            "Final Layer, Epoch [78/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [78/100], Step [468/469], Loss: 0.1903, Test Accuracy: 0.9651\n",
            "Layer 0, Epoch [79/100], Step [117/469], Positive Goodness: 0.9750, Negative Goodness: 0.0251\n",
            "Layer 1, Epoch [79/100], Step [117/469], Positive Goodness: 0.9758, Negative Goodness: 0.0192\n",
            "Layer 2, Epoch [79/100], Step [117/469], Positive Goodness: 0.9771, Negative Goodness: 0.0197\n",
            "Layer 3, Epoch [79/100], Step [117/469], Positive Goodness: 0.9620, Negative Goodness: 0.0417\n",
            "Final Layer, Epoch [79/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [79/100], Step [117/469], Loss: 0.1922, Test Accuracy: 0.9645\n",
            "Layer 0, Epoch [79/100], Step [234/469], Positive Goodness: 0.9772, Negative Goodness: 0.0256\n",
            "Layer 1, Epoch [79/100], Step [234/469], Positive Goodness: 0.9749, Negative Goodness: 0.0204\n",
            "Layer 2, Epoch [79/100], Step [234/469], Positive Goodness: 0.9787, Negative Goodness: 0.0187\n",
            "Layer 3, Epoch [79/100], Step [234/469], Positive Goodness: 0.9659, Negative Goodness: 0.0447\n",
            "Final Layer, Epoch [79/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [79/100], Step [234/469], Loss: 0.1892, Test Accuracy: 0.9649\n",
            "Layer 0, Epoch [79/100], Step [351/469], Positive Goodness: 0.9760, Negative Goodness: 0.0275\n",
            "Layer 1, Epoch [79/100], Step [351/469], Positive Goodness: 0.9746, Negative Goodness: 0.0209\n",
            "Layer 2, Epoch [79/100], Step [351/469], Positive Goodness: 0.9770, Negative Goodness: 0.0177\n",
            "Layer 3, Epoch [79/100], Step [351/469], Positive Goodness: 0.9638, Negative Goodness: 0.0430\n",
            "Final Layer, Epoch [79/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [79/100], Step [351/469], Loss: 0.1899, Test Accuracy: 0.9657\n",
            "Layer 0, Epoch [79/100], Step [468/469], Positive Goodness: 0.9770, Negative Goodness: 0.0248\n",
            "Layer 1, Epoch [79/100], Step [468/469], Positive Goodness: 0.9731, Negative Goodness: 0.0176\n",
            "Layer 2, Epoch [79/100], Step [468/469], Positive Goodness: 0.9771, Negative Goodness: 0.0185\n",
            "Layer 3, Epoch [79/100], Step [468/469], Positive Goodness: 0.9608, Negative Goodness: 0.0428\n",
            "Final Layer, Epoch [79/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [79/100], Step [468/469], Loss: 0.1955, Test Accuracy: 0.9647\n",
            "Layer 0, Epoch [80/100], Step [117/469], Positive Goodness: 0.9767, Negative Goodness: 0.0226\n",
            "Layer 1, Epoch [80/100], Step [117/469], Positive Goodness: 0.9772, Negative Goodness: 0.0172\n",
            "Layer 2, Epoch [80/100], Step [117/469], Positive Goodness: 0.9792, Negative Goodness: 0.0171\n",
            "Layer 3, Epoch [80/100], Step [117/469], Positive Goodness: 0.9645, Negative Goodness: 0.0428\n",
            "Final Layer, Epoch [80/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [80/100], Step [117/469], Loss: 0.1944, Test Accuracy: 0.9649\n",
            "Layer 0, Epoch [80/100], Step [234/469], Positive Goodness: 0.9762, Negative Goodness: 0.0242\n",
            "Layer 1, Epoch [80/100], Step [234/469], Positive Goodness: 0.9742, Negative Goodness: 0.0183\n",
            "Layer 2, Epoch [80/100], Step [234/469], Positive Goodness: 0.9778, Negative Goodness: 0.0169\n",
            "Layer 3, Epoch [80/100], Step [234/469], Positive Goodness: 0.9643, Negative Goodness: 0.0396\n",
            "Final Layer, Epoch [80/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [80/100], Step [234/469], Loss: 0.1948, Test Accuracy: 0.9647\n",
            "Layer 0, Epoch [80/100], Step [351/469], Positive Goodness: 0.9781, Negative Goodness: 0.0265\n",
            "Layer 1, Epoch [80/100], Step [351/469], Positive Goodness: 0.9729, Negative Goodness: 0.0176\n",
            "Layer 2, Epoch [80/100], Step [351/469], Positive Goodness: 0.9749, Negative Goodness: 0.0178\n",
            "Layer 3, Epoch [80/100], Step [351/469], Positive Goodness: 0.9634, Negative Goodness: 0.0431\n",
            "Final Layer, Epoch [80/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [80/100], Step [351/469], Loss: 0.1958, Test Accuracy: 0.9659\n",
            "Layer 0, Epoch [80/100], Step [468/469], Positive Goodness: 0.9743, Negative Goodness: 0.0235\n",
            "Layer 1, Epoch [80/100], Step [468/469], Positive Goodness: 0.9737, Negative Goodness: 0.0172\n",
            "Layer 2, Epoch [80/100], Step [468/469], Positive Goodness: 0.9749, Negative Goodness: 0.0162\n",
            "Layer 3, Epoch [80/100], Step [468/469], Positive Goodness: 0.9592, Negative Goodness: 0.0403\n",
            "Final Layer, Epoch [80/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [80/100], Step [468/469], Loss: 0.1971, Test Accuracy: 0.9653\n",
            "Layer 0, Epoch [81/100], Step [117/469], Positive Goodness: 0.9766, Negative Goodness: 0.0258\n",
            "Layer 1, Epoch [81/100], Step [117/469], Positive Goodness: 0.9742, Negative Goodness: 0.0182\n",
            "Layer 2, Epoch [81/100], Step [117/469], Positive Goodness: 0.9771, Negative Goodness: 0.0174\n",
            "Layer 3, Epoch [81/100], Step [117/469], Positive Goodness: 0.9658, Negative Goodness: 0.0460\n",
            "Final Layer, Epoch [81/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [81/100], Step [117/469], Loss: 0.1997, Test Accuracy: 0.9652\n",
            "Layer 0, Epoch [81/100], Step [234/469], Positive Goodness: 0.9755, Negative Goodness: 0.0236\n",
            "Layer 1, Epoch [81/100], Step [234/469], Positive Goodness: 0.9746, Negative Goodness: 0.0175\n",
            "Layer 2, Epoch [81/100], Step [234/469], Positive Goodness: 0.9769, Negative Goodness: 0.0156\n",
            "Layer 3, Epoch [81/100], Step [234/469], Positive Goodness: 0.9621, Negative Goodness: 0.0386\n",
            "Final Layer, Epoch [81/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [81/100], Step [234/469], Loss: 0.1993, Test Accuracy: 0.9647\n",
            "Layer 0, Epoch [81/100], Step [351/469], Positive Goodness: 0.9780, Negative Goodness: 0.0220\n",
            "Layer 1, Epoch [81/100], Step [351/469], Positive Goodness: 0.9777, Negative Goodness: 0.0168\n",
            "Layer 2, Epoch [81/100], Step [351/469], Positive Goodness: 0.9800, Negative Goodness: 0.0156\n",
            "Layer 3, Epoch [81/100], Step [351/469], Positive Goodness: 0.9657, Negative Goodness: 0.0414\n",
            "Final Layer, Epoch [81/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [81/100], Step [351/469], Loss: 0.1945, Test Accuracy: 0.9651\n",
            "Layer 0, Epoch [81/100], Step [468/469], Positive Goodness: 0.9758, Negative Goodness: 0.0271\n",
            "Layer 1, Epoch [81/100], Step [468/469], Positive Goodness: 0.9722, Negative Goodness: 0.0178\n",
            "Layer 2, Epoch [81/100], Step [468/469], Positive Goodness: 0.9754, Negative Goodness: 0.0177\n",
            "Layer 3, Epoch [81/100], Step [468/469], Positive Goodness: 0.9628, Negative Goodness: 0.0422\n",
            "Final Layer, Epoch [81/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [81/100], Step [468/469], Loss: 0.1960, Test Accuracy: 0.9651\n",
            "Layer 0, Epoch [82/100], Step [117/469], Positive Goodness: 0.9764, Negative Goodness: 0.0227\n",
            "Layer 1, Epoch [82/100], Step [117/469], Positive Goodness: 0.9759, Negative Goodness: 0.0158\n",
            "Layer 2, Epoch [82/100], Step [117/469], Positive Goodness: 0.9784, Negative Goodness: 0.0157\n",
            "Layer 3, Epoch [82/100], Step [117/469], Positive Goodness: 0.9641, Negative Goodness: 0.0404\n",
            "Final Layer, Epoch [82/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [82/100], Step [117/469], Loss: 0.1980, Test Accuracy: 0.9647\n",
            "Layer 0, Epoch [82/100], Step [234/469], Positive Goodness: 0.9758, Negative Goodness: 0.0243\n",
            "Layer 1, Epoch [82/100], Step [234/469], Positive Goodness: 0.9735, Negative Goodness: 0.0163\n",
            "Layer 2, Epoch [82/100], Step [234/469], Positive Goodness: 0.9773, Negative Goodness: 0.0165\n",
            "Layer 3, Epoch [82/100], Step [234/469], Positive Goodness: 0.9621, Negative Goodness: 0.0407\n",
            "Final Layer, Epoch [82/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [82/100], Step [234/469], Loss: 0.1955, Test Accuracy: 0.9645\n",
            "Layer 0, Epoch [82/100], Step [351/469], Positive Goodness: 0.9775, Negative Goodness: 0.0257\n",
            "Layer 1, Epoch [82/100], Step [351/469], Positive Goodness: 0.9741, Negative Goodness: 0.0172\n",
            "Layer 2, Epoch [82/100], Step [351/469], Positive Goodness: 0.9761, Negative Goodness: 0.0176\n",
            "Layer 3, Epoch [82/100], Step [351/469], Positive Goodness: 0.9645, Negative Goodness: 0.0414\n",
            "Final Layer, Epoch [82/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [82/100], Step [351/469], Loss: 0.1941, Test Accuracy: 0.9648\n",
            "Layer 0, Epoch [82/100], Step [468/469], Positive Goodness: 0.9761, Negative Goodness: 0.0230\n",
            "Layer 1, Epoch [82/100], Step [468/469], Positive Goodness: 0.9743, Negative Goodness: 0.0178\n",
            "Layer 2, Epoch [82/100], Step [468/469], Positive Goodness: 0.9787, Negative Goodness: 0.0162\n",
            "Layer 3, Epoch [82/100], Step [468/469], Positive Goodness: 0.9646, Negative Goodness: 0.0392\n",
            "Final Layer, Epoch [82/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [82/100], Step [468/469], Loss: 0.1905, Test Accuracy: 0.9655\n",
            "Layer 0, Epoch [83/100], Step [117/469], Positive Goodness: 0.9761, Negative Goodness: 0.0253\n",
            "Layer 1, Epoch [83/100], Step [117/469], Positive Goodness: 0.9757, Negative Goodness: 0.0166\n",
            "Layer 2, Epoch [83/100], Step [117/469], Positive Goodness: 0.9777, Negative Goodness: 0.0155\n",
            "Layer 3, Epoch [83/100], Step [117/469], Positive Goodness: 0.9625, Negative Goodness: 0.0419\n",
            "Final Layer, Epoch [83/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [83/100], Step [117/469], Loss: 0.1922, Test Accuracy: 0.9647\n",
            "Layer 0, Epoch [83/100], Step [234/469], Positive Goodness: 0.9774, Negative Goodness: 0.0229\n",
            "Layer 1, Epoch [83/100], Step [234/469], Positive Goodness: 0.9756, Negative Goodness: 0.0153\n",
            "Layer 2, Epoch [83/100], Step [234/469], Positive Goodness: 0.9786, Negative Goodness: 0.0150\n",
            "Layer 3, Epoch [83/100], Step [234/469], Positive Goodness: 0.9672, Negative Goodness: 0.0403\n",
            "Final Layer, Epoch [83/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [83/100], Step [234/469], Loss: 0.1938, Test Accuracy: 0.9643\n",
            "Layer 0, Epoch [83/100], Step [351/469], Positive Goodness: 0.9757, Negative Goodness: 0.0245\n",
            "Layer 1, Epoch [83/100], Step [351/469], Positive Goodness: 0.9744, Negative Goodness: 0.0179\n",
            "Layer 2, Epoch [83/100], Step [351/469], Positive Goodness: 0.9772, Negative Goodness: 0.0172\n",
            "Layer 3, Epoch [83/100], Step [351/469], Positive Goodness: 0.9641, Negative Goodness: 0.0424\n",
            "Final Layer, Epoch [83/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [83/100], Step [351/469], Loss: 0.1895, Test Accuracy: 0.9650\n",
            "Layer 0, Epoch [83/100], Step [468/469], Positive Goodness: 0.9777, Negative Goodness: 0.0236\n",
            "Layer 1, Epoch [83/100], Step [468/469], Positive Goodness: 0.9759, Negative Goodness: 0.0179\n",
            "Layer 2, Epoch [83/100], Step [468/469], Positive Goodness: 0.9769, Negative Goodness: 0.0162\n",
            "Layer 3, Epoch [83/100], Step [468/469], Positive Goodness: 0.9633, Negative Goodness: 0.0398\n",
            "Final Layer, Epoch [83/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [83/100], Step [468/469], Loss: 0.1929, Test Accuracy: 0.9660\n",
            "Layer 0, Epoch [84/100], Step [117/469], Positive Goodness: 0.9769, Negative Goodness: 0.0242\n",
            "Layer 1, Epoch [84/100], Step [117/469], Positive Goodness: 0.9739, Negative Goodness: 0.0155\n",
            "Layer 2, Epoch [84/100], Step [117/469], Positive Goodness: 0.9793, Negative Goodness: 0.0160\n",
            "Layer 3, Epoch [84/100], Step [117/469], Positive Goodness: 0.9651, Negative Goodness: 0.0404\n",
            "Final Layer, Epoch [84/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [84/100], Step [117/469], Loss: 0.1955, Test Accuracy: 0.9657\n",
            "Layer 0, Epoch [84/100], Step [234/469], Positive Goodness: 0.9770, Negative Goodness: 0.0240\n",
            "Layer 1, Epoch [84/100], Step [234/469], Positive Goodness: 0.9759, Negative Goodness: 0.0172\n",
            "Layer 2, Epoch [84/100], Step [234/469], Positive Goodness: 0.9778, Negative Goodness: 0.0159\n",
            "Layer 3, Epoch [84/100], Step [234/469], Positive Goodness: 0.9642, Negative Goodness: 0.0413\n",
            "Final Layer, Epoch [84/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [84/100], Step [234/469], Loss: 0.1979, Test Accuracy: 0.9654\n",
            "Layer 0, Epoch [84/100], Step [351/469], Positive Goodness: 0.9775, Negative Goodness: 0.0252\n",
            "Layer 1, Epoch [84/100], Step [351/469], Positive Goodness: 0.9771, Negative Goodness: 0.0174\n",
            "Layer 2, Epoch [84/100], Step [351/469], Positive Goodness: 0.9776, Negative Goodness: 0.0158\n",
            "Layer 3, Epoch [84/100], Step [351/469], Positive Goodness: 0.9632, Negative Goodness: 0.0402\n",
            "Final Layer, Epoch [84/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [84/100], Step [351/469], Loss: 0.1891, Test Accuracy: 0.9651\n",
            "Layer 0, Epoch [84/100], Step [468/469], Positive Goodness: 0.9757, Negative Goodness: 0.0241\n",
            "Layer 1, Epoch [84/100], Step [468/469], Positive Goodness: 0.9750, Negative Goodness: 0.0169\n",
            "Layer 2, Epoch [84/100], Step [468/469], Positive Goodness: 0.9771, Negative Goodness: 0.0169\n",
            "Layer 3, Epoch [84/100], Step [468/469], Positive Goodness: 0.9637, Negative Goodness: 0.0410\n",
            "Final Layer, Epoch [84/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [84/100], Step [468/469], Loss: 0.1910, Test Accuracy: 0.9654\n",
            "Layer 0, Epoch [85/100], Step [117/469], Positive Goodness: 0.9769, Negative Goodness: 0.0258\n",
            "Layer 1, Epoch [85/100], Step [117/469], Positive Goodness: 0.9748, Negative Goodness: 0.0176\n",
            "Layer 2, Epoch [85/100], Step [117/469], Positive Goodness: 0.9784, Negative Goodness: 0.0174\n",
            "Layer 3, Epoch [85/100], Step [117/469], Positive Goodness: 0.9648, Negative Goodness: 0.0400\n",
            "Final Layer, Epoch [85/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [85/100], Step [117/469], Loss: 0.1946, Test Accuracy: 0.9651\n",
            "Layer 0, Epoch [85/100], Step [234/469], Positive Goodness: 0.9765, Negative Goodness: 0.0248\n",
            "Layer 1, Epoch [85/100], Step [234/469], Positive Goodness: 0.9758, Negative Goodness: 0.0167\n",
            "Layer 2, Epoch [85/100], Step [234/469], Positive Goodness: 0.9780, Negative Goodness: 0.0165\n",
            "Layer 3, Epoch [85/100], Step [234/469], Positive Goodness: 0.9630, Negative Goodness: 0.0395\n",
            "Final Layer, Epoch [85/100], Step [234/469], Loss: 0.0002\n",
            "Epoch [85/100], Step [234/469], Loss: 0.1912, Test Accuracy: 0.9648\n",
            "Layer 0, Epoch [85/100], Step [351/469], Positive Goodness: 0.9770, Negative Goodness: 0.0237\n",
            "Layer 1, Epoch [85/100], Step [351/469], Positive Goodness: 0.9754, Negative Goodness: 0.0176\n",
            "Layer 2, Epoch [85/100], Step [351/469], Positive Goodness: 0.9789, Negative Goodness: 0.0172\n",
            "Layer 3, Epoch [85/100], Step [351/469], Positive Goodness: 0.9638, Negative Goodness: 0.0409\n",
            "Final Layer, Epoch [85/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [85/100], Step [351/469], Loss: 0.1965, Test Accuracy: 0.9657\n",
            "Layer 0, Epoch [85/100], Step [468/469], Positive Goodness: 0.9772, Negative Goodness: 0.0221\n",
            "Layer 1, Epoch [85/100], Step [468/469], Positive Goodness: 0.9764, Negative Goodness: 0.0149\n",
            "Layer 2, Epoch [85/100], Step [468/469], Positive Goodness: 0.9782, Negative Goodness: 0.0141\n",
            "Layer 3, Epoch [85/100], Step [468/469], Positive Goodness: 0.9654, Negative Goodness: 0.0376\n",
            "Final Layer, Epoch [85/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [85/100], Step [468/469], Loss: 0.1930, Test Accuracy: 0.9656\n",
            "Layer 0, Epoch [86/100], Step [117/469], Positive Goodness: 0.9750, Negative Goodness: 0.0237\n",
            "Layer 1, Epoch [86/100], Step [117/469], Positive Goodness: 0.9744, Negative Goodness: 0.0158\n",
            "Layer 2, Epoch [86/100], Step [117/469], Positive Goodness: 0.9781, Negative Goodness: 0.0177\n",
            "Layer 3, Epoch [86/100], Step [117/469], Positive Goodness: 0.9648, Negative Goodness: 0.0393\n",
            "Final Layer, Epoch [86/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [86/100], Step [117/469], Loss: 0.1956, Test Accuracy: 0.9654\n",
            "Layer 0, Epoch [86/100], Step [234/469], Positive Goodness: 0.9771, Negative Goodness: 0.0246\n",
            "Layer 1, Epoch [86/100], Step [234/469], Positive Goodness: 0.9757, Negative Goodness: 0.0154\n",
            "Layer 2, Epoch [86/100], Step [234/469], Positive Goodness: 0.9797, Negative Goodness: 0.0157\n",
            "Layer 3, Epoch [86/100], Step [234/469], Positive Goodness: 0.9637, Negative Goodness: 0.0397\n",
            "Final Layer, Epoch [86/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [86/100], Step [234/469], Loss: 0.1947, Test Accuracy: 0.9655\n",
            "Layer 0, Epoch [86/100], Step [351/469], Positive Goodness: 0.9781, Negative Goodness: 0.0239\n",
            "Layer 1, Epoch [86/100], Step [351/469], Positive Goodness: 0.9766, Negative Goodness: 0.0161\n",
            "Layer 2, Epoch [86/100], Step [351/469], Positive Goodness: 0.9783, Negative Goodness: 0.0149\n",
            "Layer 3, Epoch [86/100], Step [351/469], Positive Goodness: 0.9631, Negative Goodness: 0.0376\n",
            "Final Layer, Epoch [86/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [86/100], Step [351/469], Loss: 0.1939, Test Accuracy: 0.9649\n",
            "Layer 0, Epoch [86/100], Step [468/469], Positive Goodness: 0.9775, Negative Goodness: 0.0245\n",
            "Layer 1, Epoch [86/100], Step [468/469], Positive Goodness: 0.9767, Negative Goodness: 0.0162\n",
            "Layer 2, Epoch [86/100], Step [468/469], Positive Goodness: 0.9798, Negative Goodness: 0.0155\n",
            "Layer 3, Epoch [86/100], Step [468/469], Positive Goodness: 0.9669, Negative Goodness: 0.0393\n",
            "Final Layer, Epoch [86/100], Step [468/469], Loss: 0.0002\n",
            "Epoch [86/100], Step [468/469], Loss: 0.1973, Test Accuracy: 0.9658\n",
            "Layer 0, Epoch [87/100], Step [117/469], Positive Goodness: 0.9752, Negative Goodness: 0.0237\n",
            "Layer 1, Epoch [87/100], Step [117/469], Positive Goodness: 0.9728, Negative Goodness: 0.0182\n",
            "Layer 2, Epoch [87/100], Step [117/469], Positive Goodness: 0.9771, Negative Goodness: 0.0155\n",
            "Layer 3, Epoch [87/100], Step [117/469], Positive Goodness: 0.9610, Negative Goodness: 0.0397\n",
            "Final Layer, Epoch [87/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [87/100], Step [117/469], Loss: 0.1937, Test Accuracy: 0.9652\n",
            "Layer 0, Epoch [87/100], Step [234/469], Positive Goodness: 0.9755, Negative Goodness: 0.0236\n",
            "Layer 1, Epoch [87/100], Step [234/469], Positive Goodness: 0.9765, Negative Goodness: 0.0158\n",
            "Layer 2, Epoch [87/100], Step [234/469], Positive Goodness: 0.9791, Negative Goodness: 0.0141\n",
            "Layer 3, Epoch [87/100], Step [234/469], Positive Goodness: 0.9651, Negative Goodness: 0.0369\n",
            "Final Layer, Epoch [87/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [87/100], Step [234/469], Loss: 0.1928, Test Accuracy: 0.9654\n",
            "Layer 0, Epoch [87/100], Step [351/469], Positive Goodness: 0.9790, Negative Goodness: 0.0245\n",
            "Layer 1, Epoch [87/100], Step [351/469], Positive Goodness: 0.9770, Negative Goodness: 0.0154\n",
            "Layer 2, Epoch [87/100], Step [351/469], Positive Goodness: 0.9794, Negative Goodness: 0.0159\n",
            "Layer 3, Epoch [87/100], Step [351/469], Positive Goodness: 0.9647, Negative Goodness: 0.0383\n",
            "Final Layer, Epoch [87/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [87/100], Step [351/469], Loss: 0.1929, Test Accuracy: 0.9655\n",
            "Layer 0, Epoch [87/100], Step [468/469], Positive Goodness: 0.9783, Negative Goodness: 0.0230\n",
            "Layer 1, Epoch [87/100], Step [468/469], Positive Goodness: 0.9777, Negative Goodness: 0.0158\n",
            "Layer 2, Epoch [87/100], Step [468/469], Positive Goodness: 0.9794, Negative Goodness: 0.0148\n",
            "Layer 3, Epoch [87/100], Step [468/469], Positive Goodness: 0.9672, Negative Goodness: 0.0405\n",
            "Final Layer, Epoch [87/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [87/100], Step [468/469], Loss: 0.1933, Test Accuracy: 0.9658\n",
            "Layer 0, Epoch [88/100], Step [117/469], Positive Goodness: 0.9761, Negative Goodness: 0.0222\n",
            "Layer 1, Epoch [88/100], Step [117/469], Positive Goodness: 0.9750, Negative Goodness: 0.0156\n",
            "Layer 2, Epoch [88/100], Step [117/469], Positive Goodness: 0.9791, Negative Goodness: 0.0154\n",
            "Layer 3, Epoch [88/100], Step [117/469], Positive Goodness: 0.9663, Negative Goodness: 0.0400\n",
            "Final Layer, Epoch [88/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [88/100], Step [117/469], Loss: 0.1980, Test Accuracy: 0.9657\n",
            "Layer 0, Epoch [88/100], Step [234/469], Positive Goodness: 0.9780, Negative Goodness: 0.0220\n",
            "Layer 1, Epoch [88/100], Step [234/469], Positive Goodness: 0.9786, Negative Goodness: 0.0162\n",
            "Layer 2, Epoch [88/100], Step [234/469], Positive Goodness: 0.9822, Negative Goodness: 0.0154\n",
            "Layer 3, Epoch [88/100], Step [234/469], Positive Goodness: 0.9662, Negative Goodness: 0.0388\n",
            "Final Layer, Epoch [88/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [88/100], Step [234/469], Loss: 0.1911, Test Accuracy: 0.9655\n",
            "Layer 0, Epoch [88/100], Step [351/469], Positive Goodness: 0.9782, Negative Goodness: 0.0239\n",
            "Layer 1, Epoch [88/100], Step [351/469], Positive Goodness: 0.9762, Negative Goodness: 0.0154\n",
            "Layer 2, Epoch [88/100], Step [351/469], Positive Goodness: 0.9768, Negative Goodness: 0.0152\n",
            "Layer 3, Epoch [88/100], Step [351/469], Positive Goodness: 0.9628, Negative Goodness: 0.0388\n",
            "Final Layer, Epoch [88/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [88/100], Step [351/469], Loss: 0.1915, Test Accuracy: 0.9657\n",
            "Layer 0, Epoch [88/100], Step [468/469], Positive Goodness: 0.9760, Negative Goodness: 0.0238\n",
            "Layer 1, Epoch [88/100], Step [468/469], Positive Goodness: 0.9745, Negative Goodness: 0.0162\n",
            "Layer 2, Epoch [88/100], Step [468/469], Positive Goodness: 0.9773, Negative Goodness: 0.0150\n",
            "Layer 3, Epoch [88/100], Step [468/469], Positive Goodness: 0.9641, Negative Goodness: 0.0366\n",
            "Final Layer, Epoch [88/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [88/100], Step [468/469], Loss: 0.1945, Test Accuracy: 0.9654\n",
            "Layer 0, Epoch [89/100], Step [117/469], Positive Goodness: 0.9775, Negative Goodness: 0.0236\n",
            "Layer 1, Epoch [89/100], Step [117/469], Positive Goodness: 0.9763, Negative Goodness: 0.0146\n",
            "Layer 2, Epoch [89/100], Step [117/469], Positive Goodness: 0.9790, Negative Goodness: 0.0141\n",
            "Layer 3, Epoch [89/100], Step [117/469], Positive Goodness: 0.9666, Negative Goodness: 0.0366\n",
            "Final Layer, Epoch [89/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [89/100], Step [117/469], Loss: 0.1962, Test Accuracy: 0.9659\n",
            "Layer 0, Epoch [89/100], Step [234/469], Positive Goodness: 0.9777, Negative Goodness: 0.0231\n",
            "Layer 1, Epoch [89/100], Step [234/469], Positive Goodness: 0.9761, Negative Goodness: 0.0161\n",
            "Layer 2, Epoch [89/100], Step [234/469], Positive Goodness: 0.9797, Negative Goodness: 0.0148\n",
            "Layer 3, Epoch [89/100], Step [234/469], Positive Goodness: 0.9662, Negative Goodness: 0.0378\n",
            "Final Layer, Epoch [89/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [89/100], Step [234/469], Loss: 0.1924, Test Accuracy: 0.9659\n",
            "Layer 0, Epoch [89/100], Step [351/469], Positive Goodness: 0.9757, Negative Goodness: 0.0227\n",
            "Layer 1, Epoch [89/100], Step [351/469], Positive Goodness: 0.9755, Negative Goodness: 0.0164\n",
            "Layer 2, Epoch [89/100], Step [351/469], Positive Goodness: 0.9780, Negative Goodness: 0.0155\n",
            "Layer 3, Epoch [89/100], Step [351/469], Positive Goodness: 0.9629, Negative Goodness: 0.0407\n",
            "Final Layer, Epoch [89/100], Step [351/469], Loss: 0.0002\n",
            "Epoch [89/100], Step [351/469], Loss: 0.1944, Test Accuracy: 0.9666\n",
            "Layer 0, Epoch [89/100], Step [468/469], Positive Goodness: 0.9778, Negative Goodness: 0.0244\n",
            "Layer 1, Epoch [89/100], Step [468/469], Positive Goodness: 0.9770, Negative Goodness: 0.0157\n",
            "Layer 2, Epoch [89/100], Step [468/469], Positive Goodness: 0.9796, Negative Goodness: 0.0156\n",
            "Layer 3, Epoch [89/100], Step [468/469], Positive Goodness: 0.9646, Negative Goodness: 0.0397\n",
            "Final Layer, Epoch [89/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [89/100], Step [468/469], Loss: 0.1953, Test Accuracy: 0.9661\n",
            "Layer 0, Epoch [90/100], Step [117/469], Positive Goodness: 0.9744, Negative Goodness: 0.0218\n",
            "Layer 1, Epoch [90/100], Step [117/469], Positive Goodness: 0.9745, Negative Goodness: 0.0142\n",
            "Layer 2, Epoch [90/100], Step [117/469], Positive Goodness: 0.9777, Negative Goodness: 0.0164\n",
            "Layer 3, Epoch [90/100], Step [117/469], Positive Goodness: 0.9641, Negative Goodness: 0.0380\n",
            "Final Layer, Epoch [90/100], Step [117/469], Loss: 0.0002\n",
            "Epoch [90/100], Step [117/469], Loss: 0.1980, Test Accuracy: 0.9661\n",
            "Layer 0, Epoch [90/100], Step [234/469], Positive Goodness: 0.9801, Negative Goodness: 0.0219\n",
            "Layer 1, Epoch [90/100], Step [234/469], Positive Goodness: 0.9786, Negative Goodness: 0.0148\n",
            "Layer 2, Epoch [90/100], Step [234/469], Positive Goodness: 0.9823, Negative Goodness: 0.0129\n",
            "Layer 3, Epoch [90/100], Step [234/469], Positive Goodness: 0.9678, Negative Goodness: 0.0374\n",
            "Final Layer, Epoch [90/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [90/100], Step [234/469], Loss: 0.1956, Test Accuracy: 0.9658\n",
            "Layer 0, Epoch [90/100], Step [351/469], Positive Goodness: 0.9767, Negative Goodness: 0.0237\n",
            "Layer 1, Epoch [90/100], Step [351/469], Positive Goodness: 0.9744, Negative Goodness: 0.0160\n",
            "Layer 2, Epoch [90/100], Step [351/469], Positive Goodness: 0.9770, Negative Goodness: 0.0136\n",
            "Layer 3, Epoch [90/100], Step [351/469], Positive Goodness: 0.9628, Negative Goodness: 0.0377\n",
            "Final Layer, Epoch [90/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [90/100], Step [351/469], Loss: 0.1938, Test Accuracy: 0.9655\n",
            "Layer 0, Epoch [90/100], Step [468/469], Positive Goodness: 0.9783, Negative Goodness: 0.0222\n",
            "Layer 1, Epoch [90/100], Step [468/469], Positive Goodness: 0.9776, Negative Goodness: 0.0153\n",
            "Layer 2, Epoch [90/100], Step [468/469], Positive Goodness: 0.9797, Negative Goodness: 0.0161\n",
            "Layer 3, Epoch [90/100], Step [468/469], Positive Goodness: 0.9661, Negative Goodness: 0.0380\n",
            "Final Layer, Epoch [90/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [90/100], Step [468/469], Loss: 0.1943, Test Accuracy: 0.9654\n",
            "Layer 0, Epoch [91/100], Step [117/469], Positive Goodness: 0.9792, Negative Goodness: 0.0234\n",
            "Layer 1, Epoch [91/100], Step [117/469], Positive Goodness: 0.9783, Negative Goodness: 0.0164\n",
            "Layer 2, Epoch [91/100], Step [117/469], Positive Goodness: 0.9805, Negative Goodness: 0.0160\n",
            "Layer 3, Epoch [91/100], Step [117/469], Positive Goodness: 0.9661, Negative Goodness: 0.0389\n",
            "Final Layer, Epoch [91/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [91/100], Step [117/469], Loss: 0.1911, Test Accuracy: 0.9660\n",
            "Layer 0, Epoch [91/100], Step [234/469], Positive Goodness: 0.9772, Negative Goodness: 0.0233\n",
            "Layer 1, Epoch [91/100], Step [234/469], Positive Goodness: 0.9752, Negative Goodness: 0.0142\n",
            "Layer 2, Epoch [91/100], Step [234/469], Positive Goodness: 0.9788, Negative Goodness: 0.0141\n",
            "Layer 3, Epoch [91/100], Step [234/469], Positive Goodness: 0.9647, Negative Goodness: 0.0366\n",
            "Final Layer, Epoch [91/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [91/100], Step [234/469], Loss: 0.1931, Test Accuracy: 0.9657\n",
            "Layer 0, Epoch [91/100], Step [351/469], Positive Goodness: 0.9767, Negative Goodness: 0.0207\n",
            "Layer 1, Epoch [91/100], Step [351/469], Positive Goodness: 0.9756, Negative Goodness: 0.0146\n",
            "Layer 2, Epoch [91/100], Step [351/469], Positive Goodness: 0.9776, Negative Goodness: 0.0131\n",
            "Layer 3, Epoch [91/100], Step [351/469], Positive Goodness: 0.9642, Negative Goodness: 0.0369\n",
            "Final Layer, Epoch [91/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [91/100], Step [351/469], Loss: 0.1942, Test Accuracy: 0.9659\n",
            "Layer 0, Epoch [91/100], Step [468/469], Positive Goodness: 0.9765, Negative Goodness: 0.0235\n",
            "Layer 1, Epoch [91/100], Step [468/469], Positive Goodness: 0.9768, Negative Goodness: 0.0154\n",
            "Layer 2, Epoch [91/100], Step [468/469], Positive Goodness: 0.9803, Negative Goodness: 0.0151\n",
            "Layer 3, Epoch [91/100], Step [468/469], Positive Goodness: 0.9658, Negative Goodness: 0.0383\n",
            "Final Layer, Epoch [91/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [91/100], Step [468/469], Loss: 0.1951, Test Accuracy: 0.9661\n",
            "Layer 0, Epoch [92/100], Step [117/469], Positive Goodness: 0.9772, Negative Goodness: 0.0228\n",
            "Layer 1, Epoch [92/100], Step [117/469], Positive Goodness: 0.9762, Negative Goodness: 0.0151\n",
            "Layer 2, Epoch [92/100], Step [117/469], Positive Goodness: 0.9785, Negative Goodness: 0.0135\n",
            "Layer 3, Epoch [92/100], Step [117/469], Positive Goodness: 0.9651, Negative Goodness: 0.0386\n",
            "Final Layer, Epoch [92/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [92/100], Step [117/469], Loss: 0.1950, Test Accuracy: 0.9660\n",
            "Layer 0, Epoch [92/100], Step [234/469], Positive Goodness: 0.9780, Negative Goodness: 0.0228\n",
            "Layer 1, Epoch [92/100], Step [234/469], Positive Goodness: 0.9767, Negative Goodness: 0.0145\n",
            "Layer 2, Epoch [92/100], Step [234/469], Positive Goodness: 0.9784, Negative Goodness: 0.0136\n",
            "Layer 3, Epoch [92/100], Step [234/469], Positive Goodness: 0.9657, Negative Goodness: 0.0393\n",
            "Final Layer, Epoch [92/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [92/100], Step [234/469], Loss: 0.1985, Test Accuracy: 0.9658\n",
            "Layer 0, Epoch [92/100], Step [351/469], Positive Goodness: 0.9773, Negative Goodness: 0.0227\n",
            "Layer 1, Epoch [92/100], Step [351/469], Positive Goodness: 0.9778, Negative Goodness: 0.0162\n",
            "Layer 2, Epoch [92/100], Step [351/469], Positive Goodness: 0.9809, Negative Goodness: 0.0151\n",
            "Layer 3, Epoch [92/100], Step [351/469], Positive Goodness: 0.9657, Negative Goodness: 0.0395\n",
            "Final Layer, Epoch [92/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [92/100], Step [351/469], Loss: 0.1987, Test Accuracy: 0.9658\n",
            "Layer 0, Epoch [92/100], Step [468/469], Positive Goodness: 0.9769, Negative Goodness: 0.0223\n",
            "Layer 1, Epoch [92/100], Step [468/469], Positive Goodness: 0.9750, Negative Goodness: 0.0155\n",
            "Layer 2, Epoch [92/100], Step [468/469], Positive Goodness: 0.9791, Negative Goodness: 0.0145\n",
            "Layer 3, Epoch [92/100], Step [468/469], Positive Goodness: 0.9647, Negative Goodness: 0.0361\n",
            "Final Layer, Epoch [92/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [92/100], Step [468/469], Loss: 0.1961, Test Accuracy: 0.9660\n",
            "Layer 0, Epoch [93/100], Step [117/469], Positive Goodness: 0.9771, Negative Goodness: 0.0235\n",
            "Layer 1, Epoch [93/100], Step [117/469], Positive Goodness: 0.9755, Negative Goodness: 0.0153\n",
            "Layer 2, Epoch [93/100], Step [117/469], Positive Goodness: 0.9785, Negative Goodness: 0.0141\n",
            "Layer 3, Epoch [93/100], Step [117/469], Positive Goodness: 0.9637, Negative Goodness: 0.0369\n",
            "Final Layer, Epoch [93/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [93/100], Step [117/469], Loss: 0.1966, Test Accuracy: 0.9658\n",
            "Layer 0, Epoch [93/100], Step [234/469], Positive Goodness: 0.9779, Negative Goodness: 0.0214\n",
            "Layer 1, Epoch [93/100], Step [234/469], Positive Goodness: 0.9776, Negative Goodness: 0.0143\n",
            "Layer 2, Epoch [93/100], Step [234/469], Positive Goodness: 0.9800, Negative Goodness: 0.0141\n",
            "Layer 3, Epoch [93/100], Step [234/469], Positive Goodness: 0.9647, Negative Goodness: 0.0358\n",
            "Final Layer, Epoch [93/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [93/100], Step [234/469], Loss: 0.1941, Test Accuracy: 0.9659\n",
            "Layer 0, Epoch [93/100], Step [351/469], Positive Goodness: 0.9777, Negative Goodness: 0.0233\n",
            "Layer 1, Epoch [93/100], Step [351/469], Positive Goodness: 0.9759, Negative Goodness: 0.0163\n",
            "Layer 2, Epoch [93/100], Step [351/469], Positive Goodness: 0.9801, Negative Goodness: 0.0157\n",
            "Layer 3, Epoch [93/100], Step [351/469], Positive Goodness: 0.9669, Negative Goodness: 0.0406\n",
            "Final Layer, Epoch [93/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [93/100], Step [351/469], Loss: 0.1959, Test Accuracy: 0.9659\n",
            "Layer 0, Epoch [93/100], Step [468/469], Positive Goodness: 0.9772, Negative Goodness: 0.0213\n",
            "Layer 1, Epoch [93/100], Step [468/469], Positive Goodness: 0.9771, Negative Goodness: 0.0144\n",
            "Layer 2, Epoch [93/100], Step [468/469], Positive Goodness: 0.9788, Negative Goodness: 0.0134\n",
            "Layer 3, Epoch [93/100], Step [468/469], Positive Goodness: 0.9665, Negative Goodness: 0.0372\n",
            "Final Layer, Epoch [93/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [93/100], Step [468/469], Loss: 0.1943, Test Accuracy: 0.9664\n",
            "Layer 0, Epoch [94/100], Step [117/469], Positive Goodness: 0.9778, Negative Goodness: 0.0218\n",
            "Layer 1, Epoch [94/100], Step [117/469], Positive Goodness: 0.9770, Negative Goodness: 0.0154\n",
            "Layer 2, Epoch [94/100], Step [117/469], Positive Goodness: 0.9792, Negative Goodness: 0.0140\n",
            "Layer 3, Epoch [94/100], Step [117/469], Positive Goodness: 0.9647, Negative Goodness: 0.0356\n",
            "Final Layer, Epoch [94/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [94/100], Step [117/469], Loss: 0.1951, Test Accuracy: 0.9659\n",
            "Layer 0, Epoch [94/100], Step [234/469], Positive Goodness: 0.9770, Negative Goodness: 0.0235\n",
            "Layer 1, Epoch [94/100], Step [234/469], Positive Goodness: 0.9758, Negative Goodness: 0.0143\n",
            "Layer 2, Epoch [94/100], Step [234/469], Positive Goodness: 0.9803, Negative Goodness: 0.0140\n",
            "Layer 3, Epoch [94/100], Step [234/469], Positive Goodness: 0.9667, Negative Goodness: 0.0383\n",
            "Final Layer, Epoch [94/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [94/100], Step [234/469], Loss: 0.1952, Test Accuracy: 0.9661\n",
            "Layer 0, Epoch [94/100], Step [351/469], Positive Goodness: 0.9777, Negative Goodness: 0.0227\n",
            "Layer 1, Epoch [94/100], Step [351/469], Positive Goodness: 0.9780, Negative Goodness: 0.0158\n",
            "Layer 2, Epoch [94/100], Step [351/469], Positive Goodness: 0.9798, Negative Goodness: 0.0141\n",
            "Layer 3, Epoch [94/100], Step [351/469], Positive Goodness: 0.9661, Negative Goodness: 0.0380\n",
            "Final Layer, Epoch [94/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [94/100], Step [351/469], Loss: 0.1973, Test Accuracy: 0.9663\n",
            "Layer 0, Epoch [94/100], Step [468/469], Positive Goodness: 0.9776, Negative Goodness: 0.0206\n",
            "Layer 1, Epoch [94/100], Step [468/469], Positive Goodness: 0.9757, Negative Goodness: 0.0142\n",
            "Layer 2, Epoch [94/100], Step [468/469], Positive Goodness: 0.9791, Negative Goodness: 0.0137\n",
            "Layer 3, Epoch [94/100], Step [468/469], Positive Goodness: 0.9656, Negative Goodness: 0.0360\n",
            "Final Layer, Epoch [94/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [94/100], Step [468/469], Loss: 0.1953, Test Accuracy: 0.9662\n",
            "Layer 0, Epoch [95/100], Step [117/469], Positive Goodness: 0.9784, Negative Goodness: 0.0228\n",
            "Layer 1, Epoch [95/100], Step [117/469], Positive Goodness: 0.9773, Negative Goodness: 0.0157\n",
            "Layer 2, Epoch [95/100], Step [117/469], Positive Goodness: 0.9812, Negative Goodness: 0.0151\n",
            "Layer 3, Epoch [95/100], Step [117/469], Positive Goodness: 0.9665, Negative Goodness: 0.0382\n",
            "Final Layer, Epoch [95/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [95/100], Step [117/469], Loss: 0.1954, Test Accuracy: 0.9663\n",
            "Layer 0, Epoch [95/100], Step [234/469], Positive Goodness: 0.9781, Negative Goodness: 0.0218\n",
            "Layer 1, Epoch [95/100], Step [234/469], Positive Goodness: 0.9744, Negative Goodness: 0.0145\n",
            "Layer 2, Epoch [95/100], Step [234/469], Positive Goodness: 0.9787, Negative Goodness: 0.0126\n",
            "Layer 3, Epoch [95/100], Step [234/469], Positive Goodness: 0.9647, Negative Goodness: 0.0365\n",
            "Final Layer, Epoch [95/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [95/100], Step [234/469], Loss: 0.1979, Test Accuracy: 0.9665\n",
            "Layer 0, Epoch [95/100], Step [351/469], Positive Goodness: 0.9781, Negative Goodness: 0.0218\n",
            "Layer 1, Epoch [95/100], Step [351/469], Positive Goodness: 0.9789, Negative Goodness: 0.0141\n",
            "Layer 2, Epoch [95/100], Step [351/469], Positive Goodness: 0.9799, Negative Goodness: 0.0132\n",
            "Layer 3, Epoch [95/100], Step [351/469], Positive Goodness: 0.9675, Negative Goodness: 0.0365\n",
            "Final Layer, Epoch [95/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [95/100], Step [351/469], Loss: 0.1962, Test Accuracy: 0.9663\n",
            "Layer 0, Epoch [95/100], Step [468/469], Positive Goodness: 0.9757, Negative Goodness: 0.0223\n",
            "Layer 1, Epoch [95/100], Step [468/469], Positive Goodness: 0.9759, Negative Goodness: 0.0138\n",
            "Layer 2, Epoch [95/100], Step [468/469], Positive Goodness: 0.9786, Negative Goodness: 0.0140\n",
            "Layer 3, Epoch [95/100], Step [468/469], Positive Goodness: 0.9644, Negative Goodness: 0.0392\n",
            "Final Layer, Epoch [95/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [95/100], Step [468/469], Loss: 0.1958, Test Accuracy: 0.9662\n",
            "Layer 0, Epoch [96/100], Step [117/469], Positive Goodness: 0.9766, Negative Goodness: 0.0230\n",
            "Layer 1, Epoch [96/100], Step [117/469], Positive Goodness: 0.9776, Negative Goodness: 0.0153\n",
            "Layer 2, Epoch [96/100], Step [117/469], Positive Goodness: 0.9801, Negative Goodness: 0.0126\n",
            "Layer 3, Epoch [96/100], Step [117/469], Positive Goodness: 0.9642, Negative Goodness: 0.0361\n",
            "Final Layer, Epoch [96/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [96/100], Step [117/469], Loss: 0.1974, Test Accuracy: 0.9656\n",
            "Layer 0, Epoch [96/100], Step [234/469], Positive Goodness: 0.9772, Negative Goodness: 0.0224\n",
            "Layer 1, Epoch [96/100], Step [234/469], Positive Goodness: 0.9786, Negative Goodness: 0.0159\n",
            "Layer 2, Epoch [96/100], Step [234/469], Positive Goodness: 0.9792, Negative Goodness: 0.0153\n",
            "Layer 3, Epoch [96/100], Step [234/469], Positive Goodness: 0.9658, Negative Goodness: 0.0382\n",
            "Final Layer, Epoch [96/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [96/100], Step [234/469], Loss: 0.1964, Test Accuracy: 0.9664\n",
            "Layer 0, Epoch [96/100], Step [351/469], Positive Goodness: 0.9786, Negative Goodness: 0.0213\n",
            "Layer 1, Epoch [96/100], Step [351/469], Positive Goodness: 0.9765, Negative Goodness: 0.0145\n",
            "Layer 2, Epoch [96/100], Step [351/469], Positive Goodness: 0.9806, Negative Goodness: 0.0130\n",
            "Layer 3, Epoch [96/100], Step [351/469], Positive Goodness: 0.9664, Negative Goodness: 0.0364\n",
            "Final Layer, Epoch [96/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [96/100], Step [351/469], Loss: 0.1958, Test Accuracy: 0.9660\n",
            "Layer 0, Epoch [96/100], Step [468/469], Positive Goodness: 0.9778, Negative Goodness: 0.0215\n",
            "Layer 1, Epoch [96/100], Step [468/469], Positive Goodness: 0.9742, Negative Goodness: 0.0146\n",
            "Layer 2, Epoch [96/100], Step [468/469], Positive Goodness: 0.9787, Negative Goodness: 0.0135\n",
            "Layer 3, Epoch [96/100], Step [468/469], Positive Goodness: 0.9664, Negative Goodness: 0.0366\n",
            "Final Layer, Epoch [96/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [96/100], Step [468/469], Loss: 0.1961, Test Accuracy: 0.9663\n",
            "Layer 0, Epoch [97/100], Step [117/469], Positive Goodness: 0.9779, Negative Goodness: 0.0197\n",
            "Layer 1, Epoch [97/100], Step [117/469], Positive Goodness: 0.9767, Negative Goodness: 0.0134\n",
            "Layer 2, Epoch [97/100], Step [117/469], Positive Goodness: 0.9799, Negative Goodness: 0.0119\n",
            "Layer 3, Epoch [97/100], Step [117/469], Positive Goodness: 0.9659, Negative Goodness: 0.0324\n",
            "Final Layer, Epoch [97/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [97/100], Step [117/469], Loss: 0.1971, Test Accuracy: 0.9661\n",
            "Layer 0, Epoch [97/100], Step [234/469], Positive Goodness: 0.9775, Negative Goodness: 0.0206\n",
            "Layer 1, Epoch [97/100], Step [234/469], Positive Goodness: 0.9756, Negative Goodness: 0.0143\n",
            "Layer 2, Epoch [97/100], Step [234/469], Positive Goodness: 0.9794, Negative Goodness: 0.0119\n",
            "Layer 3, Epoch [97/100], Step [234/469], Positive Goodness: 0.9659, Negative Goodness: 0.0385\n",
            "Final Layer, Epoch [97/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [97/100], Step [234/469], Loss: 0.1956, Test Accuracy: 0.9663\n",
            "Layer 0, Epoch [97/100], Step [351/469], Positive Goodness: 0.9775, Negative Goodness: 0.0217\n",
            "Layer 1, Epoch [97/100], Step [351/469], Positive Goodness: 0.9763, Negative Goodness: 0.0150\n",
            "Layer 2, Epoch [97/100], Step [351/469], Positive Goodness: 0.9795, Negative Goodness: 0.0152\n",
            "Layer 3, Epoch [97/100], Step [351/469], Positive Goodness: 0.9664, Negative Goodness: 0.0377\n",
            "Final Layer, Epoch [97/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [97/100], Step [351/469], Loss: 0.1967, Test Accuracy: 0.9660\n",
            "Layer 0, Epoch [97/100], Step [468/469], Positive Goodness: 0.9779, Negative Goodness: 0.0233\n",
            "Layer 1, Epoch [97/100], Step [468/469], Positive Goodness: 0.9783, Negative Goodness: 0.0153\n",
            "Layer 2, Epoch [97/100], Step [468/469], Positive Goodness: 0.9799, Negative Goodness: 0.0134\n",
            "Layer 3, Epoch [97/100], Step [468/469], Positive Goodness: 0.9654, Negative Goodness: 0.0363\n",
            "Final Layer, Epoch [97/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [97/100], Step [468/469], Loss: 0.1954, Test Accuracy: 0.9664\n",
            "Layer 0, Epoch [98/100], Step [117/469], Positive Goodness: 0.9779, Negative Goodness: 0.0209\n",
            "Layer 1, Epoch [98/100], Step [117/469], Positive Goodness: 0.9750, Negative Goodness: 0.0121\n",
            "Layer 2, Epoch [98/100], Step [117/469], Positive Goodness: 0.9791, Negative Goodness: 0.0129\n",
            "Layer 3, Epoch [98/100], Step [117/469], Positive Goodness: 0.9635, Negative Goodness: 0.0350\n",
            "Final Layer, Epoch [98/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [98/100], Step [117/469], Loss: 0.1963, Test Accuracy: 0.9664\n",
            "Layer 0, Epoch [98/100], Step [234/469], Positive Goodness: 0.9782, Negative Goodness: 0.0219\n",
            "Layer 1, Epoch [98/100], Step [234/469], Positive Goodness: 0.9781, Negative Goodness: 0.0142\n",
            "Layer 2, Epoch [98/100], Step [234/469], Positive Goodness: 0.9805, Negative Goodness: 0.0118\n",
            "Layer 3, Epoch [98/100], Step [234/469], Positive Goodness: 0.9680, Negative Goodness: 0.0336\n",
            "Final Layer, Epoch [98/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [98/100], Step [234/469], Loss: 0.1964, Test Accuracy: 0.9659\n",
            "Layer 0, Epoch [98/100], Step [351/469], Positive Goodness: 0.9777, Negative Goodness: 0.0217\n",
            "Layer 1, Epoch [98/100], Step [351/469], Positive Goodness: 0.9766, Negative Goodness: 0.0141\n",
            "Layer 2, Epoch [98/100], Step [351/469], Positive Goodness: 0.9800, Negative Goodness: 0.0136\n",
            "Layer 3, Epoch [98/100], Step [351/469], Positive Goodness: 0.9651, Negative Goodness: 0.0360\n",
            "Final Layer, Epoch [98/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [98/100], Step [351/469], Loss: 0.1968, Test Accuracy: 0.9661\n",
            "Layer 0, Epoch [98/100], Step [468/469], Positive Goodness: 0.9771, Negative Goodness: 0.0236\n",
            "Layer 1, Epoch [98/100], Step [468/469], Positive Goodness: 0.9774, Negative Goodness: 0.0167\n",
            "Layer 2, Epoch [98/100], Step [468/469], Positive Goodness: 0.9792, Negative Goodness: 0.0158\n",
            "Layer 3, Epoch [98/100], Step [468/469], Positive Goodness: 0.9670, Negative Goodness: 0.0398\n",
            "Final Layer, Epoch [98/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [98/100], Step [468/469], Loss: 0.1956, Test Accuracy: 0.9659\n",
            "Layer 0, Epoch [99/100], Step [117/469], Positive Goodness: 0.9778, Negative Goodness: 0.0206\n",
            "Layer 1, Epoch [99/100], Step [117/469], Positive Goodness: 0.9765, Negative Goodness: 0.0138\n",
            "Layer 2, Epoch [99/100], Step [117/469], Positive Goodness: 0.9804, Negative Goodness: 0.0130\n",
            "Layer 3, Epoch [99/100], Step [117/469], Positive Goodness: 0.9674, Negative Goodness: 0.0353\n",
            "Final Layer, Epoch [99/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [99/100], Step [117/469], Loss: 0.1958, Test Accuracy: 0.9663\n",
            "Layer 0, Epoch [99/100], Step [234/469], Positive Goodness: 0.9758, Negative Goodness: 0.0225\n",
            "Layer 1, Epoch [99/100], Step [234/469], Positive Goodness: 0.9757, Negative Goodness: 0.0153\n",
            "Layer 2, Epoch [99/100], Step [234/469], Positive Goodness: 0.9784, Negative Goodness: 0.0148\n",
            "Layer 3, Epoch [99/100], Step [234/469], Positive Goodness: 0.9641, Negative Goodness: 0.0377\n",
            "Final Layer, Epoch [99/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [99/100], Step [234/469], Loss: 0.1955, Test Accuracy: 0.9663\n",
            "Layer 0, Epoch [99/100], Step [351/469], Positive Goodness: 0.9787, Negative Goodness: 0.0211\n",
            "Layer 1, Epoch [99/100], Step [351/469], Positive Goodness: 0.9773, Negative Goodness: 0.0125\n",
            "Layer 2, Epoch [99/100], Step [351/469], Positive Goodness: 0.9804, Negative Goodness: 0.0129\n",
            "Layer 3, Epoch [99/100], Step [351/469], Positive Goodness: 0.9668, Negative Goodness: 0.0371\n",
            "Final Layer, Epoch [99/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [99/100], Step [351/469], Loss: 0.1954, Test Accuracy: 0.9663\n",
            "Layer 0, Epoch [99/100], Step [468/469], Positive Goodness: 0.9788, Negative Goodness: 0.0225\n",
            "Layer 1, Epoch [99/100], Step [468/469], Positive Goodness: 0.9775, Negative Goodness: 0.0146\n",
            "Layer 2, Epoch [99/100], Step [468/469], Positive Goodness: 0.9799, Negative Goodness: 0.0124\n",
            "Layer 3, Epoch [99/100], Step [468/469], Positive Goodness: 0.9659, Negative Goodness: 0.0363\n",
            "Final Layer, Epoch [99/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [99/100], Step [468/469], Loss: 0.1963, Test Accuracy: 0.9660\n",
            "Layer 0, Epoch [100/100], Step [117/469], Positive Goodness: 0.9770, Negative Goodness: 0.0216\n",
            "Layer 1, Epoch [100/100], Step [117/469], Positive Goodness: 0.9760, Negative Goodness: 0.0146\n",
            "Layer 2, Epoch [100/100], Step [117/469], Positive Goodness: 0.9791, Negative Goodness: 0.0129\n",
            "Layer 3, Epoch [100/100], Step [117/469], Positive Goodness: 0.9647, Negative Goodness: 0.0374\n",
            "Final Layer, Epoch [100/100], Step [117/469], Loss: 0.0001\n",
            "Epoch [100/100], Step [117/469], Loss: 0.1960, Test Accuracy: 0.9661\n",
            "Layer 0, Epoch [100/100], Step [234/469], Positive Goodness: 0.9787, Negative Goodness: 0.0205\n",
            "Layer 1, Epoch [100/100], Step [234/469], Positive Goodness: 0.9779, Negative Goodness: 0.0132\n",
            "Layer 2, Epoch [100/100], Step [234/469], Positive Goodness: 0.9798, Negative Goodness: 0.0125\n",
            "Layer 3, Epoch [100/100], Step [234/469], Positive Goodness: 0.9661, Negative Goodness: 0.0360\n",
            "Final Layer, Epoch [100/100], Step [234/469], Loss: 0.0001\n",
            "Epoch [100/100], Step [234/469], Loss: 0.1963, Test Accuracy: 0.9660\n",
            "Layer 0, Epoch [100/100], Step [351/469], Positive Goodness: 0.9774, Negative Goodness: 0.0213\n",
            "Layer 1, Epoch [100/100], Step [351/469], Positive Goodness: 0.9776, Negative Goodness: 0.0150\n",
            "Layer 2, Epoch [100/100], Step [351/469], Positive Goodness: 0.9817, Negative Goodness: 0.0132\n",
            "Layer 3, Epoch [100/100], Step [351/469], Positive Goodness: 0.9695, Negative Goodness: 0.0383\n",
            "Final Layer, Epoch [100/100], Step [351/469], Loss: 0.0001\n",
            "Epoch [100/100], Step [351/469], Loss: 0.1960, Test Accuracy: 0.9662\n",
            "Layer 0, Epoch [100/100], Step [468/469], Positive Goodness: 0.9781, Negative Goodness: 0.0215\n",
            "Layer 1, Epoch [100/100], Step [468/469], Positive Goodness: 0.9757, Negative Goodness: 0.0143\n",
            "Layer 2, Epoch [100/100], Step [468/469], Positive Goodness: 0.9785, Negative Goodness: 0.0136\n",
            "Layer 3, Epoch [100/100], Step [468/469], Positive Goodness: 0.9643, Negative Goodness: 0.0366\n",
            "Final Layer, Epoch [100/100], Step [468/469], Loss: 0.0001\n",
            "Epoch [100/100], Step [468/469], Loss: 0.1961, Test Accuracy: 0.9662\n"
          ]
        }
      ],
      "source": [
        "total_pos_goodnesses_st, total_neg_goodnesses_st, train_losses_st, test_losses_st, test_accuracies_st = train_sigmoid_threshold(num_epochs, model_sigmoid_threshold, train_loader, test_loader, ceLoss, pos_optimizers, neg_optimizers, final_optimizer, thresholds, records_per_epoch, supervised)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2GZVnrctom8"
      },
      "source": [
        "## Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Functions"
      ],
      "metadata": {
        "id": "CTVgakXXaOQp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "08JW-2PaLYEF"
      },
      "outputs": [],
      "source": [
        "# plot positive goodness and negative goodness\n",
        "def plot_goodnesses(total_pos_goodnesses, total_neg_goodnesses, num_epochs, records_per_epoch):\n",
        "    figure(figsize=(8,6), dpi=80)\n",
        "    epoch_axis = torch.arange(1/records_per_epoch,num_epochs+(1/records_per_epoch),1/records_per_epoch)\n",
        "    for i in range(len(total_pos_goodnesses)):\n",
        "        plt.plot(epoch_axis,total_pos_goodnesses[i],label=\"Positive Goodness Layer {}\".format(i))\n",
        "    for i in range(len(total_neg_goodnesses)):\n",
        "        plt.plot(epoch_axis,total_neg_goodnesses[i],label=\"Negative Goodness Layer {}\".format(i))\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"goodness\")\n",
        "    plt.title(\"Average Positive/Negative Goodness For Each Layer Over Time\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ouCVgTjZXe0Y"
      },
      "outputs": [],
      "source": [
        "# plot training/test losses\n",
        "def plot_losses(train_losses, test_losses, num_epochs, records_per_epoch):\n",
        "    figure(figsize=(8,6), dpi=80)\n",
        "    epoch_axis = torch.arange(1/records_per_epoch,num_epochs+(1/records_per_epoch),1/records_per_epoch)\n",
        "    plt.plot(epoch_axis,train_losses,label=\"Training Loss\")\n",
        "    plt.plot(epoch_axis,test_losses,label=\"Test Loss\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.title(\"Average Training/Test Loss Over Time\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "vBq4AlawXe0Y"
      },
      "outputs": [],
      "source": [
        "# plot test accuracies\n",
        "def plot_accuracies(test_accuracies, num_epochs, records_per_epoch):\n",
        "    figure(figsize=(8,6), dpi=80)\n",
        "    epoch_axis = torch.arange(1/records_per_epoch,num_epochs+(1/records_per_epoch),1/records_per_epoch)\n",
        "    plt.plot(epoch_axis,test_accuracies,label=\"Test Accuracy\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.title(\"Model Test Accuracy Over Time\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model Performance"
      ],
      "metadata": {
        "id": "WyrrfPBhaWpE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "D_8yPfk03oLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5222d6d3-dd60-4581-a566-afe05db83d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the test set:  0.9662999510765076\n"
          ]
        }
      ],
      "source": [
        "_, test_acc = test(model_sigmoid_threshold, test_loader,ceLoss)\n",
        "print('Test Accuracy of the model on the test set: ', float(test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "PMheoEy12rn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "outputId": "a85afc86-926b-4fac-b035-d6c0e13122f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:  tensor([9, 5, 5, 6, 4, 2, 4, 5, 8, 5, 9, 9, 2, 1, 1, 4, 1, 6, 0, 2, 2, 2, 8, 1,\n",
            "        4, 3, 7, 8, 9, 1, 1, 8, 0, 2, 3, 4, 1, 1, 1, 5, 3, 3, 5, 5, 1, 2, 0, 8,\n",
            "        0, 3, 8, 6, 5, 7, 2, 3, 9, 7, 2, 3, 8, 9, 8, 7, 1, 7, 2, 9, 1, 8, 0, 2,\n",
            "        1, 9, 5, 3, 9, 6, 5, 4, 2, 8, 8, 3, 6, 8, 0, 8, 1, 3, 1, 6, 8, 1, 7, 3,\n",
            "        9, 9, 6, 9, 2, 8, 9, 5, 9, 6, 7, 2, 0, 2, 5, 8, 2, 4, 9, 3, 2, 5, 3, 6,\n",
            "        4, 8, 5, 4, 3, 1, 3, 6], device='cuda:0')\n",
            " Label:  tensor([9, 5, 5, 6, 4, 2, 4, 5, 8, 5, 9, 9, 2, 1, 1, 4, 1, 6, 0, 2, 2, 2, 8, 1,\n",
            "        4, 3, 7, 8, 9, 1, 1, 8, 0, 2, 3, 4, 1, 1, 1, 5, 3, 3, 5, 5, 1, 2, 0, 3,\n",
            "        0, 3, 1, 6, 5, 7, 2, 3, 9, 7, 2, 5, 8, 9, 8, 7, 1, 7, 2, 3, 1, 8, 0, 5,\n",
            "        1, 9, 5, 3, 9, 6, 5, 4, 2, 8, 8, 3, 6, 8, 0, 1, 1, 3, 1, 6, 8, 1, 7, 3,\n",
            "        9, 9, 6, 9, 2, 8, 9, 5, 9, 6, 7, 2, 0, 2, 5, 8, 2, 4, 9, 3, 2, 5, 3, 6,\n",
            "        4, 8, 5, 4, 3, 1, 3, 6], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# prediction test & visualization\n",
        "x,y = next(iter(train_loader))\n",
        "x,y = x.to(device),y.to(device)\n",
        "neg_x = model_sigmoid_threshold.construct_supervised_example(x,y,False)\n",
        "print(\"Prediction: \", model_sigmoid_threshold.predict(x,device))\n",
        "print(\" Label: \",y)\n",
        "plt.imshow(neg_x[0].squeeze().cpu(), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "KWeua6iJd78w"
      },
      "outputs": [],
      "source": [
        "for i in range(len(total_pos_goodnesses_st)):\n",
        "  for j in range(len(total_pos_goodnesses_st[i])):\n",
        "    total_pos_goodnesses_st[i][j] = total_pos_goodnesses_st[i][j].cpu().detach()\n",
        "for i in range(len(total_neg_goodnesses_st)):\n",
        "  for j in range(len(total_neg_goodnesses_st[i])):\n",
        "    total_neg_goodnesses_st[i][j] = total_neg_goodnesses_st[i][j].cpu().detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Fi0pwGIge8zW"
      },
      "outputs": [],
      "source": [
        "for i in range(len(train_losses_st)):\n",
        "  train_losses_st[i] = train_losses_st[i].cpu().detach()\n",
        "for i in range(len(test_losses_st)):\n",
        "  test_losses_st[i] = test_losses_st[i].cpu().detach()\n",
        "for i in range(len(test_accuracies_st)):\n",
        "  test_accuracies_st[i] = test_accuracies_st[i].cpu().detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "2Tf3mC_aXe0Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "fd129db2-0f2c-405c-db11-eed427344022"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot goodness for model trained using sigmoid minus threshold\n",
        "plot_goodnesses(total_pos_goodnesses_st, total_neg_goodnesses_st, num_epochs, records_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "UoKORDMdY5vt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "326723b8-4227-43b2-c0cb-c660d64ddc5f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot training/test loss for model trained using sigmoid minus threshold\n",
        "plot_losses(train_losses_st, test_losses_st, num_epochs, records_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ZVeo1AhbY9IB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "4e9909a7-c2b8-459b-f5e6-967b784ca404"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot test accuracy for model trained using sigmoid minus threshold\n",
        "plot_accuracies(test_accuracies_st, num_epochs, records_per_epoch)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZWXh5TUHjfyX",
        "8xEIHA2ZN1wj",
        "fxx1HhrVZU8b",
        "eZZ-V48mwWNK"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}