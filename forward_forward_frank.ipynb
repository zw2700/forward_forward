{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIHpnulXtom0"
      },
      "source": [
        "# Loss Graph & Hyperparameter Tune & Validation & Train\n",
        "# Make sure forward pass works (for hard negative data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay9ttw0IlPZ2"
      },
      "source": [
        "## Setup & Dataset Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J965j0wvtom2"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"ray[air]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIV97VrN-r05",
        "outputId": "712b51a1-9fc1-44ff-8c2e-31cc6fee4137"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray[air]\n",
            "  Downloading ray-2.5.1-cp310-cp310-manylinux2014_x86_64.whl (56.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.2/56.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from ray[air]) (23.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (8.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[air]) (3.12.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[air]) (4.3.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.0.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[air]) (23.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[air]) (6.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[air]) (2.27.1)\n",
            "Collecting grpcio<=1.51.3,>=1.42.0 (from ray[air])\n",
            "  Downloading grpcio-1.51.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.22.4)\n",
            "Collecting starlette (from ray[air])\n",
            "  Downloading starlette-0.28.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.9/68.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi (from ray[air])\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting virtualenv<20.21.1,>=20.0.24 (from ray[air])\n",
            "  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.5.3)\n",
            "Collecting gpustat>=1.0.0 (from ray[air])\n",
            "  Downloading gpustat-1.1.tar.gz (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiorwlock (from ray[air])\n",
            "  Downloading aiorwlock-1.3.0-py3-none-any.whl (10.0 kB)\n",
            "Collecting aiohttp-cors (from ray[air])\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[air])\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[air]) (6.3.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (9.0.0)\n",
            "Collecting opencensus (from ray[air])\n",
            "  Downloading opencensus-0.11.2-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from ray[air]) (1.10.9)\n",
            "Collecting colorful (from ray[air])\n",
            "  Downloading colorful-0.5.5-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (0.17.0)\n",
            "Collecting tensorboardX>=1.9 (from ray[air])\n",
            "  Downloading tensorboardX-2.6.1-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from ray[air])\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[air]) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[air]) (3.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[air]) (1.9.2)\n",
            "Collecting nvidia-ml-py>=11.450.129 (from gpustat>=1.0.0->ray[air])\n",
            "  Downloading nvidia_ml_py-12.535.77-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[air]) (5.9.5)\n",
            "Collecting blessed>=1.17.1 (from gpustat>=1.0.0->ray[air])\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->ray[air]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->ray[air]) (2022.7.1)\n",
            "Collecting protobuf!=3.19.5,>=3.15.3 (from ray[air])\n",
            "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.6 (from virtualenv<20.21.1,>=20.0.24->ray[air])\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[air]) (3.7.0)\n",
            "Collecting starlette (from ray[air])\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->ray[air]) (4.6.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->ray[air]) (3.7.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[air]) (0.19.3)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[air])\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[air]) (2.11.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[air]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[air]) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[air]) (3.4)\n",
            "Collecting h11>=0.8 (from uvicorn->ray[air])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->ray[air]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->ray[air]) (1.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[air]) (0.2.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[air]) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (1.59.1)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (2.17.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (0.5.0)\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.1-py3-none-any.whl size=26280 sha256=2e3b6d79100f186de1faf48209e75d72df7201741ae3493b0324785a5028c036\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/d0/2c/1e02440645c2318ba03aea99993a44a9108dc8f74de0bd370b\n",
            "Successfully built gpustat\n",
            "Installing collected packages: py-spy, opencensus-context, nvidia-ml-py, distlib, colorful, virtualenv, protobuf, h11, grpcio, blessed, aiorwlock, uvicorn, tensorboardX, starlette, ray, gpustat, fastapi, aiohttp-cors, opencensus\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.56.0\n",
            "    Uninstalling grpcio-1.56.0:\n",
            "      Successfully uninstalled grpcio-1.56.0\n",
            "Successfully installed aiohttp-cors-0.7.0 aiorwlock-1.3.0 blessed-1.20.0 colorful-0.5.5 distlib-0.3.6 fastapi-0.100.0 gpustat-1.1 grpcio-1.51.3 h11-0.14.0 nvidia-ml-py-12.535.77 opencensus-0.11.2 opencensus-context-0.1.3 protobuf-4.23.4 py-spy-0.3.14 ray-2.5.1 starlette-0.27.0 tensorboardX-2.6.1 uvicorn-0.22.0 virtualenv-20.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JqYY0cwIkJmq"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "import os\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split\n",
        "from ray import tune\n",
        "from ray.air import Checkpoint, session\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBkHOxqFtom3"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT3vR52KNjgA",
        "outputId": "90af84c6-8540-4bbd-d438-d695db254297"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "batch_size = 64\n",
        "supervised = True\n",
        "lrs = [1e-2,5e-3,1e-3,1e-4,1e-3]\n",
        "momentums = [0.9,0.9,0.9,0.9,0.9]\n",
        "weight_decays = [1e-5,5e-5,1e-3,1e-3,1e-5]\n",
        "thresholds = [50,1000,1000,1000]\n",
        "num_epochs = 15\n",
        "records_per_epoch = 4\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eGYNvwIhker4"
      },
      "outputs": [],
      "source": [
        "# import MNIST\n",
        "def load_mnist(data_dir=\"./data\"):\n",
        "  train_data = datasets.MNIST(\n",
        "      root = data_dir,\n",
        "      train = True,\n",
        "      transform = ToTensor(),\n",
        "      download = True,\n",
        "  )\n",
        "  test_data = datasets.MNIST(\n",
        "      root = data_dir,\n",
        "      train = False,\n",
        "      transform = ToTensor(),\n",
        "      download = True\n",
        "  )\n",
        "  return train_data,test_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = load_mnist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exjpqMVzcMBG",
        "outputId": "165c81f7-9b89-4173-a2b6-4206984457c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 212531943.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 22959760.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 71708881.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4489872.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4C-DcpX9p2o",
        "outputId": "a312024c-9380-4022-f923-9df5e458d844"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7f853725ad70>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7f853725bfd0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Dataloader\n",
        "train_loader = DataLoader(train_data,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        num_workers=1)\n",
        "\n",
        "test_loader = DataLoader(test_data,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=1)\n",
        "\n",
        "train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xEIHA2ZN1wj"
      },
      "source": [
        "## Construct Negative Examples for Unsupervised Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfySAAjudWtk"
      },
      "outputs": [],
      "source": [
        "filter = torch.tensor([[0.0625,0.125,0.0625],[0.125,0.25,0.125],[0.0625,0.125,0.0625]])\n",
        "\n",
        "def blur(img,filter):\n",
        "  # blur img using filter\n",
        "  # params: filter --> torch.tensor(2*radius+1,2*radius+1)\n",
        "  m,n = img.shape\n",
        "  radius = (filter.shape[0]-1)//2\n",
        "  new_img = torch.zeros(m,n)\n",
        "  for i in range(m):\n",
        "    for j in range(n):\n",
        "      top,left,bottom,right = max(0,i-radius),max(0,j-radius),min(m-1,i+radius),min(m-1,j+radius)\n",
        "      new_img[i][j] = sum([img[x][y]*filter[x-i+radius][y-j+radius]\n",
        "                           for y in range(left,right+1) for x in range(top,bottom+1)])\n",
        "  return new_img\n",
        "\n",
        "def generate_negative_example():\n",
        "  # construct a negative example for unsupervised case (3.2 in paper)\n",
        "  mask = (torch.rand(28,28) > 0.5).long() # initiate as random bit image\n",
        "\n",
        "  for i in range(6): # repeat blurring 6 times\n",
        "    mask = blur(mask,filter)\n",
        "\n",
        "  mask = (mask > 0.5).long() # mask\n",
        "\n",
        "  index1,index2 = torch.randint(len(train_data),(2,))\n",
        "  return (mask * train_data[index1][0] + (1-mask) * train_data[index2][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxx1HhrVZU8b"
      },
      "source": [
        "## Model Definition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dOcFixAKTEOh"
      },
      "outputs": [],
      "source": [
        "# single-layer NN\n",
        "class one_layer_net(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(one_layer_net, self).__init__()\n",
        "\n",
        "        # hidden layer\n",
        "        # self.layer = torch.nn.Sequential(\n",
        "            # torch.nn.Flatten(), # nn.Flatten to standardize input shape\n",
        "            # torch.nn.Linear(in_size, out_size),\n",
        "            # torch.nn.ReLU() # activation function\n",
        "        # )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.layer = nn.Linear(in_size, out_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      f = self.flatten(x)\n",
        "      l = self.layer(f)\n",
        "      r = self.relu(l)\n",
        "      return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0iLwMtYODMgb"
      },
      "outputs": [],
      "source": [
        "# full network\n",
        "class net(nn.Module):\n",
        "    # Constructor\n",
        "    def __init__(self,in_size):\n",
        "        super(net, self).__init__()\n",
        "        self.layer1 = one_layer_net(in_size,2000)\n",
        "        self.layer2 = one_layer_net(2000,2000)\n",
        "        self.layer3 = one_layer_net(2000,2000)\n",
        "        self.layer4 = one_layer_net(2000,2000)\n",
        "        self.ln = nn.LayerNorm(2000) # layernorm\n",
        "\n",
        "        # softmax layer for hard negative label generation\n",
        "        self.out_layer = nn.Linear(6000, 10)\n",
        "\n",
        "        self.layers = [self.layer1,self.layer2,self.layer3,self.layer4,self.out_layer]\n",
        "\n",
        "    def forward(self,x):\n",
        "        # x-->tensor (batch_size,channel,height,width)\n",
        "        # return res --> tensor (batch_size,10)\n",
        "        interms = []\n",
        "        interm = x.detach().clone()\n",
        "\n",
        "        for index in range(len(self.layers)-1):\n",
        "          layer = self.layers[index] # linear layer\n",
        "          output = layer(interm) # run the layer\n",
        "          interm = self.ln(output) # layer-norm\n",
        "          interms.append(interm.detach().clone()) # store normalized activity\n",
        "\n",
        "        final_input = torch.cat(interms[1:],dim=1)\n",
        "        res = self.out_layer(final_input) # final output layer\n",
        "        return res\n",
        "\n",
        "    def predict(self,x,device):\n",
        "        # x-->tensor (batch_size,channel,height,width)\n",
        "        # return labels --> tensor (batch_size,1)\n",
        "        accumulate_goodness = torch.zeros(x.shape[0],10).to(device) # accumulate goodness for each label\n",
        "        for i in range(10): # iterate through all labels\n",
        "          interm = x.detach().clone()\n",
        "\n",
        "          # update one-hot label encoding\n",
        "          interm[:,:,0,:10] = 0\n",
        "          interm[:,:,0,i] = 1\n",
        "\n",
        "          for index in range(len(self.layers)-1):\n",
        "            layer = self.layers[index]\n",
        "            interm = layer(interm)\n",
        "\n",
        "            if index > 0: # accumulate goodness for all activities but first hidden layer\n",
        "              accumulate_goodness[:,i] = accumulate_goodness[:,i] + self.goodness(interm)\n",
        "\n",
        "            if index + 1 < len(self.layers): # layernorm\n",
        "              interm = self.ln(interm)\n",
        "        return torch.argmax(accumulate_goodness,dim=1)\n",
        "\n",
        "    def goodness(self,x):\n",
        "        # goodness functions--sum of squares\n",
        "        # input: x-->tensor (batch_size, ...) (arbitrary shape that starts with batch_size)\n",
        "        # return goodness-->tensor (batch_size,1)\n",
        "        x = x.reshape(x.shape[0],-1)\n",
        "        return torch.sum(torch.square(x),dim=1)\n",
        "\n",
        "    def criterion(self,x, threshold):\n",
        "        # criterion function--mean(sigmoid(goodness - threshold))\n",
        "        # input: x-->tensor (batch_size, ...) (arbitrary shape that starts with batch_size)\n",
        "        # return loss-->float\n",
        "        goodness = self.goodness(x)\n",
        "        res = torch.mean(torch.sigmoid(goodness - threshold))\n",
        "        # print(goodness,res)\n",
        "        return res\n",
        "\n",
        "    def construct_supervised_example(self,x,y,positive=True):\n",
        "        # construct positive or negative examples for supervised training\n",
        "        # x-->tensor (batch_size,channel,height,width)\n",
        "        # y-->tensor (batch_size,)\n",
        "        # return ans-->tensor (batch_size,channel,height,width)\n",
        "\n",
        "        ans = x.detach().clone()\n",
        "\n",
        "        if positive:\n",
        "          # one-hot encoding\n",
        "          ans[:,:,0,:10] = 0\n",
        "          for i in range(x.shape[0]):\n",
        "             ans[i,:,0,y[i]] = 1\n",
        "\n",
        "        else:\n",
        "          ans[:,:,0,:10] = 0.1 # initialize neutral label\n",
        "\n",
        "          # run forward pass\n",
        "          labels = self.forward(ans)\n",
        "\n",
        "          for i in range(x.shape[0]):\n",
        "             labels[i][y[i]] = -float('inf') # omit positive label\n",
        "          prob = torch.softmax(labels,dim=1) # probability distribution to choose label\n",
        "          negative_label = torch.multinomial(prob,1).squeeze() # choose from distribution\n",
        "\n",
        "          # generate negative example from forward pass\n",
        "          ans[:,:,0,:10] = 0\n",
        "          for i in range(x.shape[0]):\n",
        "              ans[i,:,0,negative_label[i]] = 1\n",
        "\n",
        "        return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXS93nQJOmoD",
        "outputId": "6ed216e6-19ba-4824-d4bd-71f26a9684f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "net(\n",
              "  (layer1): one_layer_net(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (layer): Linear(in_features=784, out_features=2000, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (layer2): one_layer_net(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (layer): Linear(in_features=2000, out_features=2000, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (layer3): one_layer_net(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (layer): Linear(in_features=2000, out_features=2000, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (layer4): one_layer_net(\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (layer): Linear(in_features=2000, out_features=2000, bias=True)\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (ln): LayerNorm((2000,), eps=1e-05, elementwise_affine=True)\n",
              "  (out_layer): Linear(in_features=6000, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# create model\n",
        "model = net(784)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kC0QvPgPfknW"
      },
      "outputs": [],
      "source": [
        "# apply xavier initialization on weights\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wbVlBrAYtom7"
      },
      "outputs": [],
      "source": [
        "pos_optimizers, neg_optimizers = [],[]\n",
        "final_optimizer = torch.optim.SGD(model.layers[-1].parameters(), lr=lrs[-1], momentum = momentums[-1], weight_decay=weight_decays[-1], maximize=False)\n",
        "# initialize each layer\n",
        "for index in range(len(model.layers)):\n",
        "    # layer & training initialization\n",
        "    layer = model.layers[index]\n",
        "    layer.apply(init_weights)\n",
        "    # clip gradients to avoid exploding gradients\n",
        "    torch.nn.utils.clip_grad_norm_(parameters=layer.parameters(), max_norm=1,norm_type=2.0)\n",
        "    # define optimizers\n",
        "    if index < len(model.layers)-1:\n",
        "      pos_optimizers.append(torch.optim.SGD(layer.parameters(), lr=lrs[index], momentum = momentums[index], weight_decay=weight_decays[index], maximize=True))\n",
        "      neg_optimizers.append(torch.optim.SGD(layer.parameters(), lr=lrs[index], momentum = momentums[index], weight_decay=weight_decays[index], maximize=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-KK7uRkQMKTi"
      },
      "outputs": [],
      "source": [
        "# design loss function\n",
        "def goodness_loss(pos_goodness,neg_goodness):\n",
        "  # loss function intended for maximizing goodness in positive examples\n",
        "  # and minimizing goodness in negative examples\n",
        "  return torch.log(1+torch.exp(neg_goodness-pos_goodness))\n",
        "\n",
        "\n",
        "# CrossEntropyLoss for final softmax layer optimization\n",
        "ceLoss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparemeter Tuning"
      ],
      "metadata": {
        "id": "eZZ-V48mwWNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train function implementing Ray Tune for hyperparameter tuning\n",
        "# train model\n",
        "def train_tune(config, data_dir='./data'):\n",
        "\n",
        "    # initialize model\n",
        "    model = net(784)\n",
        "\n",
        "    # configure device\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            model = nn.DataParallel(model)\n",
        "    model.to(device)\n",
        "\n",
        "    # configure loss function and optimizers\n",
        "    final_loss = config[\"final_loss\"]\n",
        "    pos_optimizers, neg_optimizers = [],[]\n",
        "    final_optimizer = torch.optim.SGD(model.layers[-1].parameters(), lr=config['lrs'][-1], momentum = config['momentums'][-1], weight_decay=config['weight_decays'][-1], maximize=False)\n",
        "    # iterate each layer\n",
        "    for index in range(len(model.layers)):\n",
        "        # layer & training initialization\n",
        "        layer = model.layers[index]\n",
        "        layer.apply(init_weights)\n",
        "        # clip gradients to avoid exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=layer.parameters(), max_norm=1,norm_type=2.0)\n",
        "        # define optimizers\n",
        "        if index < len(model.layers)-1:\n",
        "          pos_optimizers.append(torch.optim.SGD(layer.parameters(), lr=config['lrs'][index], momentum = config['momentums'][index], weight_decay=config['weight_decays'][index], maximize=True))\n",
        "          neg_optimizers.append(torch.optim.SGD(layer.parameters(), lr=config['lrs'][index], momentum = config['momentums'][index], weight_decay=config['weight_decays'][index], maximize=False))\n",
        "\n",
        "    # configure state of model if checkpoint exists\n",
        "    checkpoint = session.get_checkpoint()\n",
        "    if checkpoint:\n",
        "        checkpoint_state = checkpoint.to_dict()\n",
        "        start_epoch = checkpoint_state[\"epoch\"]\n",
        "        net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
        "        for index in range(len(model.layers)-1):\n",
        "          pos_optimizers[index].load_state_dict(checkpoint_state[\"optimizer_state_dict\"][\"pos\"][index])\n",
        "          neg_optimizers[index].load_state_dict(checkpoint_state[\"optimizer_state_dict\"][\"neg\"][index])\n",
        "        final_optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"][\"final\"])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "    # load data\n",
        "    train_data, test_data = load_mnist(data_dir)\n",
        "\n",
        "    test_abs = int(len(train_data) * 0.9)\n",
        "    train_subset, val_subset = random_split(\n",
        "        train_data, [test_abs, len(train_data) - test_abs]\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
        "    )\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # variables for training output\n",
        "    # total_pos_goodnesses,total_neg_goodnesses = [],[]\n",
        "    # total_losses = []\n",
        "    # test_accuracies = []\n",
        "    total_step = len(train_loader)\n",
        "    record_period = total_step//records_per_epoch\n",
        "\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        total_pos_goodness,total_neg_goodness = [0]*(len(model.layers)-1),[0]*(len(model.layers)-1)\n",
        "        total_loss = 0\n",
        "        # total_loss = [0]*len(model.layers)\n",
        "        for i,(x,y) in enumerate(train_loader): # iterate through dataset\n",
        "            x,y = x.to(device),y.to(device)\n",
        "            if not config[\"supervised\"]: # unsupervised training input\n",
        "              pos_imgs = x.squeeze()\n",
        "              neg_imgs = generate_negative_example().squeeze().to(device)\n",
        "            else: # supervised learning input\n",
        "              pos_imgs = model.construct_supervised_example(x,y,True)\n",
        "              neg_imgs = model.construct_supervised_example(x,y,False)\n",
        "\n",
        "            # intermediate variables\n",
        "            pos_interms = []\n",
        "            pos_interm = pos_imgs.clone()\n",
        "            neg_interm = neg_imgs.clone()\n",
        "\n",
        "            # iterate over intermediate layers\n",
        "            for index in range(len(model.layers)-1):\n",
        "              layer = model.layers[index]\n",
        "\n",
        "              # positive pass\n",
        "              pos_output = layer(pos_interm)\n",
        "              pos_goodness = model.criterion(pos_output, threshold=config[\"thresholds\"][index])\n",
        "              pos_interm = model.ln(pos_output)\n",
        "              pos_interms.append(pos_interm.detach().clone())\n",
        "\n",
        "              # update variables\n",
        "              total_pos_goodness[index] += pos_goodness\n",
        "\n",
        "              # clear gradients for this training step\n",
        "              pos_optimizers[index].zero_grad()\n",
        "\n",
        "              # take gradient step\n",
        "              pos_goodness.backward(retain_graph=True)\n",
        "              pos_optimizers[index].step()\n",
        "\n",
        "              # negative pass\n",
        "              neg_output = layer(neg_interm)\n",
        "              neg_goodness = model.criterion(neg_output, threshold=thresholds[index])\n",
        "              neg_interm = model.ln(neg_output)\n",
        "\n",
        "              total_neg_goodness[index] += neg_goodness\n",
        "              neg_optimizers[index].zero_grad()\n",
        "              neg_goodness.backward(retain_graph=True)\n",
        "              neg_optimizers[index].step()\n",
        "\n",
        "              # update variables\n",
        "              total_pos_goodness[index] += pos_goodness\n",
        "              total_neg_goodness[index] += neg_goodness\n",
        "\n",
        "              # check progress\n",
        "              if (i+1) % (record_period) == 0:\n",
        "                  avg_pos_goodness = total_pos_goodness[index]/(record_period)\n",
        "                  avg_neg_goodness = total_neg_goodness[index]/(record_period)\n",
        "                  # if len(total_pos_goodnesses)>index:\n",
        "                  #   total_pos_goodnesses[index].append(avg_pos_goodness)\n",
        "                  # else:\n",
        "                  #   total_pos_goodnesses.append([avg_pos_goodness])\n",
        "                  # if len(total_neg_goodnesses)>index:\n",
        "                  #   total_neg_goodnesses[index].append(avg_neg_goodness)\n",
        "                  # else:\n",
        "                  #   total_neg_goodnesses.append([avg_neg_goodness])\n",
        "\n",
        "                  print ('Layer {}, Epoch [{}/{}], Step [{}/{}], Positive Goodness: {:.4f}, Negative Goodness: {:.4f}'\n",
        "                        .format(index, epoch + 1, num_epochs, i + 1, total_step, avg_pos_goodness, avg_neg_goodness))\n",
        "                  total_pos_goodness[index],total_neg_goodness[index] = 0,0\n",
        "\n",
        "            # output layer\n",
        "            final_input = torch.cat(pos_interms[1:],dim=1)\n",
        "            labels = model.layers[-1](final_input)\n",
        "            loss = ceLoss(labels,y)\n",
        "            total_loss += loss\n",
        "\n",
        "            final_optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            final_optimizer.step()\n",
        "\n",
        "            if (i+1) % record_period == 0:\n",
        "              avg_loss = total_loss/record_period\n",
        "              # total_losses.append(avg_loss)\n",
        "\n",
        "              print ('Final Layer, Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                    .format(epoch + 1, num_epochs, i + 1, total_step, avg_loss))\n",
        "              total_loss = 0\n",
        "\n",
        "              # validation\n",
        "              val_loss, val_acc = test(model, val_loader, config[\"final_loss\"])\n",
        "              print ('Epoch [{}/{}], Step [{}/{}], Test Accuracy: {:.4f}'\n",
        "                    .format(epoch + 1, num_epochs, i + 1, total_step, val_acc))\n",
        "              # test_accuracies.append(test_acc)\n",
        "\n",
        "              # record in ray tune\n",
        "              checkpoint_data = {\n",
        "                  \"epoch\": epoch,\n",
        "                  \"net_state_dict\": model.state_dict(),\n",
        "                  \"optimizer_state_dict\": {\n",
        "                      \"pos\":{}, \"neg\":{}, \"final\": final_optimizer.state_dict()\n",
        "                  }\n",
        "              }\n",
        "              for index in range(len(model.layers)-1):\n",
        "                  checkpoint_data[\"optimizer_state_dict\"][\"pos\"][index] = pos_optimizers[index].state_dict()\n",
        "                  checkpoint_data[\"optimizer_state_dict\"][\"neg\"][index] = neg_optimizers[index].state_dict()\n",
        "\n",
        "              checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
        "\n",
        "              session.report(\n",
        "                  {\"loss\": val_loss, \"accuracy\": val_acc},\n",
        "                  checkpoint=checkpoint,\n",
        "              )\n",
        "\n",
        "    print(\"finished training\")"
      ],
      "metadata": {
        "id": "_gvRB08YYgz3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "\n",
        "    config = {\n",
        "    \"batch_size\": tune.choice([4,8,16,32,64]),\n",
        "    \"num_epochs\": max_num_epochs,\n",
        "    \"final_loss\": ceLoss,\n",
        "    \"lrs\": [tune.loguniform(1e-5, 1e-1),tune.loguniform(1e-5, 1e-1),tune.loguniform(1e-5, 1e-1),tune.loguniform(1e-5, 1e-1),tune.loguniform(1e-5, 1e-1)],\n",
        "    \"momentums\": [0.9,0.9,0.9,0.9,0.9],\n",
        "    \"weight_decays\": [tune.loguniform(1e-5, 1e-2),tune.loguniform(1e-5, 1e-2),tune.loguniform(1e-5, 1e-2),tune.loguniform(1e-5, 1e-2),tune.loguniform(1e-5, 1e-2)],\n",
        "    \"thresholds\": [50,1000,1000,1000],\n",
        "    \"records_per_epoch\": 4,\n",
        "    \"supervised\" : True\n",
        "    }\n",
        "\n",
        "    _, test_data = load_mnist(data_dir)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=64, shuffle=True, num_workers=8\n",
        "    )\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2,\n",
        "    )\n",
        "\n",
        "    result = tune.run(\n",
        "        partial(train_tune, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "    )\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(f\"Best trial config: {best_trial.config}\")\n",
        "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
        "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
        "\n",
        "    best_trained_model = net(784)\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if gpus_per_trial > 1:\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    best_trained_model.to(device)\n",
        "\n",
        "    best_checkpoint = best_trial.checkpoint.to_air_checkpoint()\n",
        "    best_checkpoint_data = best_checkpoint.to_dict()\n",
        "\n",
        "    best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n",
        "\n",
        "    test_acc = test(best_trained_model, test_loader, config[\"final_loss\"])\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the number of GPUs per trial here:\n",
        "    main(num_samples=10, max_num_epochs=20, gpus_per_trial=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q6zgrTsvekwo",
        "outputId": "7b116ed8-69c5-4ce5-dc57-737c1a12b040"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "2023-07-08 03:04:34,313\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "2023-07-08 03:04:35,796\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-07-08 03:04:36 (running for 00:00:00.24)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "+------------------------+----------+-------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc   |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+-------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00000 | PENDING  |       |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "| train_tune_2b67e_00001 | PENDING  |       |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "| train_tune_2b67e_00002 | PENDING  |       |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "| train_tune_2b67e_00003 | PENDING  |       |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "| train_tune_2b67e_00004 | PENDING  |       |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "| train_tune_2b67e_00005 | PENDING  |       |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "| train_tune_2b67e_00006 | PENDING  |       |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "| train_tune_2b67e_00007 | PENDING  |       |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "| train_tune_2b67e_00008 | PENDING  |       |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "| train_tune_2b67e_00009 | PENDING  |       |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "+------------------------+----------+-------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Current time: 2023-07-08 03:04:41 (running for 00:00:05.28)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "+------------------------+----------+-------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc   |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+-------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00000 | PENDING  |       |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "| train_tune_2b67e_00001 | PENDING  |       |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "| train_tune_2b67e_00002 | PENDING  |       |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "| train_tune_2b67e_00003 | PENDING  |       |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "| train_tune_2b67e_00004 | PENDING  |       |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "| train_tune_2b67e_00005 | PENDING  |       |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "| train_tune_2b67e_00006 | PENDING  |       |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "| train_tune_2b67e_00007 | PENDING  |       |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "| train_tune_2b67e_00008 | PENDING  |       |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "| train_tune_2b67e_00009 | PENDING  |       |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "+------------------------+----------+-------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2371)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(func pid=2371)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "2023-07-08 03:04:43,053\tERROR tune_controller.py:873 -- Trial task failed for trial train_tune_2b67e_00000\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2540, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2371, ip=172.28.0.12, actor_id=44a09c6cf7981f8af61679a801000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-14-de0babcfe350>\", line 105, in train_tune\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2000, 2000]], which is output 0 of AsStridedBackward0, is at version 4; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_tune_2b67e_00000:\n",
            "  date: 2023-07-08_03-04-41\n",
            "  hostname: 3ff3fc96ba34\n",
            "  node_ip: 172.28.0.12\n",
            "  pid: 2371\n",
            "  timestamp: 1688785481\n",
            "  trial_id: 2b67e_00000\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2023-07-08 03:04:46 (running for 00:00:10.28)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (1 ERROR, 9 PENDING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00001 | PENDING  |                  |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "| train_tune_2b67e_00002 | PENDING  |                  |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "| train_tune_2b67e_00003 | PENDING  |                  |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "| train_tune_2b67e_00004 | PENDING  |                  |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "| train_tune_2b67e_00005 | PENDING  |                  |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "| train_tune_2b67e_00006 | PENDING  |                  |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "| train_tune_2b67e_00007 | PENDING  |                  |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "| train_tune_2b67e_00008 | PENDING  |                  |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "| train_tune_2b67e_00009 | PENDING  |                  |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "| train_tune_2b67e_00000 | ERROR    | 172.28.0.12:2371 |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "Number of errored trials: 1\n",
            "+------------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name             |   # failures | error file                                                                                                                                                                                                     |\n",
            "|------------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_tune_2b67e_00000 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00000_0_batch_size=8,0=0.0002,1=0.0015,2=0.0002,3=0.0864,4=0.0000,0=0.0000,1=0.0033,2=0.0000,3=0.0000,4=0.0006_2023-07-08_03-04-36/error.txt |\n",
            "+------------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-07-08 03:04:47,742\tERROR tune_controller.py:873 -- Trial task failed for trial train_tune_2b67e_00001\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2540, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2536, ip=172.28.0.12, actor_id=8752a43eaa4f7dfa3513752f01000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-14-de0babcfe350>\", line 105, in train_tune\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2000, 2000]], which is output 0 of AsStridedBackward0, is at version 4; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_tune_2b67e_00001:\n",
            "  date: 2023-07-08_03-04-46\n",
            "  hostname: 3ff3fc96ba34\n",
            "  node_ip: 172.28.0.12\n",
            "  pid: 2536\n",
            "  timestamp: 1688785486\n",
            "  trial_id: 2b67e_00001\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2023-07-08 03:04:51 (running for 00:00:15.37)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (2 ERROR, 8 PENDING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00002 | PENDING  |                  |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "| train_tune_2b67e_00003 | PENDING  |                  |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "| train_tune_2b67e_00004 | PENDING  |                  |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "| train_tune_2b67e_00005 | PENDING  |                  |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "| train_tune_2b67e_00006 | PENDING  |                  |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "| train_tune_2b67e_00007 | PENDING  |                  |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "| train_tune_2b67e_00008 | PENDING  |                  |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "| train_tune_2b67e_00009 | PENDING  |                  |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "| train_tune_2b67e_00000 | ERROR    | 172.28.0.12:2371 |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "| train_tune_2b67e_00001 | ERROR    | 172.28.0.12:2536 |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "Number of errored trials: 2\n",
            "+------------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name             |   # failures | error file                                                                                                                                                                                                     |\n",
            "|------------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_tune_2b67e_00000 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00000_0_batch_size=8,0=0.0002,1=0.0015,2=0.0002,3=0.0864,4=0.0000,0=0.0000,1=0.0033,2=0.0000,3=0.0000,4=0.0006_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00001 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00001_1_batch_size=8,0=0.0000,1=0.0025,2=0.0124,3=0.0022,4=0.0001,0=0.0001,1=0.0013,2=0.0004,3=0.0008,4=0.0023_2023-07-08_03-04-36/error.txt |\n",
            "+------------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2684)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=2684)\u001b[0m   warnings.warn(_create_warning_msg(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "2023-07-08 03:04:53,130\tERROR tune_controller.py:873 -- Trial task failed for trial train_tune_2b67e_00002\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2540, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2684, ip=172.28.0.12, actor_id=481e1f259cbfe9132ed3cdbd01000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-14-de0babcfe350>\", line 105, in train_tune\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2000, 2000]], which is output 0 of AsStridedBackward0, is at version 4; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_tune_2b67e_00002:\n",
            "  date: 2023-07-08_03-04-52\n",
            "  hostname: 3ff3fc96ba34\n",
            "  node_ip: 172.28.0.12\n",
            "  pid: 2684\n",
            "  timestamp: 1688785492\n",
            "  trial_id: 2b67e_00002\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2023-07-08 03:04:56 (running for 00:00:20.46)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (3 ERROR, 7 PENDING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00003 | PENDING  |                  |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "| train_tune_2b67e_00004 | PENDING  |                  |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "| train_tune_2b67e_00005 | PENDING  |                  |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "| train_tune_2b67e_00006 | PENDING  |                  |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "| train_tune_2b67e_00007 | PENDING  |                  |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "| train_tune_2b67e_00008 | PENDING  |                  |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "| train_tune_2b67e_00009 | PENDING  |                  |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "| train_tune_2b67e_00000 | ERROR    | 172.28.0.12:2371 |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "| train_tune_2b67e_00001 | ERROR    | 172.28.0.12:2536 |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "| train_tune_2b67e_00002 | ERROR    | 172.28.0.12:2684 |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "Number of errored trials: 3\n",
            "+------------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name             |   # failures | error file                                                                                                                                                                                                     |\n",
            "|------------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_tune_2b67e_00000 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00000_0_batch_size=8,0=0.0002,1=0.0015,2=0.0002,3=0.0864,4=0.0000,0=0.0000,1=0.0033,2=0.0000,3=0.0000,4=0.0006_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00001 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00001_1_batch_size=8,0=0.0000,1=0.0025,2=0.0124,3=0.0022,4=0.0001,0=0.0001,1=0.0013,2=0.0004,3=0.0008,4=0.0023_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00002 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00002_2_batch_size=8,0=0.0000,1=0.0000,2=0.0141,3=0.0001,4=0.0019,0=0.0001,1=0.0005,2=0.0002,3=0.0000,4=0.0028_2023-07-08_03-04-36/error.txt |\n",
            "+------------------------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2832)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "\u001b[2m\u001b[36m(func pid=2832)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "2023-07-08 03:04:59,364\tERROR tune_controller.py:873 -- Trial task failed for trial train_tune_2b67e_00003\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2540, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2832, ip=172.28.0.12, actor_id=06112240200e64e48843ef8d01000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-14-de0babcfe350>\", line 105, in train_tune\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2000, 2000]], which is output 0 of AsStridedBackward0, is at version 4; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_tune_2b67e_00003:\n",
            "  date: 2023-07-08_03-04-58\n",
            "  hostname: 3ff3fc96ba34\n",
            "  node_ip: 172.28.0.12\n",
            "  pid: 2832\n",
            "  timestamp: 1688785498\n",
            "  trial_id: 2b67e_00003\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2023-07-08 03:05:01 (running for 00:00:25.48)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (4 ERROR, 6 PENDING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00004 | PENDING  |                  |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "| train_tune_2b67e_00005 | PENDING  |                  |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "| train_tune_2b67e_00006 | PENDING  |                  |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "| train_tune_2b67e_00007 | PENDING  |                  |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "| train_tune_2b67e_00008 | PENDING  |                  |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "| train_tune_2b67e_00009 | PENDING  |                  |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "| train_tune_2b67e_00000 | ERROR    | 172.28.0.12:2371 |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "| train_tune_2b67e_00001 | ERROR    | 172.28.0.12:2536 |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "| train_tune_2b67e_00002 | ERROR    | 172.28.0.12:2684 |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "| train_tune_2b67e_00003 | ERROR    | 172.28.0.12:2832 |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "Number of errored trials: 4\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name             |   # failures | error file                                                                                                                                                                                                      |\n",
            "|------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_tune_2b67e_00000 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00000_0_batch_size=8,0=0.0002,1=0.0015,2=0.0002,3=0.0864,4=0.0000,0=0.0000,1=0.0033,2=0.0000,3=0.0000,4=0.0006_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00001 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00001_1_batch_size=8,0=0.0000,1=0.0025,2=0.0124,3=0.0022,4=0.0001,0=0.0001,1=0.0013,2=0.0004,3=0.0008,4=0.0023_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00002 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00002_2_batch_size=8,0=0.0000,1=0.0000,2=0.0141,3=0.0001,4=0.0019,0=0.0001,1=0.0005,2=0.0002,3=0.0000,4=0.0028_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00003 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00003_3_batch_size=32,0=0.0000,1=0.0000,2=0.0937,3=0.0000,4=0.0039,0=0.0000,1=0.0001,2=0.0006,3=0.0026,4=0.0023_2023-07-08_03-04-36/error.txt |\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=2983)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(func pid=2983)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "2023-07-08 03:05:03,731\tERROR tune_controller.py:873 -- Trial task failed for trial train_tune_2b67e_00004\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2540, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2983, ip=172.28.0.12, actor_id=5037c37f0056c8f01a35300b01000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-14-de0babcfe350>\", line 105, in train_tune\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2000, 2000]], which is output 0 of AsStridedBackward0, is at version 4; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_tune_2b67e_00004:\n",
            "  date: 2023-07-08_03-05-02\n",
            "  hostname: 3ff3fc96ba34\n",
            "  node_ip: 172.28.0.12\n",
            "  pid: 2983\n",
            "  timestamp: 1688785502\n",
            "  trial_id: 2b67e_00004\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2023-07-08 03:05:06 (running for 00:00:30.56)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (5 ERROR, 5 PENDING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00005 | PENDING  |                  |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "| train_tune_2b67e_00006 | PENDING  |                  |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "| train_tune_2b67e_00007 | PENDING  |                  |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "| train_tune_2b67e_00008 | PENDING  |                  |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "| train_tune_2b67e_00009 | PENDING  |                  |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "| train_tune_2b67e_00000 | ERROR    | 172.28.0.12:2371 |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "| train_tune_2b67e_00001 | ERROR    | 172.28.0.12:2536 |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "| train_tune_2b67e_00002 | ERROR    | 172.28.0.12:2684 |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "| train_tune_2b67e_00003 | ERROR    | 172.28.0.12:2832 |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "| train_tune_2b67e_00004 | ERROR    | 172.28.0.12:2983 |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "Number of errored trials: 5\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name             |   # failures | error file                                                                                                                                                                                                      |\n",
            "|------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_tune_2b67e_00000 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00000_0_batch_size=8,0=0.0002,1=0.0015,2=0.0002,3=0.0864,4=0.0000,0=0.0000,1=0.0033,2=0.0000,3=0.0000,4=0.0006_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00001 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00001_1_batch_size=8,0=0.0000,1=0.0025,2=0.0124,3=0.0022,4=0.0001,0=0.0001,1=0.0013,2=0.0004,3=0.0008,4=0.0023_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00002 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00002_2_batch_size=8,0=0.0000,1=0.0000,2=0.0141,3=0.0001,4=0.0019,0=0.0001,1=0.0005,2=0.0002,3=0.0000,4=0.0028_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00003 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00003_3_batch_size=32,0=0.0000,1=0.0000,2=0.0937,3=0.0000,4=0.0039,0=0.0000,1=0.0001,2=0.0006,3=0.0026,4=0.0023_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00004 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00004_4_batch_size=8,0=0.0000,1=0.0008,2=0.0336,3=0.0001,4=0.0660,0=0.0001,1=0.0019,2=0.0001,3=0.0000,4=0.0019_2023-07-08_03-04-36/error.txt  |\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=3131)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "\u001b[2m\u001b[36m(func pid=3131)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "2023-07-08 03:05:08,920\tERROR tune_controller.py:873 -- Trial task failed for trial train_tune_2b67e_00005\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2540, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3131, ip=172.28.0.12, actor_id=7249bfab05e9e0a07f3cff1b01000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-14-de0babcfe350>\", line 105, in train_tune\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2000, 2000]], which is output 0 of AsStridedBackward0, is at version 4; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_tune_2b67e_00005:\n",
            "  date: 2023-07-08_03-05-07\n",
            "  hostname: 3ff3fc96ba34\n",
            "  node_ip: 172.28.0.12\n",
            "  pid: 3131\n",
            "  timestamp: 1688785507\n",
            "  trial_id: 2b67e_00005\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2023-07-08 03:05:11 (running for 00:00:35.64)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (6 ERROR, 4 PENDING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00006 | PENDING  |                  |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "| train_tune_2b67e_00007 | PENDING  |                  |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "| train_tune_2b67e_00008 | PENDING  |                  |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "| train_tune_2b67e_00009 | PENDING  |                  |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "| train_tune_2b67e_00000 | ERROR    | 172.28.0.12:2371 |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "| train_tune_2b67e_00001 | ERROR    | 172.28.0.12:2536 |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "| train_tune_2b67e_00002 | ERROR    | 172.28.0.12:2684 |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "| train_tune_2b67e_00003 | ERROR    | 172.28.0.12:2832 |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "| train_tune_2b67e_00004 | ERROR    | 172.28.0.12:2983 |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "| train_tune_2b67e_00005 | ERROR    | 172.28.0.12:3131 |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "Number of errored trials: 6\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name             |   # failures | error file                                                                                                                                                                                                      |\n",
            "|------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_tune_2b67e_00000 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00000_0_batch_size=8,0=0.0002,1=0.0015,2=0.0002,3=0.0864,4=0.0000,0=0.0000,1=0.0033,2=0.0000,3=0.0000,4=0.0006_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00001 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00001_1_batch_size=8,0=0.0000,1=0.0025,2=0.0124,3=0.0022,4=0.0001,0=0.0001,1=0.0013,2=0.0004,3=0.0008,4=0.0023_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00002 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00002_2_batch_size=8,0=0.0000,1=0.0000,2=0.0141,3=0.0001,4=0.0019,0=0.0001,1=0.0005,2=0.0002,3=0.0000,4=0.0028_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00003 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00003_3_batch_size=32,0=0.0000,1=0.0000,2=0.0937,3=0.0000,4=0.0039,0=0.0000,1=0.0001,2=0.0006,3=0.0026,4=0.0023_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00004 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00004_4_batch_size=8,0=0.0000,1=0.0008,2=0.0336,3=0.0001,4=0.0660,0=0.0001,1=0.0019,2=0.0001,3=0.0000,4=0.0019_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00005 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00005_5_batch_size=32,0=0.0105,1=0.0177,2=0.0068,3=0.0084,4=0.0116,0=0.0011,1=0.0065,2=0.0001,3=0.0061,4=0.0038_2023-07-08_03-04-36/error.txt |\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=3277)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(func pid=3277)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "2023-07-08 03:05:15,176\tERROR tune_controller.py:873 -- Trial task failed for trial train_tune_2b67e_00006\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2540, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3277, ip=172.28.0.12, actor_id=2b36a8547e68166edaedfe9401000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-14-de0babcfe350>\", line 105, in train_tune\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2000, 2000]], which is output 0 of AsStridedBackward0, is at version 4; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_tune_2b67e_00006:\n",
            "  date: 2023-07-08_03-05-14\n",
            "  hostname: 3ff3fc96ba34\n",
            "  node_ip: 172.28.0.12\n",
            "  pid: 3277\n",
            "  timestamp: 1688785514\n",
            "  trial_id: 2b67e_00006\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2023-07-08 03:05:16 (running for 00:00:40.69)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (7 ERROR, 3 PENDING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00007 | PENDING  |                  |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "| train_tune_2b67e_00008 | PENDING  |                  |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "| train_tune_2b67e_00009 | PENDING  |                  |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "| train_tune_2b67e_00000 | ERROR    | 172.28.0.12:2371 |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "| train_tune_2b67e_00001 | ERROR    | 172.28.0.12:2536 |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "| train_tune_2b67e_00002 | ERROR    | 172.28.0.12:2684 |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "| train_tune_2b67e_00003 | ERROR    | 172.28.0.12:2832 |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "| train_tune_2b67e_00004 | ERROR    | 172.28.0.12:2983 |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "| train_tune_2b67e_00005 | ERROR    | 172.28.0.12:3131 |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "| train_tune_2b67e_00006 | ERROR    | 172.28.0.12:3277 |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "Number of errored trials: 7\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name             |   # failures | error file                                                                                                                                                                                                      |\n",
            "|------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_tune_2b67e_00000 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00000_0_batch_size=8,0=0.0002,1=0.0015,2=0.0002,3=0.0864,4=0.0000,0=0.0000,1=0.0033,2=0.0000,3=0.0000,4=0.0006_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00001 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00001_1_batch_size=8,0=0.0000,1=0.0025,2=0.0124,3=0.0022,4=0.0001,0=0.0001,1=0.0013,2=0.0004,3=0.0008,4=0.0023_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00002 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00002_2_batch_size=8,0=0.0000,1=0.0000,2=0.0141,3=0.0001,4=0.0019,0=0.0001,1=0.0005,2=0.0002,3=0.0000,4=0.0028_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00003 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00003_3_batch_size=32,0=0.0000,1=0.0000,2=0.0937,3=0.0000,4=0.0039,0=0.0000,1=0.0001,2=0.0006,3=0.0026,4=0.0023_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00004 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00004_4_batch_size=8,0=0.0000,1=0.0008,2=0.0336,3=0.0001,4=0.0660,0=0.0001,1=0.0019,2=0.0001,3=0.0000,4=0.0019_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00005 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00005_5_batch_size=32,0=0.0105,1=0.0177,2=0.0068,3=0.0084,4=0.0116,0=0.0011,1=0.0065,2=0.0001,3=0.0061,4=0.0038_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00006 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00006_6_batch_size=64,0=0.0219,1=0.0418,2=0.0004,3=0.0001,4=0.0000,0=0.0088,1=0.0001,2=0.0000,3=0.0003,4=0.0067_2023-07-08_03-04-36/error.txt |\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-07-08 03:05:19,826\tERROR tune_controller.py:873 -- Trial task failed for trial train_tune_2b67e_00007\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2540, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3429, ip=172.28.0.12, actor_id=c34fb4d1b27539a9a9546b6001000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-14-de0babcfe350>\", line 105, in train_tune\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2000, 2000]], which is output 0 of AsStridedBackward0, is at version 4; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_tune_2b67e_00007:\n",
            "  date: 2023-07-08_03-05-19\n",
            "  hostname: 3ff3fc96ba34\n",
            "  node_ip: 172.28.0.12\n",
            "  pid: 3429\n",
            "  timestamp: 1688785519\n",
            "  trial_id: 2b67e_00007\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2023-07-08 03:05:21 (running for 00:00:45.74)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (8 ERROR, 2 PENDING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00008 | PENDING  |                  |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "| train_tune_2b67e_00009 | PENDING  |                  |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "| train_tune_2b67e_00000 | ERROR    | 172.28.0.12:2371 |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "| train_tune_2b67e_00001 | ERROR    | 172.28.0.12:2536 |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "| train_tune_2b67e_00002 | ERROR    | 172.28.0.12:2684 |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "| train_tune_2b67e_00003 | ERROR    | 172.28.0.12:2832 |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "| train_tune_2b67e_00004 | ERROR    | 172.28.0.12:2983 |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "| train_tune_2b67e_00005 | ERROR    | 172.28.0.12:3131 |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "| train_tune_2b67e_00006 | ERROR    | 172.28.0.12:3277 |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "| train_tune_2b67e_00007 | ERROR    | 172.28.0.12:3429 |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "Number of errored trials: 8\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name             |   # failures | error file                                                                                                                                                                                                      |\n",
            "|------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_tune_2b67e_00000 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00000_0_batch_size=8,0=0.0002,1=0.0015,2=0.0002,3=0.0864,4=0.0000,0=0.0000,1=0.0033,2=0.0000,3=0.0000,4=0.0006_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00001 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00001_1_batch_size=8,0=0.0000,1=0.0025,2=0.0124,3=0.0022,4=0.0001,0=0.0001,1=0.0013,2=0.0004,3=0.0008,4=0.0023_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00002 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00002_2_batch_size=8,0=0.0000,1=0.0000,2=0.0141,3=0.0001,4=0.0019,0=0.0001,1=0.0005,2=0.0002,3=0.0000,4=0.0028_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00003 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00003_3_batch_size=32,0=0.0000,1=0.0000,2=0.0937,3=0.0000,4=0.0039,0=0.0000,1=0.0001,2=0.0006,3=0.0026,4=0.0023_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00004 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00004_4_batch_size=8,0=0.0000,1=0.0008,2=0.0336,3=0.0001,4=0.0660,0=0.0001,1=0.0019,2=0.0001,3=0.0000,4=0.0019_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00005 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00005_5_batch_size=32,0=0.0105,1=0.0177,2=0.0068,3=0.0084,4=0.0116,0=0.0011,1=0.0065,2=0.0001,3=0.0061,4=0.0038_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00006 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00006_6_batch_size=64,0=0.0219,1=0.0418,2=0.0004,3=0.0001,4=0.0000,0=0.0088,1=0.0001,2=0.0000,3=0.0003,4=0.0067_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00007 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00007_7_batch_size=4,0=0.0768,1=0.0008,2=0.0132,3=0.0000,4=0.0028,0=0.0001,1=0.0000,2=0.0000,3=0.0005,4=0.0001_2023-07-08_03-04-36/error.txt  |\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=3577)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(func pid=3577)\u001b[0m   warnings.warn(_create_warning_msg(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "2023-07-08 03:05:25,852\tERROR tune_controller.py:873 -- Trial task failed for trial train_tune_2b67e_00008\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2540, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3577, ip=172.28.0.12, actor_id=edad029b8d3625446992c55701000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-14-de0babcfe350>\", line 105, in train_tune\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2000, 2000]], which is output 0 of AsStridedBackward0, is at version 4; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_tune_2b67e_00008:\n",
            "  date: 2023-07-08_03-05-24\n",
            "  hostname: 3ff3fc96ba34\n",
            "  node_ip: 172.28.0.12\n",
            "  pid: 3577\n",
            "  timestamp: 1688785524\n",
            "  trial_id: 2b67e_00008\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2023-07-08 03:05:26 (running for 00:00:50.77)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (9 ERROR, 1 PENDING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00009 | PENDING  |                  |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "| train_tune_2b67e_00000 | ERROR    | 172.28.0.12:2371 |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "| train_tune_2b67e_00001 | ERROR    | 172.28.0.12:2536 |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "| train_tune_2b67e_00002 | ERROR    | 172.28.0.12:2684 |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "| train_tune_2b67e_00003 | ERROR    | 172.28.0.12:2832 |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "| train_tune_2b67e_00004 | ERROR    | 172.28.0.12:2983 |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "| train_tune_2b67e_00005 | ERROR    | 172.28.0.12:3131 |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "| train_tune_2b67e_00006 | ERROR    | 172.28.0.12:3277 |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "| train_tune_2b67e_00007 | ERROR    | 172.28.0.12:3429 |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "| train_tune_2b67e_00008 | ERROR    | 172.28.0.12:3577 |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "Number of errored trials: 9\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name             |   # failures | error file                                                                                                                                                                                                      |\n",
            "|------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_tune_2b67e_00000 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00000_0_batch_size=8,0=0.0002,1=0.0015,2=0.0002,3=0.0864,4=0.0000,0=0.0000,1=0.0033,2=0.0000,3=0.0000,4=0.0006_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00001 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00001_1_batch_size=8,0=0.0000,1=0.0025,2=0.0124,3=0.0022,4=0.0001,0=0.0001,1=0.0013,2=0.0004,3=0.0008,4=0.0023_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00002 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00002_2_batch_size=8,0=0.0000,1=0.0000,2=0.0141,3=0.0001,4=0.0019,0=0.0001,1=0.0005,2=0.0002,3=0.0000,4=0.0028_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00003 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00003_3_batch_size=32,0=0.0000,1=0.0000,2=0.0937,3=0.0000,4=0.0039,0=0.0000,1=0.0001,2=0.0006,3=0.0026,4=0.0023_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00004 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00004_4_batch_size=8,0=0.0000,1=0.0008,2=0.0336,3=0.0001,4=0.0660,0=0.0001,1=0.0019,2=0.0001,3=0.0000,4=0.0019_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00005 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00005_5_batch_size=32,0=0.0105,1=0.0177,2=0.0068,3=0.0084,4=0.0116,0=0.0011,1=0.0065,2=0.0001,3=0.0061,4=0.0038_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00006 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00006_6_batch_size=64,0=0.0219,1=0.0418,2=0.0004,3=0.0001,4=0.0000,0=0.0088,1=0.0001,2=0.0000,3=0.0003,4=0.0067_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00007 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00007_7_batch_size=4,0=0.0768,1=0.0008,2=0.0132,3=0.0000,4=0.0028,0=0.0001,1=0.0000,2=0.0000,3=0.0005,4=0.0001_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00008 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00008_8_batch_size=64,0=0.0000,1=0.0073,2=0.0010,3=0.0420,4=0.0009,0=0.0001,1=0.0005,2=0.0000,3=0.0000,4=0.0004_2023-07-08_03-04-36/error.txt |\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "\u001b[2m\u001b[36m(func pid=3735)\u001b[0m   warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2023-07-08 03:05:31 (running for 00:00:55.82)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (9 ERROR, 1 RUNNING)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00009 | RUNNING  | 172.28.0.12:3735 |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "| train_tune_2b67e_00000 | ERROR    | 172.28.0.12:2371 |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "| train_tune_2b67e_00001 | ERROR    | 172.28.0.12:2536 |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "| train_tune_2b67e_00002 | ERROR    | 172.28.0.12:2684 |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "| train_tune_2b67e_00003 | ERROR    | 172.28.0.12:2832 |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "| train_tune_2b67e_00004 | ERROR    | 172.28.0.12:2983 |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "| train_tune_2b67e_00005 | ERROR    | 172.28.0.12:3131 |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "| train_tune_2b67e_00006 | ERROR    | 172.28.0.12:3277 |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "| train_tune_2b67e_00007 | ERROR    | 172.28.0.12:3429 |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "| train_tune_2b67e_00008 | ERROR    | 172.28.0.12:3577 |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "Number of errored trials: 9\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name             |   # failures | error file                                                                                                                                                                                                      |\n",
            "|------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_tune_2b67e_00000 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00000_0_batch_size=8,0=0.0002,1=0.0015,2=0.0002,3=0.0864,4=0.0000,0=0.0000,1=0.0033,2=0.0000,3=0.0000,4=0.0006_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00001 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00001_1_batch_size=8,0=0.0000,1=0.0025,2=0.0124,3=0.0022,4=0.0001,0=0.0001,1=0.0013,2=0.0004,3=0.0008,4=0.0023_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00002 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00002_2_batch_size=8,0=0.0000,1=0.0000,2=0.0141,3=0.0001,4=0.0019,0=0.0001,1=0.0005,2=0.0002,3=0.0000,4=0.0028_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00003 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00003_3_batch_size=32,0=0.0000,1=0.0000,2=0.0937,3=0.0000,4=0.0039,0=0.0000,1=0.0001,2=0.0006,3=0.0026,4=0.0023_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00004 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00004_4_batch_size=8,0=0.0000,1=0.0008,2=0.0336,3=0.0001,4=0.0660,0=0.0001,1=0.0019,2=0.0001,3=0.0000,4=0.0019_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00005 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00005_5_batch_size=32,0=0.0105,1=0.0177,2=0.0068,3=0.0084,4=0.0116,0=0.0011,1=0.0065,2=0.0001,3=0.0061,4=0.0038_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00006 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00006_6_batch_size=64,0=0.0219,1=0.0418,2=0.0004,3=0.0001,4=0.0000,0=0.0088,1=0.0001,2=0.0000,3=0.0003,4=0.0067_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00007 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00007_7_batch_size=4,0=0.0768,1=0.0008,2=0.0132,3=0.0000,4=0.0028,0=0.0001,1=0.0000,2=0.0000,3=0.0005,4=0.0001_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00008 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00008_8_batch_size=64,0=0.0000,1=0.0073,2=0.0010,3=0.0420,4=0.0009,0=0.0001,1=0.0005,2=0.0000,3=0.0000,4=0.0004_2023-07-08_03-04-36/error.txt |\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-07-08 03:05:33,206\tERROR tune_controller.py:873 -- Trial task failed for trial train_tune_2b67e_00009\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2540, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=3735, ip=172.28.0.12, actor_id=121a6acb2406b96a21852f5f01000000, repr=func)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n",
            "    return self._trainable_func(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"<ipython-input-14-de0babcfe350>\", line 105, in train_tune\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2000, 2000]], which is output 0 of AsStridedBackward0, is at version 4; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for train_tune_2b67e_00009:\n",
            "  date: 2023-07-08_03-05-30\n",
            "  hostname: 3ff3fc96ba34\n",
            "  node_ip: 172.28.0.12\n",
            "  pid: 3735\n",
            "  timestamp: 1688785530\n",
            "  trial_id: 2b67e_00009\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2023-07-08 03:05:33 (running for 00:00:57.21)\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs\n",
            "Result logdir: /root/ray_results/train_tune_2023-07-08_03-04-35\n",
            "Number of trials: 10/10 (10 ERROR)\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "| Trial name             | status   | loc              |   batch_size |       lrs/0 |       lrs/1 |       lrs/2 |       lrs/3 |       lrs/4 |   weight_decays/0 |   weight_decays/1 |   weight_decays/2 |   weight_decays/3 |   weight_decays/4 |\n",
            "|------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------|\n",
            "| train_tune_2b67e_00000 | ERROR    | 172.28.0.12:2371 |            8 | 0.000231233 | 0.00151764  | 0.000183988 | 0.0864386   | 1.23852e-05 |       2.80864e-05 |       0.00334544  |       1.9774e-05  |       1.21783e-05 |       0.000601674 |\n",
            "| train_tune_2b67e_00001 | ERROR    | 172.28.0.12:2536 |            8 | 1.09866e-05 | 0.00245136  | 0.0124172   | 0.0022416   | 7.31615e-05 |       0.000121135 |       0.00131714  |       0.000375873 |       0.000804642 |       0.00225363  |\n",
            "| train_tune_2b67e_00002 | ERROR    | 172.28.0.12:2684 |            8 | 2.52757e-05 | 1.29059e-05 | 0.0141081   | 8.06037e-05 | 0.00188035  |       6.01634e-05 |       0.000515268 |       0.000186408 |       1.34493e-05 |       0.00277386  |\n",
            "| train_tune_2b67e_00003 | ERROR    | 172.28.0.12:2832 |           32 | 1.27073e-05 | 3.38245e-05 | 0.0937307   | 1.7353e-05  | 0.00391765  |       2.00081e-05 |       0.000123207 |       0.000619341 |       0.00261214  |       0.00233187  |\n",
            "| train_tune_2b67e_00004 | ERROR    | 172.28.0.12:2983 |            8 | 3.9627e-05  | 0.000752127 | 0.0335773   | 9.28683e-05 | 0.0659756   |       9.78942e-05 |       0.00186027  |       8.25858e-05 |       1.22711e-05 |       0.00189509  |\n",
            "| train_tune_2b67e_00005 | ERROR    | 172.28.0.12:3131 |           32 | 0.0104532   | 0.0176856   | 0.00682706  | 0.00839048  | 0.0115873   |       0.00106238  |       0.00650927  |       0.00013725  |       0.00613714  |       0.00377981  |\n",
            "| train_tune_2b67e_00006 | ERROR    | 172.28.0.12:3277 |           64 | 0.0218756   | 0.0418042   | 0.000392627 | 0.000120185 | 1.03278e-05 |       0.00884265  |       5.39457e-05 |       1.02988e-05 |       0.000307204 |       0.00667498  |\n",
            "| train_tune_2b67e_00007 | ERROR    | 172.28.0.12:3429 |            4 | 0.0768178   | 0.000758124 | 0.0132423   | 3.23387e-05 | 0.00283544  |       7.984e-05   |       1.56175e-05 |       1.61012e-05 |       0.000470606 |       5.08277e-05 |\n",
            "| train_tune_2b67e_00008 | ERROR    | 172.28.0.12:3577 |           64 | 2.50842e-05 | 0.00727307  | 0.00100597  | 0.0420341   | 0.000903832 |       0.000128075 |       0.000514913 |       2.56695e-05 |       3.65906e-05 |       0.000410403 |\n",
            "| train_tune_2b67e_00009 | ERROR    | 172.28.0.12:3735 |           16 | 0.000115676 | 0.00175206  | 0.0014111   | 0.049195    | 0.0216671   |       3.10544e-05 |       3.06999e-05 |       0.0043982   |       4.75352e-05 |       6.02756e-05 |\n",
            "+------------------------+----------+------------------+--------------+-------------+-------------+-------------+-------------+-------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "Number of errored trials: 10\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name             |   # failures | error file                                                                                                                                                                                                      |\n",
            "|------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| train_tune_2b67e_00000 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00000_0_batch_size=8,0=0.0002,1=0.0015,2=0.0002,3=0.0864,4=0.0000,0=0.0000,1=0.0033,2=0.0000,3=0.0000,4=0.0006_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00001 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00001_1_batch_size=8,0=0.0000,1=0.0025,2=0.0124,3=0.0022,4=0.0001,0=0.0001,1=0.0013,2=0.0004,3=0.0008,4=0.0023_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00002 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00002_2_batch_size=8,0=0.0000,1=0.0000,2=0.0141,3=0.0001,4=0.0019,0=0.0001,1=0.0005,2=0.0002,3=0.0000,4=0.0028_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00003 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00003_3_batch_size=32,0=0.0000,1=0.0000,2=0.0937,3=0.0000,4=0.0039,0=0.0000,1=0.0001,2=0.0006,3=0.0026,4=0.0023_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00004 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00004_4_batch_size=8,0=0.0000,1=0.0008,2=0.0336,3=0.0001,4=0.0660,0=0.0001,1=0.0019,2=0.0001,3=0.0000,4=0.0019_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00005 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00005_5_batch_size=32,0=0.0105,1=0.0177,2=0.0068,3=0.0084,4=0.0116,0=0.0011,1=0.0065,2=0.0001,3=0.0061,4=0.0038_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00006 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00006_6_batch_size=64,0=0.0219,1=0.0418,2=0.0004,3=0.0001,4=0.0000,0=0.0088,1=0.0001,2=0.0000,3=0.0003,4=0.0067_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00007 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00007_7_batch_size=4,0=0.0768,1=0.0008,2=0.0132,3=0.0000,4=0.0028,0=0.0001,1=0.0000,2=0.0000,3=0.0005,4=0.0001_2023-07-08_03-04-36/error.txt  |\n",
            "| train_tune_2b67e_00008 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00008_8_batch_size=64,0=0.0000,1=0.0073,2=0.0010,3=0.0420,4=0.0009,0=0.0001,1=0.0005,2=0.0000,3=0.0000,4=0.0004_2023-07-08_03-04-36/error.txt |\n",
            "| train_tune_2b67e_00009 |            1 | /root/ray_results/train_tune_2023-07-08_03-04-35/train_tune_2b67e_00009_9_batch_size=16,0=0.0001,1=0.0018,2=0.0014,3=0.0492,4=0.0217,0=0.0000,1=0.0000,2=0.0044,3=0.0000,4=0.0001_2023-07-08_03-04-36/error.txt |\n",
            "+------------------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TuneError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ea431a029216>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# You can change the number of GPUs per trial here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-ea431a029216>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     result = tune.run(\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mresources_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gpu\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, checkpoint_keep_all_ranks, checkpoint_upload_from_workers, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, trial_executor, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexperiment_interrupted_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_tune_2b67e_00000, train_tune_2b67e_00001, train_tune_2b67e_00002, train_tune_2b67e_00003, train_tune_2b67e_00004, train_tune_2b67e_00005, train_tune_2b67e_00006, train_tune_2b67e_00007, train_tune_2b67e_00008, train_tune_2b67e_00009])"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi5CDidzl5jN"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-ZLMO_AQ2Cj5"
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "def train(num_epochs, model, train_loader, test_loader, final_loss, pos_optimizers, neg_optimizers, final_optimizer, thresholds, records_per_epoch, supervised = True):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # variables for training output\n",
        "    total_pos_goodnesses,total_neg_goodnesses = [],[]\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "    total_step = len(train_loader)\n",
        "    record_period = total_step//records_per_epoch\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_pos_goodness,total_neg_goodness = [0]*(len(model.layers)-1),[0]*(len(model.layers)-1)\n",
        "        total_loss = 0\n",
        "\n",
        "        for i,(x,y) in enumerate(train_loader): # iterate through dataset\n",
        "            x,y = x.to(device),y.to(device)\n",
        "            if not supervised: # unsupervised training input\n",
        "              pos_imgs = x.squeeze()\n",
        "              neg_imgs = generate_negative_example().squeeze().to(device)\n",
        "            else: # supervised learning input\n",
        "              pos_imgs = model.construct_supervised_example(x,y,True)\n",
        "              neg_imgs = model.construct_supervised_example(x,y,False)\n",
        "\n",
        "            # intermediate variables\n",
        "            pos_interms = []\n",
        "            pos_interm = pos_imgs.clone()\n",
        "            neg_interm = neg_imgs.clone()\n",
        "\n",
        "            # iterate over intermediate layers\n",
        "            for index in range(len(model.layers)-1):\n",
        "              layer = model.layers[index]\n",
        "\n",
        "              # positive pass\n",
        "              pos_output = layer(pos_interm)\n",
        "              pos_goodness = model.criterion(pos_output, threshold=thresholds[index])\n",
        "              pos_interm = model.ln(pos_output)\n",
        "              pos_interms.append(pos_interm.detach().clone())\n",
        "\n",
        "              # update variables\n",
        "              total_pos_goodness[index] += pos_goodness\n",
        "\n",
        "              # clear gradients for this training step\n",
        "              pos_optimizers[index].zero_grad()\n",
        "\n",
        "              # take gradient step\n",
        "              pos_goodness.backward(retain_graph=True)\n",
        "              pos_optimizers[index].step()\n",
        "\n",
        "              # negative pass\n",
        "              neg_output = layer(neg_interm)\n",
        "              neg_goodness = model.criterion(neg_output, threshold=thresholds[index])\n",
        "              neg_interm = model.ln(neg_output)\n",
        "\n",
        "              total_neg_goodness[index] += neg_goodness\n",
        "              neg_optimizers[index].zero_grad()\n",
        "              neg_goodness.backward(retain_graph=True)\n",
        "              neg_optimizers[index].step()\n",
        "\n",
        "              # update variables\n",
        "              total_pos_goodness[index] += pos_goodness\n",
        "              total_neg_goodness[index] += neg_goodness\n",
        "\n",
        "              # check progress\n",
        "              if (i+1) % (record_period) == 0:\n",
        "                  avg_pos_goodness = total_pos_goodness[index]/(record_period)\n",
        "                  avg_neg_goodness = total_neg_goodness[index]/(record_period)\n",
        "                  if len(total_pos_goodnesses)>index:\n",
        "                    total_pos_goodnesses[index].append(avg_pos_goodness)\n",
        "                  else:\n",
        "                    total_pos_goodnesses.append([avg_pos_goodness])\n",
        "                  if len(total_neg_goodnesses)>index:\n",
        "                    total_neg_goodnesses[index].append(avg_neg_goodness)\n",
        "                  else:\n",
        "                    total_neg_goodnesses.append([avg_neg_goodness])\n",
        "\n",
        "                  print ('Layer {}, Epoch [{}/{}], Step [{}/{}], Positive Goodness: {:.4f}, Negative Goodness: {:.4f}'\n",
        "                        .format(index, epoch + 1, num_epochs, i + 1, total_step, avg_pos_goodness, avg_neg_goodness))\n",
        "                  total_pos_goodness[index],total_neg_goodness[index] = 0,0\n",
        "\n",
        "            # output layer\n",
        "            final_input = torch.cat(pos_interms[1:],dim=1)\n",
        "            labels = model.layers[-1](final_input)\n",
        "            loss = ceLoss(labels,y)\n",
        "            total_loss += loss\n",
        "\n",
        "            final_optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            final_optimizer.step()\n",
        "\n",
        "            if (i+1) % record_period == 0:\n",
        "              avg_loss = total_loss/record_period\n",
        "              train_losses.append(avg_loss)\n",
        "\n",
        "              print ('Final Layer, Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                    .format(epoch + 1, num_epochs, i + 1, total_step, avg_loss))\n",
        "              total_loss = 0\n",
        "\n",
        "              # test model on test set\n",
        "              test_loss, test_acc = test(model, test_loader, final_loss)\n",
        "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Test Accuracy: {:.4f}'\n",
        "                    .format(epoch + 1, num_epochs, i + 1, total_step, test_loss, test_acc))\n",
        "              test_losses.append(test_loss)\n",
        "              test_accuracies.append(test_acc)\n",
        "\n",
        "        pass\n",
        "\n",
        "        # save model every 5 epochs\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            torch.save(model.state_dict(), 'model_v1.1_epoch{}.pth'.format(epoch+1))\n",
        "        pass\n",
        "    return total_pos_goodnesses, total_neg_goodnesses, train_losses, test_losses, test_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "X2baZqMWtom8",
        "outputId": "1477455d-9855-4674-9711-c234c032efe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0, Epoch [1/15], Step [234/938], Positive Goodness: 1.8662, Negative Goodness: 1.8616\n",
            "Layer 1, Epoch [1/15], Step [234/938], Positive Goodness: 1.9969, Negative Goodness: 2.0000\n",
            "Layer 2, Epoch [1/15], Step [234/938], Positive Goodness: 1.9895, Negative Goodness: 1.9941\n",
            "Layer 3, Epoch [1/15], Step [234/938], Positive Goodness: 1.9596, Negative Goodness: 1.9702\n",
            "Final Layer, Epoch [1/15], Step [234/938], Loss: 0.3814\n",
            "Epoch [1/15], Step [234/938], Loss: 0.4106, Test Accuracy: 0.0496\n",
            "Layer 0, Epoch [1/15], Step [468/938], Positive Goodness: 1.8694, Negative Goodness: 1.7920\n",
            "Layer 1, Epoch [1/15], Step [468/938], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "Layer 2, Epoch [1/15], Step [468/938], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "Layer 3, Epoch [1/15], Step [468/938], Positive Goodness: 1.9999, Negative Goodness: 1.9999\n",
            "Final Layer, Epoch [1/15], Step [468/938], Loss: 0.1387\n",
            "Epoch [1/15], Step [468/938], Loss: 0.3858, Test Accuracy: 0.0205\n",
            "Layer 0, Epoch [1/15], Step [702/938], Positive Goodness: 1.8212, Negative Goodness: 1.6409\n",
            "Layer 1, Epoch [1/15], Step [702/938], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "Layer 2, Epoch [1/15], Step [702/938], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "Layer 3, Epoch [1/15], Step [702/938], Positive Goodness: 1.9940, Negative Goodness: 1.9865\n",
            "Final Layer, Epoch [1/15], Step [702/938], Loss: 0.0887\n",
            "Epoch [1/15], Step [702/938], Loss: 0.3682, Test Accuracy: 0.0181\n",
            "Layer 0, Epoch [1/15], Step [936/938], Positive Goodness: 1.8210, Negative Goodness: 1.5708\n",
            "Layer 1, Epoch [1/15], Step [936/938], Positive Goodness: 2.0000, Negative Goodness: 2.0000\n",
            "Layer 2, Epoch [1/15], Step [936/938], Positive Goodness: 2.0000, Negative Goodness: 1.9991\n",
            "Layer 3, Epoch [1/15], Step [936/938], Positive Goodness: 1.9903, Negative Goodness: 1.9584\n",
            "Final Layer, Epoch [1/15], Step [936/938], Loss: 0.0698\n",
            "Epoch [1/15], Step [936/938], Loss: 0.3403, Test Accuracy: 0.0112\n",
            "Layer 0, Epoch [2/15], Step [234/938], Positive Goodness: 1.7544, Negative Goodness: 1.4763\n",
            "Layer 1, Epoch [2/15], Step [234/938], Positive Goodness: 1.9073, Negative Goodness: 1.8971\n",
            "Layer 2, Epoch [2/15], Step [234/938], Positive Goodness: 1.9871, Negative Goodness: 1.9558\n",
            "Layer 3, Epoch [2/15], Step [234/938], Positive Goodness: 1.9701, Negative Goodness: 1.9106\n",
            "Final Layer, Epoch [2/15], Step [234/938], Loss: 0.0594\n",
            "Epoch [2/15], Step [234/938], Loss: 0.3958, Test Accuracy: 0.0119\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6e934f874c97>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_pos_goodnesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_neg_goodnesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_optimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_optimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupervised\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-64cb0bf81407>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, train_loader, test_loader, final_loss, pos_optimizers, neg_optimizers, final_optimizer, thresholds, records_per_epoch, supervised)\u001b[0m\n\u001b[1;32m     48\u001b[0m               \u001b[0;31m# take gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m               \u001b[0mpos_goodness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m               \u001b[0mpos_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m               \u001b[0;31m# negative pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mhas_sparse_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             sgd(params_with_grad,\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_sgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    223\u001b[0m          \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m_multi_tensor_sgd\u001b[0;34m(params, grads, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mdevice_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 14.75 GiB total capacity; 13.35 GiB already allocated; 14.81 MiB free; 13.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "total_pos_goodnesses, total_neg_goodnesses, train_losses, test_losses, test_accuracies = train(num_epochs, model, train_loader, test_loader, ceLoss, pos_optimizers, neg_optimizers, final_optimizer, thresholds, records_per_epoch, supervised)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2GZVnrctom8"
      },
      "source": [
        "## Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "K5-yXqsh1-HP"
      },
      "outputs": [],
      "source": [
        "# model evaluation using test set\n",
        "def test(model,test_loader,loss_func):\n",
        "    # return loss-->float, accuracy-->float\n",
        "    model.eval() # switch to eval mode\n",
        "    with torch.no_grad(): # disable gradient calculation\n",
        "        correct = 0\n",
        "        total_loss = 0\n",
        "        total_steps = 0\n",
        "        total = 0\n",
        "        for x, y in test_loader:\n",
        "            x,y = x.to(device),y.to(device)\n",
        "            # use forward to calculate loss\n",
        "            labels = model.forward(x)\n",
        "            loss = loss_func(labels,y)\n",
        "            total_loss += loss\n",
        "            total_steps += 1\n",
        "            # use predict to predict accuracies\n",
        "            predictions = model.predict(x,device)\n",
        "            acc = torch.sum(predictions == y)\n",
        "            correct += acc\n",
        "            total += x.shape[0]\n",
        "    model.train() # switch back to train mode\n",
        "    return total_loss/total_steps, correct/total"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = test(model, test_loader,ceLoss)\n",
        "print('Test Accuracy of the model on the test set: %.2f' % (test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_8yPfk03oLT",
        "outputId": "681bdc82-57ce-4d90-8ae9-612bedeffd7b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the test set: 0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "PMheoEy12rn2",
        "outputId": "800c2db2-baa9-4c66-887a-1643a3e7e146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:  tensor([7, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 1, 0, 0, 1, 7, 7, 1, 1, 0,\n",
            "        0, 0, 7, 0, 0, 7, 1, 0, 0, 7, 0, 0, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 7, 1, 0, 1, 0, 7, 1, 0, 7, 0], device='cuda:0')\n",
            " Label:  tensor([7, 0, 7, 8, 3, 0, 2, 5, 3, 3, 8, 2, 9, 2, 9, 1, 6, 0, 1, 9, 7, 1, 1, 8,\n",
            "        8, 8, 9, 3, 0, 7, 5, 5, 8, 4, 8, 9, 9, 7, 6, 7, 4, 9, 8, 7, 8, 5, 0, 7,\n",
            "        0, 9, 9, 0, 4, 4, 9, 2, 1, 9, 2, 4, 1, 7, 1, 2], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# prediction test & visualization\n",
        "x,y = next(iter(train_loader))\n",
        "x,y = x.to(device),y.to(device)\n",
        "neg_x = model.construct_supervised_example(x,y,False)\n",
        "print(\"Prediction: \", model.predict(x,device))\n",
        "print(\" Label: \",y)\n",
        "plt.imshow(neg_x[0].squeeze().cpu(), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWYmB7jR9ZXz"
      },
      "source": [
        "## Previous Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLzUiNdH9YWN",
        "outputId": "b2f6ff8d-b849-49e1-97b7-308e5cc99ccc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('model_v1.pth',map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgSMzN_F-Ibw",
        "outputId": "886249df-924d-4661-967c-cf10206446b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f129f0bc310>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Accuracy of the model on the test set: 0.65\n"
          ]
        }
      ],
      "source": [
        "# adjusted test function (label is messed up when model_v1)\n",
        "def test_v1():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for x, label in test_loader:\n",
        "            test_output = model.predict(x.to(device)) + 1\n",
        "            if label == 0:\n",
        "              test_output -= 5\n",
        "            correct += (test_output == label)\n",
        "            total += 1\n",
        "    print()\n",
        "    print('Test Accuracy of the model on the test set: %.2f' % (correct/total))\n",
        "test_v1()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "8xEIHA2ZN1wj"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}